{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 컴퓨터 비전 학습 경진대회, Dacon.io\n",
    "### https://dacon.io/competitions/official/235626/overview/\n",
    "\n",
    "6가지의 모델을 Ensemble하여 사용하였습니다. <br>\n",
    "\n",
    "모델: thin_resnet, xception, inception, densenet, vggnet, resnet <br>\n",
    "thin_resnet은 resnet기반으로 만든 조금 작은 모델입니다. <br>\n",
    "나머지 모델들은 keras(tensorflow)의 application을 그대로 사용하였습니다. <br>\n",
    "\n",
    "train dataset으로 학습을 시키고 test dataset을 predict한 결과 중 일정 점수 이상을\n",
    "받은 데이터들을 train dataset에 추가하여 다시 학습하였습니다.\n",
    "\n",
    "Ensemble은 여러 모델의 각 fold로 예측한 값의 root sum을 사용하였고, <br>\n",
    "Loss는 mse, optimizer는 adam을 사용하였습니다. <br>\n",
    "\n",
    "Data Augmentation은 keras의 RandomRotation과 직접 구현한 RandomRoll을 사용하였습니다.\n",
    "keras의 RandomTranslation은 성능이 좋지 않아 사용하지 않았습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "D9sOjZhGZlLC",
    "outputId": "59582597-8413-4a0c-96de-eb49e433c3c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug 29 18:29:45 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 452.06       Driver Version: 452.06       CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 2060   WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   54C    P8    11W / 190W |    731MiB /  6144MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      3448    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A      6788    C+G   ..._dt26b99r8h8gj\\RtkUWP.exe    N/A      |\n",
      "|    0   N/A  N/A      8180    C+G   ...nputApp\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     11300    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     12212    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13904    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     14552    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A     17104    C+G   ...in7x64\\steamwebhelper.exe    N/A      |\n",
      "|    0   N/A  N/A     17684    C+G   ...Battle.net\\Battle.net.exe    N/A      |\n",
      "|    0   N/A  N/A     18868    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     21200    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     22640    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ysAu28WeZmLp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import shutil\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from albumentations import Compose, RandomBrightness, JpegCompression, HueSaturationValue, RandomContrast, HorizontalFlip, Rotate, RandomCrop, RandomBrightnessContrast\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, EarlyStopping\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D, Permute\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout, Reshape, Multiply, Conv2D, MaxPool2D, LSTM, Add, Lambda, AveragePooling2D\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D, LeakyReLU, PReLU, GlobalAveragePooling2D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling, RandomTranslation, RandomRotation, RandomZoom\n",
    "from tensorflow_addons.layers.netvlad import NetVLAD\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier \n",
    "from tensorflow.keras.applications import ResNet50, ResNet152\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.applications import EfficientNetB7, EfficientNetB4, EfficientNetB2, EfficientNetB1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wo9-NVPdZyWB"
   },
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv('data/train.csv')\n",
    "test_csv = pd.read_csv('data/test.csv')\n",
    "submission_csv = pd.read_csv('data/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "colab_type": "code",
    "id": "MqvhCXr4fSMT",
    "outputId": "0310decc-810d-4ed7-d213-b7aa0638153b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>digit</th>\n",
       "      <th>letter</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>774</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>L</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>L</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043</th>\n",
       "      <td>2044</td>\n",
       "      <td>6</td>\n",
       "      <td>V</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044</th>\n",
       "      <td>2045</td>\n",
       "      <td>1</td>\n",
       "      <td>L</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2045</th>\n",
       "      <td>2046</td>\n",
       "      <td>9</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2046</th>\n",
       "      <td>2047</td>\n",
       "      <td>0</td>\n",
       "      <td>Z</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2047</th>\n",
       "      <td>2048</td>\n",
       "      <td>5</td>\n",
       "      <td>Z</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2048 rows × 787 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  digit letter  0  1  2  3  4  5  6  ...  774  775  776  777  778  \\\n",
       "0        1      5      L  1  1  1  4  3  0  0  ...    2    1    0    1    2   \n",
       "1        2      0      B  0  4  0  0  4  1  1  ...    0    3    0    1    4   \n",
       "2        3      4      L  1  1  2  2  1  1  1  ...    3    3    3    0    2   \n",
       "3        4      9      D  1  2  0  2  0  4  0  ...    3    3    2    0    1   \n",
       "4        5      6      A  3  0  2  4  0  3  0  ...    4    4    3    2    1   \n",
       "...    ...    ...    ... .. .. .. .. .. .. ..  ...  ...  ...  ...  ...  ...   \n",
       "2043  2044      6      V  2  4  3  4  2  4  4  ...    0    2    2    0    0   \n",
       "2044  2045      1      L  3  2  2  1  1  4  0  ...    2    3    4    2    1   \n",
       "2045  2046      9      A  4  0  4  0  2  4  4  ...    2    3    1    1    3   \n",
       "2046  2047      0      Z  2  3  3  0  3  0  4  ...    2    3    1    1    0   \n",
       "2047  2048      5      Z  4  2  2  1  3  0  0  ...    4    2    4    0    4   \n",
       "\n",
       "      779  780  781  782  783  \n",
       "0       4    4    4    3    4  \n",
       "1       1    4    2    1    2  \n",
       "2       0    3    0    2    2  \n",
       "3       4    0    0    1    1  \n",
       "4       3    4    3    1    2  \n",
       "...   ...  ...  ...  ...  ...  \n",
       "2043    1    3    1    4    0  \n",
       "2044    2    3    4    1    1  \n",
       "2045    4    2    2    0    0  \n",
       "2046    4    1    4    3    1  \n",
       "2047    3    2    4    3    4  \n",
       "\n",
       "[2048 rows x 787 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "s7Dhk8WMcFK6",
    "outputId": "7da4f378-c1ed-4320-d658-af4253e49bf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 26) float32\n",
      "(2048, 10) float32\n",
      "(2048, 28, 28, 1) float32\n",
      "(20480, 26) float32\n",
      "(20480, 28, 28, 1) float32\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# TRAIN\n",
    "train_letters_char = train_csv['letter'].values\n",
    "train_letters_int = label_encoder.fit_transform(train_letters_char)\n",
    "train_letters = keras.utils.to_categorical(train_letters_int)\n",
    "\n",
    "train_pixels = train_csv.loc[:, '0':'783'].values\n",
    "train_pixels = train_pixels.astype(np.float32)\n",
    "train_pixels = train_pixels.reshape((-1, 28, 28, 1))\n",
    "\n",
    "train_digits_int = train_csv['digit'].values\n",
    "train_digits = keras.utils.to_categorical(train_digits_int)\n",
    "\n",
    "# TEST\n",
    "test_letters_char = test_csv['letter'].values\n",
    "test_letters_int = label_encoder.fit_transform(test_letters_char)\n",
    "test_letters = keras.utils.to_categorical(test_letters_int)\n",
    "\n",
    "test_pixels = test_csv.loc[:, '0':'783'].values\n",
    "test_pixels = test_pixels.astype(np.float32)\n",
    "test_pixels = test_pixels.reshape((-1, 28, 28, 1))\n",
    "\n",
    "print(train_letters.shape, train_letters.dtype)\n",
    "print(train_digits.shape, train_digits.dtype)\n",
    "print(train_pixels.shape, train_pixels.dtype)\n",
    "\n",
    "print(test_letters.shape, test_letters.dtype)\n",
    "print(test_pixels.shape, test_pixels.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 954
    },
    "colab_type": "code",
    "id": "kPJzgqhmijB8",
    "outputId": "57a02872-f6c8-4ed2-f042-a2d9af2d465b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams.update({'font.size': 17})\n",
    "fig, axs = plt.subplots(4, 4)\n",
    "fig.suptitle('image samples')\n",
    "fig.set_size_inches(15, 15)\n",
    "\n",
    "# train_pixels[train_pixels < 0.6] = 0\n",
    "\n",
    "for i in range(len(axs)):\n",
    "    for j in range(len(axs[i])):\n",
    "        img = train_pixels[i * 4 + j].reshape((28, 28))\n",
    "        # img[img < 150] = 0.0\n",
    "        # img[img >= 150] = 1.0\n",
    "        axs[i][j].imshow(img, cmap='Greys')\n",
    "        axs[i][j].axis('off')\n",
    "        axs[i][j].title.set_text(f'{train_digits_int[i * 4 + j]}  {train_letters_char[i * 4 + j]}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 954
    },
    "colab_type": "code",
    "id": "5_90aBN4_Qvw",
    "outputId": "2abd760e-84d3-4d54-b1d7-f331338c6de1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams.update({'font.size': 17})\n",
    "fig, axs = plt.subplots(4, 4)\n",
    "fig.suptitle('image samples')\n",
    "fig.set_size_inches(15, 15)\n",
    "\n",
    "# train_pixels[train_pixels < 0.6] = 0\n",
    "\n",
    "for i in range(len(axs)):\n",
    "    for j in range(len(axs[i])):\n",
    "        img = test_pixels[i * 4 + j].reshape((28, 28))\n",
    "        # img[img < 150] = 0\n",
    "        # img[img >= 150] = 1\n",
    "        axs[i][j].imshow(img, cmap='Greys')\n",
    "        axs[i][j].axis('off')\n",
    "        axs[i][j].title.set_text(f'{test_letters_char[i * 4 + j]}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "xgbi9eOyf8vY",
    "outputId": "3d5d9a98-8118-425a-fa67-663d8f90432e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26,)\n",
      "(28, 28, 1)\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "input_shape_1 = train_letters.shape[1:]\n",
    "input_shape_2 = train_pixels.shape[1:]\n",
    "output_size = train_digits.shape[1]\n",
    "\n",
    "print(input_shape_1)\n",
    "print(input_shape_2)\n",
    "print(output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N3qy0vD3WM6D"
   },
   "outputs": [],
   "source": [
    "class RandomRollLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"Shift data\"\"\"\n",
    "\n",
    "    def __init__(self, roll_limit=0.1, u=0.5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.roll_limit = roll_limit\n",
    "        self.u = u\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.non_trainable_weights.append(self.roll_limit)\n",
    "        self.non_trainable_weights.append(self.u)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, pixels, training=None):\n",
    "        if training is None:\n",
    "            training = K.learning_phase()\n",
    "\n",
    "        if not training:\n",
    "            return pixels\n",
    "\n",
    "        if tf.random.uniform(shape=[]) < self.u:\n",
    "            roll_limit = self.roll_limit * pixels.shape[1]\n",
    "            roll_limit = tf.cast(roll_limit, tf.int32)\n",
    "            roll = tf.random.uniform(shape=[], minval=-roll_limit, maxval=roll_limit, dtype=tf.int32)\n",
    "\n",
    "            pixels = tf.roll(pixels, shift=roll, axis=1)\n",
    "\n",
    "        if tf.random.uniform(shape=[]) < self.u:\n",
    "            roll_limit = self.roll_limit * pixels.shape[2]\n",
    "            roll_limit = tf.cast(roll_limit, tf.int32)\n",
    "            roll = tf.random.uniform(shape=[], minval=-roll_limit, maxval=roll_limit, dtype=tf.int32)\n",
    "\n",
    "            pixels = tf.roll(pixels, shift=roll, axis=2)\n",
    "            \n",
    "        return pixels\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'roll_limit': self.roll_limit,\n",
    "            'u': self.u,\n",
    "        }\n",
    "        config.update(super().get_config())\n",
    "\n",
    "        return config\n",
    "\n",
    "class RandomAddLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"Add data\"\"\"\n",
    "\n",
    "    def __init__(self, add_limit=0.1, u=0.5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.add_limit = add_limit\n",
    "        self.u = u\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.non_trainable_weights.append(self.add_limit)\n",
    "        self.non_trainable_weights.append(self.u)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, pixels, training=None):\n",
    "        if training is None:\n",
    "            training = K.learning_phase()\n",
    "\n",
    "        if not training:\n",
    "            return pixels\n",
    "\n",
    "        if tf.random.uniform(shape=[]) < self.u:\n",
    "            add = tf.random.uniform(shape=[], minval=-self.add_limit, maxval=self.add_limit, dtype=tf.float32)\n",
    "            pixels = pixels + add\n",
    "\n",
    "        return pixels\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'add_limit': self.add_limit,\n",
    "            'u': self.u,\n",
    "        }\n",
    "        config.update(super().get_config())\n",
    "\n",
    "        return config\n",
    "\n",
    "class RandomMultipleLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"Multiple data\"\"\"\n",
    "\n",
    "    def __init__(self, multiple_limit=0.5, u=0.5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.multiple_limit = multiple_limit\n",
    "        self.u = u\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.non_trainable_weights.append(self.multiple_limit)\n",
    "        self.non_trainable_weights.append(self.u)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, pixels, training=None):\n",
    "        if training is None:\n",
    "            training = K.learning_phase()\n",
    "\n",
    "        if not training:\n",
    "            return pixels\n",
    "\n",
    "        if tf.random.uniform(shape=[]) < self.u:\n",
    "            multiple = tf.random.uniform(shape=[], minval=-self.multiple_limit, maxval=self.multiple_limit, dtype=tf.float32)\n",
    "            pixels = pixels * (1 + multiple)\n",
    "\n",
    "        return pixels\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'multiple_limit': self.multiple_limit,\n",
    "            'u': self.u,\n",
    "        }\n",
    "        config.update(super().get_config())\n",
    "\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GSnA5KoJeMRr"
   },
   "outputs": [],
   "source": [
    "def thin_resnet_model(input_shape_1, input_shape_2, output_size=10, num_clusters=10):\n",
    "    input_1 = Input(shape=input_shape_1)  # letter\n",
    "    input_2 = Input(shape=input_shape_2)  # pixel\n",
    "\n",
    "    y = input_2\n",
    "    y = Rescaling(scale=1.0/127.5, offset=-1.0)(y)\n",
    "    y = RandomRollLayer(roll_limit=0.2, u=0.8)(y)\n",
    "    y = RandomRotation(factor=(-0.1, 0.1), fill_mode='constant')(y)\n",
    "\n",
    "    # y = RandomTranslation(0.1, 0.1)(y)\n",
    "    # y = RandomRotation(0.1)(y)\n",
    "    # y = RandomZoom(0.1)(y)\n",
    "\n",
    "    # CONV 1\n",
    "    y = Conv2D(64, (7, 7), padding='same')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = MaxPool2D((2, 2), strides=(2, 2))(y)\n",
    "\n",
    "    # CONV 2 - 1\n",
    "    y1 = Conv2D(48, (1, 1), padding='valid')(y)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(48, (3, 3), padding='same')(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(96, (1, 1), padding='valid')(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    \n",
    "    y2 = Conv2D(96, (1, 1), padding='valid')(y)\n",
    "    y2 = BatchNormalization()(y2)\n",
    "    y = Add()([y1, y2])\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    # CONV 2 - 2\n",
    "    y1 = Conv2D(48, (1, 1), padding='valid')(y)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(48, (3, 3), padding='same')(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(96, (1, 1), padding='valid')(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    \n",
    "    y = Add()([y1, y])\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "\n",
    "    # CONV 3 - 1\n",
    "    y1 = Conv2D(96, (1, 1), padding='valid', strides=(2, 2))(y)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(96, (3, 3), padding='same')(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(128, (1, 1), padding='valid')(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "\n",
    "    y2 = AveragePooling2D((2, 2), strides=2, padding='same')(y)\n",
    "    y2 = Conv2D(128, (1, 1), padding='valid')(y2)\n",
    "    # y2 = Conv2D(128, (1, 1), padding='valid', strides=(2, 2))(y)\n",
    "    y2 = BatchNormalization()(y2)\n",
    "    y = Add()([y1, y2])\n",
    "    y = Activation('relu')(y)\n",
    "  \n",
    "    # CONV 3 - 2\n",
    "    y1 = Conv2D(96, (1, 1), padding='valid')(y)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(96, (3, 3), padding='same')(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(128, (1, 1), padding='valid')(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "\n",
    "    y = Add()([y1, y])\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    # CONV 4 - 1\n",
    "    y1 = Conv2D(128, (1, 1), padding='valid', strides=(2, 2))(y)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(128, (3, 3), padding='same')(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(256, (1, 1), padding='valid')(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "\n",
    "    # y2 = Conv2D(256, (1, 1), padding='valid', strides=(2, 2))(y)\n",
    "    y2 = AveragePooling2D((2, 2), strides=2, padding='same')(y)\n",
    "    y2 = Conv2D(256, (1, 1), padding='valid')(y2)\n",
    "    y2 = BatchNormalization()(y2)\n",
    "    y = Add()([y1, y2])\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    # CONV 4 - 2\n",
    "    y1 = Conv2D(128, (1, 1), padding='valid')(y)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(128, (3, 3), padding='same')(y1)\n",
    "    y1 = BatchNormalization(gamma_initializer='zeros')(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(256, (1, 1), padding='valid')(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "\n",
    "    y = Add()([y1, y])\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    # CONV 5 - 1\n",
    "    y1 = Conv2D(256, (1, 1), padding='valid', strides=(2, 2))(y)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(256, (3, 3), padding='same')(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(512, (1, 1), padding='valid')(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "\n",
    "    # y2 = Conv2D(512, (1, 1), padding='valid', strides=(2, 2))(y)\n",
    "    y2 = AveragePooling2D((2, 2), strides=2, padding='same')(y)\n",
    "    y2 = Conv2D(512, (1, 1), padding='valid')(y2)\n",
    "    y2 = BatchNormalization()(y2)\n",
    "    y = Add()([y1, y2])\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    # CONV 5 - 2\n",
    "    y1 = Conv2D(256, (1, 1), padding='valid')(y)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(256, (3, 3), padding='same')(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation('relu')(y1)\n",
    "    y1 = Conv2D(512, (1, 1), padding='valid')(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "\n",
    "    y = Add()([y1, y])\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    # CONV 6\n",
    "    y = Conv2D(512, (2, 2), padding='valid')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    # y = Reshape((-1, 512))(y)\n",
    "    # y = Flatten()(y)\n",
    "    y = GlobalAveragePooling2D()(y)\n",
    "    # y = NetVLAD(num_clusters=num_clusters)(y)\n",
    "    y = tf.keras.layers.Concatenate(axis=1)([y, input_1])\n",
    "    y = Dropout(0.3)(y)\n",
    "\n",
    "    y = Dense(output_size)(y)\n",
    "    y = Activation('softmax')(y)\n",
    "    output = y\n",
    "\n",
    "    model = Model(inputs=[input_1, input_2], outputs=output, name='thin_resnet_model')\n",
    "    # optimizer = tfa.optimizers.AdamW(learning_rate=0.05, weight_decay=0.0001)\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cwgwof2-PEeq"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG19\n",
    "\n",
    "\n",
    "def vggnet_model(input_shape_1, input_shape_2, output_size=10):\n",
    "    input_1 = Input(shape=input_shape_1)  # letter\n",
    "    input_2 = Input(shape=input_shape_2)  # pixel\n",
    "\n",
    "    y = input_2\n",
    "\n",
    "    y = Rescaling(scale=1.0/127.5, offset=-1.0)(y)\n",
    "    y = tf.keras.layers.experimental.preprocessing.Resizing(height=56, width=56)(y)\n",
    "    y = RandomRollLayer(roll_limit=0.2, u=0.8)(y)\n",
    "   \n",
    "\n",
    "    model = VGG19(include_top=False, input_tensor=y, pooling='max', input_shape=y.shape[1:], weights=None)\n",
    "\n",
    "    y = model.output\n",
    "    y = Dense(1024)(y)\n",
    "    y = Dropout(0.25)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = tf.keras.layers.Concatenate(axis=1)([y, input_1])\n",
    "    y = Dense(256)(y)\n",
    "    y = Dropout(0.25)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = Dense(output_size, activation='softmax', name='softmax')(y)\n",
    "\n",
    "    output = y\n",
    "\n",
    "    model = Model(inputs=[input_1, input_2], outputs=output, name='vggnet_model')\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oqd8ur_AT4_y"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet101V2\n",
    "\n",
    "\n",
    "def resnet_model(input_shape_1, input_shape_2, output_size=10):\n",
    "    input_1 = Input(shape=input_shape_1)  # letter\n",
    "    input_2 = Input(shape=input_shape_2)  # pixel\n",
    "\n",
    "    y = input_2\n",
    "\n",
    "    y = Rescaling(scale=1.0/127.5, offset=-1.0)(y)\n",
    "    y = tf.keras.layers.experimental.preprocessing.Resizing(height=56, width=56)(y)\n",
    "    y = RandomRollLayer(roll_limit=0.2, u=0.8)(y)\n",
    "   \n",
    "\n",
    "    model = ResNet101V2(include_top=False, input_tensor=y, pooling='max', input_shape=y.shape[1:], weights=None)\n",
    "\n",
    "    y = model.output\n",
    "    y = Dense(1024)(y)\n",
    "    y = Dropout(0.25)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = tf.keras.layers.Concatenate(axis=1)([y, input_1])\n",
    "    y = Dense(256)(y)\n",
    "    y = Dropout(0.25)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = Dense(output_size, activation='softmax', name='softmax')(y)\n",
    "\n",
    "    output = y\n",
    "\n",
    "    model = Model(inputs=[input_1, input_2], outputs=output, name='resnet_model')\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fIwajPzbqrHB"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import DenseNet121\n",
    "\n",
    "\n",
    "def densenet_model(input_shape_1, input_shape_2, output_size=10):\n",
    "    input_1 = Input(shape=input_shape_1)  # letter\n",
    "    input_2 = Input(shape=input_shape_2)  # pixel\n",
    "\n",
    "    y = input_2\n",
    "\n",
    "    y = Rescaling(scale=1.0/127.5, offset=-1.0)(y)\n",
    "    y = tf.keras.layers.experimental.preprocessing.Resizing(height=112, width=112)(y)\n",
    "    y = RandomRollLayer(roll_limit=0.2, u=0.8)(y)\n",
    "    y = RandomRotation(factor=(-0.1, 0.1), fill_mode='constant')(y)\n",
    "\n",
    "    model = DenseNet121(include_top=False, input_tensor=y, input_shape=y.shape[1:], weights=None, pooling='max')\n",
    "\n",
    "    y = model.output\n",
    "    y = Dense(1024)(y)\n",
    "    y = Dropout(0.25)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = tf.keras.layers.Concatenate(axis=1)([y, input_1])\n",
    "    y = Dense(256)(y)\n",
    "    y = Dropout(0.25)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = Dense(output_size, activation='softmax', name='softmax')(y)\n",
    "    output = y\n",
    "\n",
    "    model = Model(inputs=[input_1, input_2], outputs=output, name='densenet_model')\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nP9RUXuownR_"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import Xception\n",
    "\n",
    "\n",
    "def xception_model(input_shape_1, input_shape_2, output_size=10):\n",
    "    input_1 = Input(shape=input_shape_1)  # letter\n",
    "    input_2 = Input(shape=input_shape_2)  # pixel\n",
    "\n",
    "    y = input_2\n",
    "\n",
    "    y = Rescaling(scale=1.0/127.5, offset=-1.0)(y)\n",
    "    y = tf.keras.layers.experimental.preprocessing.Resizing(height=112, width=112)(y)\n",
    "    y = RandomRollLayer(roll_limit=0.2, u=0.8)(y)\n",
    "    y = RandomRotation(factor=(-0.1, 0.1), fill_mode='constant')(y)\n",
    "\n",
    "    model = Xception(include_top=False, input_tensor=y, input_shape=y.shape[1:], weights=None, pooling='max')\n",
    "\n",
    "    y = model.output\n",
    "    y = Dense(1024)(y)\n",
    "    y = Dropout(0.25)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = tf.keras.layers.Concatenate(axis=1)([y, input_1])\n",
    "    y = Dense(256)(y)\n",
    "    y = Dropout(0.25)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = Dense(output_size, activation='softmax', name='softmax')(y)\n",
    "    output = y\n",
    "\n",
    "    model = Model(inputs=[input_1, input_2], outputs=output, name='xception_model')\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dHe_a-GIYuZJ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import InceptionV3\n",
    "\n",
    "\n",
    "def inception_model(input_shape_1, input_shape_2, output_size=10):\n",
    "    input_1 = Input(shape=input_shape_1)  # letter\n",
    "    input_2 = Input(shape=input_shape_2)  # pixel\n",
    "\n",
    "    y = input_2\n",
    "\n",
    "    y = Rescaling(scale=1.0/127.5, offset=-1.0)(y)\n",
    "    y = tf.keras.layers.experimental.preprocessing.Resizing(height=112, width=112)(y)\n",
    "    y = RandomRollLayer(roll_limit=0.2, u=0.8)(y)\n",
    "    y = RandomRotation(factor=(-0.1, 0.1), fill_mode='constant')(y)\n",
    "\n",
    "    model = InceptionV3(include_top=False, input_tensor=y, input_shape=y.shape[1:], weights=None, pooling='max')\n",
    "\n",
    "    y = model.output\n",
    "    y = Dense(1024)(y)\n",
    "    y = Dropout(0.25)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = tf.keras.layers.Concatenate(axis=1)([y, input_1])\n",
    "    y = Dense(256)(y)\n",
    "    y = Dropout(0.25)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = Dense(output_size, activation='softmax', name='softmax')(y)\n",
    "    output = y\n",
    "\n",
    "    model = Model(inputs=[input_1, input_2], outputs=output, name='inception_model')\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SyC2GYA0H776"
   },
   "outputs": [],
   "source": [
    "def k_fold_validation(model_fn, n_splits=5, verbose=1):\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    sum_accuracy = 0\n",
    "    sum_epoch = 0\n",
    "    start_time = time.time()\n",
    "    for i, (train_index, val_index) in enumerate(kf.split(train_digits)):\n",
    "        model = model_fn(input_shape_1=input_shape_1, input_shape_2=input_shape_2, output_size=output_size)\n",
    "\n",
    "        train_data = [train_letters[train_index], train_pixels[train_index]]\n",
    "        train_label = train_digits[train_index]\n",
    "\n",
    "        val_data = [train_letters[val_index], train_pixels[val_index]]\n",
    "        val_label = train_digits[val_index]\n",
    "\n",
    "        history = model.fit(\n",
    "            train_data, train_label,\n",
    "            epochs=EPOCHS,\n",
    "            validation_data=(val_data, val_label),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            verbose=verbose,\n",
    "            callbacks=[early_stopping],\n",
    "        )\n",
    "\n",
    "        sum_epoch += len(history.history['val_accuracy'])\n",
    "        sum_accuracy += max(history.history['val_accuracy'])\n",
    "        \n",
    "        if verbose >= 0:            \n",
    "            print(f'{i+1}/{n_splits} fold result: ')\n",
    "            print('epoch num:', len(history.history['val_accuracy']))\n",
    "            print('best val accuracy: ', max(history.history['val_accuracy']))\n",
    "            print('average 20: ', np.mean(history.history['val_accuracy'][-20:]))\n",
    "            print('='*50)\n",
    "\n",
    "    print('Average Accuracy: ', sum_accuracy/n_splits)\n",
    "    print('Average Epoch: ', sum_epoch/n_splits)\n",
    "    print('Time taken: ', time.time() - start_time)\n",
    "    print('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "k_fold_validation(resnet_model, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 실험 결과\n",
    "\n",
    "### Augumentation\n",
    "No Augumentation -> 0.69\n",
    "RandomRollLayer(roll_limit=0.2, u=0.8)(y) -> 0.835\n",
    "Roll + ResNet-D -> 0.824\n",
    "RandomTranslation(height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2), fill_mode='constant')(y) -> 0.768\n",
    "RandomRotation(factor=(-0.2, 0.2), fill_mode='constant')(y) -> 0.731\n",
    "RandomAddLayer(add_limit=0.1, u=0.8)(y) -> 0.751\n",
    "RandomZoom(height_factor=(0, 0.2), width_factor=(0, 0.2), fill_mode='reflect')(y) -> 0.689\n",
    "\n",
    "### Model\n",
    "thin_resnet_model -> 0.835\n",
    "xception_model -> 0.894\n",
    "inception_model -> 0.886\n",
    "densenet_model -> 0.885\n",
    "vggnet_model -> 0.833\n",
    "resnet_model -> 0.840\n",
    "\n",
    "### Loss\n",
    "RMSProp + categorical_crossentropy -> 0.823\n",
    "\n",
    "### Batch size\n",
    "smaller is better"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zzTmAAqGim7E"
   },
   "outputs": [],
   "source": [
    "model_fn_list = [\n",
    "    thin_resnet_model,\n",
    "    vggnet_model,\n",
    "    resnet_model,\n",
    "    densenet_model,\n",
    "    xception_model,\n",
    "    inception_model\n",
    "]\n",
    "\n",
    "EPOCHS = 300\n",
    "BATCH_SIZE = 16\n",
    "CHECKPOINT_PATH = 'checkpoint/'\n",
    "MODEL_PATH = 'model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mv9vgzQAhymY"
   },
   "outputs": [],
   "source": [
    "if os.path.isdir(CHECKPOINT_PATH):\n",
    "    shutil.rmtree(CHECKPOINT_PATH, ignore_errors=True)\n",
    "os.mkdir(CHECKPOINT_PATH)\n",
    "\n",
    "if os.path.isdir(MODEL_PATH):\n",
    "    shutil.rmtree(MODEL_PATH, ignore_errors=True)\n",
    "os.mkdir(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "colab_type": "code",
    "id": "U3JubR0CjBp1",
    "outputId": "34b357ad-e23b-43e7-cd00-813529f24dd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0873 - accuracy: 0.2726 - val_loss: 0.0984 - val_accuracy: 0.2552\n",
      "Epoch 2/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0748 - accuracy: 0.4154 - val_loss: 0.0779 - val_accuracy: 0.4170\n",
      "Epoch 3/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0629 - accuracy: 0.5301 - val_loss: 0.0449 - val_accuracy: 0.6753\n",
      "Epoch 4/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0475 - accuracy: 0.6573 - val_loss: 0.0995 - val_accuracy: 0.3034\n",
      "Epoch 5/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0386 - accuracy: 0.7234 - val_loss: 0.0368 - val_accuracy: 0.7410\n",
      "Epoch 6/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0335 - accuracy: 0.7650 - val_loss: 0.0376 - val_accuracy: 0.7349\n",
      "Epoch 7/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0293 - accuracy: 0.7970 - val_loss: 0.0544 - val_accuracy: 0.6116\n",
      "Epoch 8/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0255 - accuracy: 0.8243 - val_loss: 0.0292 - val_accuracy: 0.8044\n",
      "Epoch 9/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0231 - accuracy: 0.8429 - val_loss: 0.0694 - val_accuracy: 0.5211\n",
      "Epoch 10/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0224 - accuracy: 0.8494 - val_loss: 0.0377 - val_accuracy: 0.7423\n",
      "Epoch 11/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0202 - accuracy: 0.8641 - val_loss: 0.0757 - val_accuracy: 0.5005\n",
      "Epoch 12/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0193 - accuracy: 0.8685 - val_loss: 0.0276 - val_accuracy: 0.8105\n",
      "Epoch 13/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0181 - accuracy: 0.8787 - val_loss: 0.0166 - val_accuracy: 0.8881\n",
      "Epoch 14/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0170 - accuracy: 0.8863 - val_loss: 0.0271 - val_accuracy: 0.8130\n",
      "Epoch 15/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0163 - accuracy: 0.8903 - val_loss: 0.0148 - val_accuracy: 0.9039\n",
      "Epoch 16/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0162 - accuracy: 0.8904 - val_loss: 0.0374 - val_accuracy: 0.7463\n",
      "Epoch 17/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0150 - accuracy: 0.8997 - val_loss: 0.0358 - val_accuracy: 0.7689\n",
      "Epoch 18/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0156 - accuracy: 0.8938 - val_loss: 0.0171 - val_accuracy: 0.8853\n",
      "Epoch 19/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0141 - accuracy: 0.9072 - val_loss: 0.0165 - val_accuracy: 0.8902\n",
      "Epoch 20/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0143 - accuracy: 0.9050 - val_loss: 0.0150 - val_accuracy: 0.8998\n",
      "Epoch 21/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0137 - accuracy: 0.9073 - val_loss: 0.0342 - val_accuracy: 0.7714\n",
      "Epoch 22/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0132 - accuracy: 0.9102 - val_loss: 0.0176 - val_accuracy: 0.8848\n",
      "Epoch 23/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0126 - accuracy: 0.9165 - val_loss: 0.0479 - val_accuracy: 0.6730\n",
      "Epoch 24/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0129 - accuracy: 0.9141 - val_loss: 0.0135 - val_accuracy: 0.9105\n",
      "Epoch 25/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0125 - accuracy: 0.9170 - val_loss: 0.0357 - val_accuracy: 0.7692\n",
      "Epoch 26/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0120 - accuracy: 0.9202 - val_loss: 0.0142 - val_accuracy: 0.9077\n",
      "Epoch 27/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0113 - accuracy: 0.9262 - val_loss: 0.0120 - val_accuracy: 0.9226\n",
      "Epoch 28/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0120 - accuracy: 0.9209 - val_loss: 0.0124 - val_accuracy: 0.9173\n",
      "Epoch 29/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0110 - accuracy: 0.9281 - val_loss: 0.0195 - val_accuracy: 0.8699\n",
      "Epoch 30/300\n",
      "986/986 [==============================] - 14s 14ms/step - loss: 0.0111 - accuracy: 0.9264 - val_loss: 0.0101 - val_accuracy: 0.9363\n",
      "Epoch 31/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0113 - accuracy: 0.9249 - val_loss: 0.0178 - val_accuracy: 0.8836\n",
      "Epoch 32/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0106 - accuracy: 0.9300 - val_loss: 0.0108 - val_accuracy: 0.9307\n",
      "Epoch 33/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0102 - accuracy: 0.9331 - val_loss: 0.0150 - val_accuracy: 0.9011\n",
      "Epoch 34/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0106 - accuracy: 0.9299 - val_loss: 0.0264 - val_accuracy: 0.8305\n",
      "Epoch 35/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0105 - accuracy: 0.9298 - val_loss: 0.0170 - val_accuracy: 0.8889\n",
      "Epoch 36/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0098 - accuracy: 0.9363 - val_loss: 0.0351 - val_accuracy: 0.7697\n",
      "Epoch 37/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0096 - accuracy: 0.9370 - val_loss: 0.0201 - val_accuracy: 0.8673\n",
      "Epoch 38/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0099 - accuracy: 0.9344 - val_loss: 0.0218 - val_accuracy: 0.8513\n",
      "Epoch 39/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0100 - accuracy: 0.9339 - val_loss: 0.0115 - val_accuracy: 0.9234\n",
      "Epoch 40/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0089 - accuracy: 0.9406 - val_loss: 0.0091 - val_accuracy: 0.9396\n",
      "Epoch 41/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0092 - accuracy: 0.9406 - val_loss: 0.0134 - val_accuracy: 0.9132\n",
      "Epoch 42/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0092 - accuracy: 0.9399 - val_loss: 0.0172 - val_accuracy: 0.8869\n",
      "Epoch 43/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0092 - accuracy: 0.9403 - val_loss: 0.0111 - val_accuracy: 0.9277\n",
      "Epoch 44/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0086 - accuracy: 0.9428 - val_loss: 0.0114 - val_accuracy: 0.9280\n",
      "Epoch 45/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0090 - accuracy: 0.9414 - val_loss: 0.0137 - val_accuracy: 0.9125\n",
      "Epoch 46/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0088 - accuracy: 0.9424 - val_loss: 0.0116 - val_accuracy: 0.9259\n",
      "Epoch 47/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0085 - accuracy: 0.9456 - val_loss: 0.0110 - val_accuracy: 0.9310\n",
      "Epoch 48/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0084 - accuracy: 0.9447 - val_loss: 0.0112 - val_accuracy: 0.9285\n",
      "Epoch 49/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0085 - accuracy: 0.9443 - val_loss: 0.0121 - val_accuracy: 0.9198\n",
      "Epoch 50/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0082 - accuracy: 0.9467 - val_loss: 0.0100 - val_accuracy: 0.9346\n",
      "==================================================\n",
      "Result of thin_resnet_model, fold 1\n",
      "Epoch: 50\n",
      "Accuracy: 0.939624547958374\n",
      "Time taken:  623.5998275279999\n",
      "==================================================\n",
      "Epoch 1/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0872 - accuracy: 0.2719 - val_loss: 0.0740 - val_accuracy: 0.3962\n",
      "Epoch 2/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0794 - accuracy: 0.3786 - val_loss: 0.0576 - val_accuracy: 0.5652\n",
      "Epoch 3/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0671 - accuracy: 0.4900 - val_loss: 0.0479 - val_accuracy: 0.6575\n",
      "Epoch 4/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0523 - accuracy: 0.6147 - val_loss: 0.0315 - val_accuracy: 0.7755\n",
      "Epoch 5/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0440 - accuracy: 0.6839 - val_loss: 0.0626 - val_accuracy: 0.5211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0367 - accuracy: 0.7407 - val_loss: 0.0904 - val_accuracy: 0.4094\n",
      "Epoch 7/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0318 - accuracy: 0.7770 - val_loss: 0.0583 - val_accuracy: 0.6106\n",
      "Epoch 8/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0286 - accuracy: 0.8025 - val_loss: 0.0187 - val_accuracy: 0.8765\n",
      "Epoch 9/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0253 - accuracy: 0.8274 - val_loss: 0.0175 - val_accuracy: 0.8795\n",
      "Epoch 10/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0238 - accuracy: 0.8354 - val_loss: 0.0184 - val_accuracy: 0.8752\n",
      "Epoch 11/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0223 - accuracy: 0.8456 - val_loss: 0.0204 - val_accuracy: 0.8617\n",
      "Epoch 12/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0211 - accuracy: 0.8571 - val_loss: 0.0163 - val_accuracy: 0.8929\n",
      "Epoch 13/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0195 - accuracy: 0.8686 - val_loss: 0.0146 - val_accuracy: 0.9031\n",
      "Epoch 14/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0198 - accuracy: 0.8654 - val_loss: 0.0141 - val_accuracy: 0.9051\n",
      "Epoch 15/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0190 - accuracy: 0.8708 - val_loss: 0.0159 - val_accuracy: 0.8932\n",
      "Epoch 16/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0173 - accuracy: 0.8826 - val_loss: 0.0103 - val_accuracy: 0.9358\n",
      "Epoch 17/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0169 - accuracy: 0.8874 - val_loss: 0.0187 - val_accuracy: 0.8727\n",
      "Epoch 18/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0162 - accuracy: 0.8916 - val_loss: 0.0147 - val_accuracy: 0.9044\n",
      "Epoch 19/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0159 - accuracy: 0.8936 - val_loss: 0.0151 - val_accuracy: 0.8970\n",
      "Epoch 20/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0152 - accuracy: 0.8997 - val_loss: 0.0162 - val_accuracy: 0.8950\n",
      "Epoch 21/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0153 - accuracy: 0.8981 - val_loss: 0.0092 - val_accuracy: 0.9373\n",
      "Epoch 22/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0144 - accuracy: 0.9029 - val_loss: 0.0213 - val_accuracy: 0.8546\n",
      "Epoch 23/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0143 - accuracy: 0.9037 - val_loss: 0.0146 - val_accuracy: 0.9011\n",
      "Epoch 24/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0142 - accuracy: 0.9053 - val_loss: 0.0100 - val_accuracy: 0.9348\n",
      "Epoch 25/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0135 - accuracy: 0.9098 - val_loss: 0.0094 - val_accuracy: 0.9348\n",
      "Epoch 26/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0135 - accuracy: 0.9098 - val_loss: 0.0109 - val_accuracy: 0.9254\n",
      "Epoch 27/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0134 - accuracy: 0.9111 - val_loss: 0.0101 - val_accuracy: 0.9346\n",
      "Epoch 28/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0127 - accuracy: 0.9158 - val_loss: 0.0225 - val_accuracy: 0.8488\n",
      "Epoch 29/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0127 - accuracy: 0.9168 - val_loss: 0.0082 - val_accuracy: 0.9452\n",
      "Epoch 30/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0127 - accuracy: 0.9160 - val_loss: 0.0060 - val_accuracy: 0.9609\n",
      "Epoch 31/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0121 - accuracy: 0.9204 - val_loss: 0.0065 - val_accuracy: 0.9566\n",
      "Epoch 32/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0120 - accuracy: 0.9199 - val_loss: 0.0156 - val_accuracy: 0.8945\n",
      "Epoch 33/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0119 - accuracy: 0.9218 - val_loss: 0.0077 - val_accuracy: 0.9485\n",
      "Epoch 34/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0113 - accuracy: 0.9252 - val_loss: 0.0066 - val_accuracy: 0.9569\n",
      "Epoch 35/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0112 - accuracy: 0.9257 - val_loss: 0.0075 - val_accuracy: 0.9480\n",
      "Epoch 36/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0116 - accuracy: 0.9229 - val_loss: 0.0062 - val_accuracy: 0.9584\n",
      "Epoch 37/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0112 - accuracy: 0.9257 - val_loss: 0.0078 - val_accuracy: 0.9475\n",
      "Epoch 38/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0106 - accuracy: 0.9300 - val_loss: 0.0077 - val_accuracy: 0.9467\n",
      "Epoch 39/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0106 - accuracy: 0.9294 - val_loss: 0.0073 - val_accuracy: 0.9523\n",
      "Epoch 40/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0104 - accuracy: 0.9308 - val_loss: 0.0054 - val_accuracy: 0.9652\n",
      "Epoch 41/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0105 - accuracy: 0.9316 - val_loss: 0.0052 - val_accuracy: 0.9640\n",
      "Epoch 42/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0100 - accuracy: 0.9334 - val_loss: 0.0050 - val_accuracy: 0.9668\n",
      "Epoch 43/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0098 - accuracy: 0.9362 - val_loss: 0.0063 - val_accuracy: 0.9564\n",
      "Epoch 44/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0102 - accuracy: 0.9314 - val_loss: 0.0058 - val_accuracy: 0.9599\n",
      "Epoch 45/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0102 - accuracy: 0.9321 - val_loss: 0.0079 - val_accuracy: 0.9485\n",
      "Epoch 46/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0098 - accuracy: 0.9361 - val_loss: 0.0192 - val_accuracy: 0.8742\n",
      "Epoch 47/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0102 - accuracy: 0.9348 - val_loss: 0.0070 - val_accuracy: 0.9546\n",
      "Epoch 48/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0093 - accuracy: 0.9388 - val_loss: 0.0067 - val_accuracy: 0.9551\n",
      "Epoch 49/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0093 - accuracy: 0.9389 - val_loss: 0.0049 - val_accuracy: 0.9673\n",
      "Epoch 50/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0091 - accuracy: 0.9407 - val_loss: 0.0072 - val_accuracy: 0.9513\n",
      "Epoch 51/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0096 - accuracy: 0.9364 - val_loss: 0.0140 - val_accuracy: 0.9072\n",
      "Epoch 52/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0093 - accuracy: 0.9385 - val_loss: 0.0061 - val_accuracy: 0.9589\n",
      "Epoch 53/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0088 - accuracy: 0.9417 - val_loss: 0.0065 - val_accuracy: 0.9564\n",
      "Epoch 54/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0087 - accuracy: 0.9427 - val_loss: 0.0083 - val_accuracy: 0.9434\n",
      "Epoch 55/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0092 - accuracy: 0.9398 - val_loss: 0.0048 - val_accuracy: 0.9652\n",
      "Epoch 56/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0089 - accuracy: 0.9413 - val_loss: 0.0082 - val_accuracy: 0.9444\n",
      "Epoch 57/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0088 - accuracy: 0.9424 - val_loss: 0.0113 - val_accuracy: 0.9257\n",
      "Epoch 58/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0090 - accuracy: 0.9404 - val_loss: 0.0082 - val_accuracy: 0.9460\n",
      "Epoch 59/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0083 - accuracy: 0.9446 - val_loss: 0.0051 - val_accuracy: 0.9668\n",
      "==================================================\n",
      "Result of thin_resnet_model, fold 2\n",
      "Epoch: 59\n",
      "Accuracy: 0.9672755002975464\n",
      "Time taken:  736.2034697532654\n",
      "==================================================\n",
      "Epoch 1/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0879 - accuracy: 0.2669 - val_loss: 0.0755 - val_accuracy: 0.4140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0767 - accuracy: 0.4026 - val_loss: 0.0966 - val_accuracy: 0.3014\n",
      "Epoch 3/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0647 - accuracy: 0.5125 - val_loss: 0.0895 - val_accuracy: 0.3219\n",
      "Epoch 4/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0530 - accuracy: 0.6133 - val_loss: 0.0963 - val_accuracy: 0.3229\n",
      "Epoch 5/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0442 - accuracy: 0.6859 - val_loss: 0.0467 - val_accuracy: 0.6578\n",
      "Epoch 6/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0373 - accuracy: 0.7376 - val_loss: 0.0335 - val_accuracy: 0.7656\n",
      "Epoch 7/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0321 - accuracy: 0.7757 - val_loss: 0.0204 - val_accuracy: 0.8584\n",
      "Epoch 8/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0276 - accuracy: 0.8086 - val_loss: 0.0178 - val_accuracy: 0.8775\n",
      "Epoch 9/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0262 - accuracy: 0.8205 - val_loss: 0.0702 - val_accuracy: 0.5205\n",
      "Epoch 10/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0235 - accuracy: 0.8409 - val_loss: 0.0270 - val_accuracy: 0.8143\n",
      "Epoch 11/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0225 - accuracy: 0.8465 - val_loss: 0.0161 - val_accuracy: 0.8856\n",
      "Epoch 12/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0208 - accuracy: 0.8574 - val_loss: 0.0244 - val_accuracy: 0.8364\n",
      "Epoch 13/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0202 - accuracy: 0.8618 - val_loss: 0.0145 - val_accuracy: 0.9054\n",
      "Epoch 14/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0187 - accuracy: 0.8745 - val_loss: 0.0147 - val_accuracy: 0.9051\n",
      "Epoch 15/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0184 - accuracy: 0.8751 - val_loss: 0.0138 - val_accuracy: 0.9074\n",
      "Epoch 16/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0173 - accuracy: 0.8848 - val_loss: 0.0137 - val_accuracy: 0.9077\n",
      "Epoch 17/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0176 - accuracy: 0.8810 - val_loss: 0.0072 - val_accuracy: 0.9510\n",
      "Epoch 18/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0165 - accuracy: 0.8904 - val_loss: 0.0108 - val_accuracy: 0.9272\n",
      "Epoch 19/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0153 - accuracy: 0.8976 - val_loss: 0.0372 - val_accuracy: 0.7357\n",
      "Epoch 20/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0159 - accuracy: 0.8947 - val_loss: 0.0370 - val_accuracy: 0.7473\n",
      "Epoch 21/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0148 - accuracy: 0.9013 - val_loss: 0.0234 - val_accuracy: 0.8399\n",
      "Epoch 22/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0147 - accuracy: 0.9027 - val_loss: 0.0133 - val_accuracy: 0.9102\n",
      "Epoch 23/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0146 - accuracy: 0.9025 - val_loss: 0.0157 - val_accuracy: 0.8922\n",
      "Epoch 24/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0141 - accuracy: 0.9080 - val_loss: 0.0084 - val_accuracy: 0.9409\n",
      "Epoch 25/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0138 - accuracy: 0.9082 - val_loss: 0.0107 - val_accuracy: 0.9267\n",
      "Epoch 26/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0129 - accuracy: 0.9144 - val_loss: 0.0118 - val_accuracy: 0.9201\n",
      "Epoch 27/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0129 - accuracy: 0.9150 - val_loss: 0.0057 - val_accuracy: 0.9614\n",
      "Epoch 28/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0131 - accuracy: 0.9122 - val_loss: 0.0198 - val_accuracy: 0.8628\n",
      "Epoch 29/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0127 - accuracy: 0.9140 - val_loss: 0.0072 - val_accuracy: 0.9477\n",
      "Epoch 30/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0124 - accuracy: 0.9167 - val_loss: 0.0088 - val_accuracy: 0.9401\n",
      "Epoch 31/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0117 - accuracy: 0.9233 - val_loss: 0.0056 - val_accuracy: 0.9632\n",
      "Epoch 32/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0120 - accuracy: 0.9211 - val_loss: 0.0167 - val_accuracy: 0.8894\n",
      "Epoch 33/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0119 - accuracy: 0.9217 - val_loss: 0.0069 - val_accuracy: 0.9515\n",
      "Epoch 34/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0115 - accuracy: 0.9237 - val_loss: 0.0125 - val_accuracy: 0.9160\n",
      "Epoch 35/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0118 - accuracy: 0.9227 - val_loss: 0.0041 - val_accuracy: 0.9736\n",
      "Epoch 36/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0112 - accuracy: 0.9257 - val_loss: 0.0113 - val_accuracy: 0.9259\n",
      "Epoch 37/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0109 - accuracy: 0.9262 - val_loss: 0.0145 - val_accuracy: 0.9003\n",
      "Epoch 38/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0112 - accuracy: 0.9262 - val_loss: 0.0227 - val_accuracy: 0.8470\n",
      "Epoch 39/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0110 - accuracy: 0.9254 - val_loss: 0.0064 - val_accuracy: 0.9581\n",
      "Epoch 40/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0110 - accuracy: 0.9268 - val_loss: 0.0055 - val_accuracy: 0.9637\n",
      "Epoch 41/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0104 - accuracy: 0.9316 - val_loss: 0.0143 - val_accuracy: 0.9023\n",
      "Epoch 42/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0100 - accuracy: 0.9344 - val_loss: 0.0096 - val_accuracy: 0.9394\n",
      "Epoch 43/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0105 - accuracy: 0.9321 - val_loss: 0.0054 - val_accuracy: 0.9652\n",
      "Epoch 44/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0101 - accuracy: 0.9332 - val_loss: 0.0065 - val_accuracy: 0.9559\n",
      "Epoch 45/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0103 - accuracy: 0.9330 - val_loss: 0.0128 - val_accuracy: 0.9145\n",
      "==================================================\n",
      "Result of thin_resnet_model, fold 3\n",
      "Epoch: 45\n",
      "Accuracy: 0.973617434501648\n",
      "Time taken:  552.1622734069824\n",
      "==================================================\n",
      "Epoch 1/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0878 - accuracy: 0.2645 - val_loss: 0.1263 - val_accuracy: 0.2024\n",
      "Epoch 2/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0775 - accuracy: 0.3895 - val_loss: 0.1054 - val_accuracy: 0.1433\n",
      "Epoch 3/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0661 - accuracy: 0.5025 - val_loss: 0.1195 - val_accuracy: 0.1943\n",
      "Epoch 4/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0527 - accuracy: 0.6169 - val_loss: 0.0641 - val_accuracy: 0.5216\n",
      "Epoch 5/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0428 - accuracy: 0.6939 - val_loss: 0.0202 - val_accuracy: 0.8678\n",
      "Epoch 6/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0360 - accuracy: 0.7470 - val_loss: 0.0636 - val_accuracy: 0.5434\n",
      "Epoch 7/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0324 - accuracy: 0.7751 - val_loss: 0.1166 - val_accuracy: 0.3158\n",
      "Epoch 8/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0285 - accuracy: 0.8047 - val_loss: 0.1109 - val_accuracy: 0.3153\n",
      "Epoch 9/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0250 - accuracy: 0.8283 - val_loss: 0.0189 - val_accuracy: 0.8716\n",
      "Epoch 10/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0233 - accuracy: 0.8399 - val_loss: 0.0830 - val_accuracy: 0.4680\n",
      "Epoch 11/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0225 - accuracy: 0.8472 - val_loss: 0.0124 - val_accuracy: 0.9158\n",
      "Epoch 12/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0213 - accuracy: 0.8564 - val_loss: 0.0309 - val_accuracy: 0.7859\n",
      "Epoch 13/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0205 - accuracy: 0.8621 - val_loss: 0.0288 - val_accuracy: 0.7991\n",
      "Epoch 14/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0185 - accuracy: 0.8743 - val_loss: 0.0303 - val_accuracy: 0.7874\n",
      "Epoch 15/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0188 - accuracy: 0.8745 - val_loss: 0.0168 - val_accuracy: 0.8881\n",
      "Epoch 16/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0179 - accuracy: 0.8806 - val_loss: 0.0241 - val_accuracy: 0.8343\n",
      "Epoch 17/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0170 - accuracy: 0.8855 - val_loss: 0.0089 - val_accuracy: 0.9368\n",
      "Epoch 18/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0163 - accuracy: 0.8915 - val_loss: 0.0073 - val_accuracy: 0.9531\n",
      "Epoch 19/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0158 - accuracy: 0.8952 - val_loss: 0.0339 - val_accuracy: 0.7760\n",
      "Epoch 20/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0155 - accuracy: 0.8949 - val_loss: 0.0142 - val_accuracy: 0.9031\n",
      "Epoch 21/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0148 - accuracy: 0.9022 - val_loss: 0.0086 - val_accuracy: 0.9427\n",
      "Epoch 22/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0144 - accuracy: 0.9054 - val_loss: 0.0070 - val_accuracy: 0.9566\n",
      "Epoch 23/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0141 - accuracy: 0.9061 - val_loss: 0.0075 - val_accuracy: 0.9515\n",
      "Epoch 24/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0145 - accuracy: 0.9038 - val_loss: 0.0099 - val_accuracy: 0.9315\n",
      "Epoch 25/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0135 - accuracy: 0.9112 - val_loss: 0.0077 - val_accuracy: 0.9500\n",
      "Epoch 26/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0128 - accuracy: 0.9163 - val_loss: 0.0165 - val_accuracy: 0.8924\n",
      "Epoch 27/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0129 - accuracy: 0.9147 - val_loss: 0.0146 - val_accuracy: 0.8980\n",
      "Epoch 28/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0130 - accuracy: 0.9144 - val_loss: 0.0071 - val_accuracy: 0.9536\n",
      "Epoch 29/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0129 - accuracy: 0.9143 - val_loss: 0.0082 - val_accuracy: 0.9432\n",
      "Epoch 30/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0122 - accuracy: 0.9200 - val_loss: 0.0094 - val_accuracy: 0.9394\n",
      "Epoch 31/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0125 - accuracy: 0.9168 - val_loss: 0.0082 - val_accuracy: 0.9462\n",
      "Epoch 32/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0116 - accuracy: 0.9236 - val_loss: 0.0097 - val_accuracy: 0.9348\n",
      "==================================================\n",
      "Result of thin_resnet_model, fold 4\n",
      "Epoch: 32\n",
      "Accuracy: 0.956620991230011\n",
      "Time taken:  395.9627048969269\n",
      "==================================================\n",
      "Epoch 1/300\n",
      "986/986 [==============================] - 13s 13ms/step - loss: 0.0871 - accuracy: 0.2731 - val_loss: 0.1608 - val_accuracy: 0.0984\n",
      "Epoch 2/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0769 - accuracy: 0.3957 - val_loss: 0.1065 - val_accuracy: 0.2126\n",
      "Epoch 3/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0633 - accuracy: 0.5240 - val_loss: 0.0768 - val_accuracy: 0.4214\n",
      "Epoch 4/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0502 - accuracy: 0.6360 - val_loss: 0.1104 - val_accuracy: 0.2121\n",
      "Epoch 5/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0414 - accuracy: 0.7043 - val_loss: 0.0268 - val_accuracy: 0.8087\n",
      "Epoch 6/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0347 - accuracy: 0.7549 - val_loss: 0.0506 - val_accuracy: 0.6456\n",
      "Epoch 7/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0298 - accuracy: 0.7931 - val_loss: 0.0437 - val_accuracy: 0.7075\n",
      "Epoch 8/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0266 - accuracy: 0.8177 - val_loss: 0.0203 - val_accuracy: 0.8638\n",
      "Epoch 9/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0249 - accuracy: 0.8302 - val_loss: 0.0256 - val_accuracy: 0.8262\n",
      "Epoch 10/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0231 - accuracy: 0.8439 - val_loss: 0.0515 - val_accuracy: 0.6370\n",
      "Epoch 11/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0218 - accuracy: 0.8501 - val_loss: 0.0106 - val_accuracy: 0.9330\n",
      "Epoch 12/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0205 - accuracy: 0.8617 - val_loss: 0.0118 - val_accuracy: 0.9201\n",
      "Epoch 13/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0193 - accuracy: 0.8692 - val_loss: 0.0334 - val_accuracy: 0.7722\n",
      "Epoch 14/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0186 - accuracy: 0.8753 - val_loss: 0.0174 - val_accuracy: 0.8803\n",
      "Epoch 15/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0181 - accuracy: 0.8788 - val_loss: 0.0100 - val_accuracy: 0.9343\n",
      "Epoch 16/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0173 - accuracy: 0.8831 - val_loss: 0.0093 - val_accuracy: 0.9373\n",
      "Epoch 17/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0169 - accuracy: 0.8862 - val_loss: 0.0098 - val_accuracy: 0.9366\n",
      "Epoch 18/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0161 - accuracy: 0.8920 - val_loss: 0.0085 - val_accuracy: 0.9457\n",
      "Epoch 19/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0157 - accuracy: 0.8953 - val_loss: 0.0131 - val_accuracy: 0.9170\n",
      "Epoch 20/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0155 - accuracy: 0.8950 - val_loss: 0.0191 - val_accuracy: 0.8716\n",
      "Epoch 21/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0146 - accuracy: 0.9019 - val_loss: 0.0077 - val_accuracy: 0.9485\n",
      "Epoch 22/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0146 - accuracy: 0.9046 - val_loss: 0.0152 - val_accuracy: 0.8985\n",
      "Epoch 23/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0138 - accuracy: 0.9087 - val_loss: 0.0101 - val_accuracy: 0.9363\n",
      "Epoch 24/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0139 - accuracy: 0.9053 - val_loss: 0.0077 - val_accuracy: 0.9472\n",
      "Epoch 25/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0136 - accuracy: 0.9096 - val_loss: 0.0140 - val_accuracy: 0.9072\n",
      "Epoch 26/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0126 - accuracy: 0.9168 - val_loss: 0.0254 - val_accuracy: 0.8245\n",
      "Epoch 27/300\n",
      "986/986 [==============================] - 12s 13ms/step - loss: 0.0130 - accuracy: 0.9148 - val_loss: 0.0052 - val_accuracy: 0.9663\n",
      "Epoch 28/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0128 - accuracy: 0.9141 - val_loss: 0.0068 - val_accuracy: 0.9556\n",
      "Epoch 29/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0124 - accuracy: 0.9183 - val_loss: 0.0073 - val_accuracy: 0.9515\n",
      "Epoch 30/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0127 - accuracy: 0.9152 - val_loss: 0.0088 - val_accuracy: 0.9455\n",
      "Epoch 31/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0121 - accuracy: 0.9191 - val_loss: 0.0070 - val_accuracy: 0.9523\n",
      "Epoch 32/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0121 - accuracy: 0.9193 - val_loss: 0.0138 - val_accuracy: 0.9061\n",
      "Epoch 33/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0113 - accuracy: 0.9248 - val_loss: 0.0126 - val_accuracy: 0.9132\n",
      "Epoch 34/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0112 - accuracy: 0.9274 - val_loss: 0.0221 - val_accuracy: 0.8503.0112 \n",
      "Epoch 35/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0113 - accuracy: 0.9256 - val_loss: 0.0088 - val_accuracy: 0.9394\n",
      "Epoch 36/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0111 - accuracy: 0.9262 - val_loss: 0.0113 - val_accuracy: 0.9249\n",
      "Epoch 37/300\n",
      "986/986 [==============================] - 12s 12ms/step - loss: 0.0107 - accuracy: 0.9294 - val_loss: 0.0071 - val_accuracy: 0.9551\n",
      "==================================================\n",
      "Result of thin_resnet_model, fold 5\n",
      "Epoch: 37\n",
      "Accuracy: 0.9662607908248901\n",
      "Time taken:  455.87963223457336\n",
      "==================================================\n",
      "Epoch 1/300\n",
      "  1/986 [..............................] - ETA: 0s - loss: 0.0916 - accuracy: 0.0625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0130s vs `on_train_batch_end` time: 0.0229s). Check your callbacks.\n",
      "986/986 [==============================] - 38s 39ms/step - loss: 0.0893 - accuracy: 0.1868 - val_loss: 0.0928 - val_accuracy: 0.2440\n",
      "Epoch 2/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0764 - accuracy: 0.3447 - val_loss: 0.0777 - val_accuracy: 0.3508\n",
      "Epoch 3/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0624 - accuracy: 0.5129 - val_loss: 0.0643 - val_accuracy: 0.5005\n",
      "Epoch 4/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0507 - accuracy: 0.6301 - val_loss: 0.0641 - val_accuracy: 0.5211\n",
      "Epoch 5/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0432 - accuracy: 0.6938 - val_loss: 0.0408 - val_accuracy: 0.7047\n",
      "Epoch 6/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0329 - accuracy: 0.7707 - val_loss: 0.0322 - val_accuracy: 0.7780\n",
      "Epoch 7/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0243 - accuracy: 0.8356 - val_loss: 0.0341 - val_accuracy: 0.7659\n",
      "Epoch 8/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0193 - accuracy: 0.8731 - val_loss: 0.0171 - val_accuracy: 0.8874\n",
      "Epoch 9/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0159 - accuracy: 0.8958 - val_loss: 0.0213 - val_accuracy: 0.8503\n",
      "Epoch 10/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0139 - accuracy: 0.9105 - val_loss: 0.0144 - val_accuracy: 0.9061\n",
      "Epoch 11/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0114 - accuracy: 0.9266 - val_loss: 0.0317 - val_accuracy: 0.7948\n",
      "Epoch 12/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0111 - accuracy: 0.9288 - val_loss: 0.0160 - val_accuracy: 0.8932\n",
      "Epoch 13/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0105 - accuracy: 0.9323 - val_loss: 0.0114 - val_accuracy: 0.9280\n",
      "Epoch 14/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0092 - accuracy: 0.9419 - val_loss: 0.0176 - val_accuracy: 0.8846\n",
      "Epoch 15/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0082 - accuracy: 0.9482 - val_loss: 0.0110 - val_accuracy: 0.9318\n",
      "Epoch 16/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0084 - accuracy: 0.9469 - val_loss: 0.0232 - val_accuracy: 0.8501\n",
      "Epoch 17/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0076 - accuracy: 0.9517 - val_loss: 0.0101 - val_accuracy: 0.9366\n",
      "Epoch 18/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0073 - accuracy: 0.9548 - val_loss: 0.0108 - val_accuracy: 0.9335\n",
      "Epoch 19/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0068 - accuracy: 0.9576 - val_loss: 0.0101 - val_accuracy: 0.9351\n",
      "Epoch 20/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0059 - accuracy: 0.9639 - val_loss: 0.0128 - val_accuracy: 0.9155\n",
      "Epoch 21/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0069 - accuracy: 0.9583 - val_loss: 0.0114 - val_accuracy: 0.9351\n",
      "Epoch 22/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0065 - accuracy: 0.9603 - val_loss: 0.0116 - val_accuracy: 0.9297\n",
      "Epoch 23/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0056 - accuracy: 0.9658 - val_loss: 0.0097 - val_accuracy: 0.9368\n",
      "Epoch 24/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0062 - accuracy: 0.9633 - val_loss: 0.0091 - val_accuracy: 0.9432\n",
      "Epoch 25/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0045 - accuracy: 0.9732 - val_loss: 0.0124 - val_accuracy: 0.9234\n",
      "Epoch 26/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0054 - accuracy: 0.9677 - val_loss: 0.0094 - val_accuracy: 0.9419\n",
      "Epoch 27/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0047 - accuracy: 0.9720 - val_loss: 0.0093 - val_accuracy: 0.9452\n",
      "Epoch 28/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0047 - accuracy: 0.9714 - val_loss: 0.0135 - val_accuracy: 0.9198\n",
      "Epoch 29/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0057 - accuracy: 0.9649 - val_loss: 0.0100 - val_accuracy: 0.9368\n",
      "Epoch 30/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0048 - accuracy: 0.9704 - val_loss: 0.0092 - val_accuracy: 0.9429\n",
      "Epoch 31/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0036 - accuracy: 0.9789 - val_loss: 0.0090 - val_accuracy: 0.9444\n",
      "Epoch 32/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0046 - accuracy: 0.9725 - val_loss: 0.0108 - val_accuracy: 0.9356\n",
      "Epoch 33/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0046 - accuracy: 0.9725 - val_loss: 0.0109 - val_accuracy: 0.9335\n",
      "Epoch 34/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0038 - accuracy: 0.9770 - val_loss: 0.0086 - val_accuracy: 0.9470\n",
      "Epoch 35/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0038 - accuracy: 0.9774 - val_loss: 0.0088 - val_accuracy: 0.9457\n",
      "Epoch 36/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0037 - accuracy: 0.9770 - val_loss: 0.0105 - val_accuracy: 0.9386\n",
      "Epoch 37/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0043 - accuracy: 0.9746 - val_loss: 0.0109 - val_accuracy: 0.9323\n",
      "Epoch 38/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0036 - accuracy: 0.9788 - val_loss: 0.0113 - val_accuracy: 0.9277\n",
      "Epoch 39/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0032 - accuracy: 0.9801 - val_loss: 0.0090 - val_accuracy: 0.9472\n",
      "Epoch 40/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0031 - accuracy: 0.9817 - val_loss: 0.0093 - val_accuracy: 0.9422\n",
      "Epoch 41/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0038 - accuracy: 0.9784 - val_loss: 0.0101 - val_accuracy: 0.9384\n",
      "Epoch 42/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0037 - accuracy: 0.9779 - val_loss: 0.0091 - val_accuracy: 0.9439\n",
      "Epoch 43/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0029 - accuracy: 0.9826 - val_loss: 0.0087 - val_accuracy: 0.9470\n",
      "Epoch 44/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0038 - accuracy: 0.9785 - val_loss: 0.0104 - val_accuracy: 0.9356\n",
      "Epoch 45/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0031 - accuracy: 0.9815 - val_loss: 0.0077 - val_accuracy: 0.9526\n",
      "Epoch 46/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0025 - accuracy: 0.9854 - val_loss: 0.0083 - val_accuracy: 0.9508\n",
      "Epoch 47/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0030 - accuracy: 0.9823 - val_loss: 0.0158 - val_accuracy: 0.9064\n",
      "Epoch 48/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0042 - accuracy: 0.9748 - val_loss: 0.0094 - val_accuracy: 0.9424\n",
      "Epoch 49/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0026 - accuracy: 0.9847 - val_loss: 0.0079 - val_accuracy: 0.9518\n",
      "Epoch 50/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0027 - accuracy: 0.9839 - val_loss: 0.0101 - val_accuracy: 0.9391\n",
      "Epoch 51/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0028 - accuracy: 0.9843 - val_loss: 0.0085 - val_accuracy: 0.9490\n",
      "Epoch 52/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0035 - accuracy: 0.9800 - val_loss: 0.0089 - val_accuracy: 0.9457\n",
      "Epoch 53/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0029 - accuracy: 0.9833 - val_loss: 0.0082 - val_accuracy: 0.9531\n",
      "Epoch 54/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0023 - accuracy: 0.9868 - val_loss: 0.0096 - val_accuracy: 0.9419\n",
      "Epoch 55/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0029 - accuracy: 0.9826 - val_loss: 0.0106 - val_accuracy: 0.9346\n",
      "Epoch 56/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0024 - accuracy: 0.9862 - val_loss: 0.0083 - val_accuracy: 0.9475\n",
      "Epoch 57/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0026 - accuracy: 0.9848 - val_loss: 0.0088 - val_accuracy: 0.9475\n",
      "Epoch 58/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0036 - accuracy: 0.9790 - val_loss: 0.0079 - val_accuracy: 0.9526\n",
      "Epoch 59/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0020 - accuracy: 0.9883 - val_loss: 0.0089 - val_accuracy: 0.9477\n",
      "Epoch 60/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0025 - accuracy: 0.9859 - val_loss: 0.0084 - val_accuracy: 0.9498\n",
      "Epoch 61/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0024 - accuracy: 0.9866 - val_loss: 0.0078 - val_accuracy: 0.9538\n",
      "Epoch 62/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0029 - accuracy: 0.9827 - val_loss: 0.0093 - val_accuracy: 0.9434\n",
      "Epoch 63/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0032 - accuracy: 0.9817 - val_loss: 0.0084 - val_accuracy: 0.9493\n",
      "Epoch 64/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0029 - accuracy: 0.9838 - val_loss: 0.0093 - val_accuracy: 0.9452\n",
      "Epoch 65/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0026 - accuracy: 0.9848 - val_loss: 0.0137 - val_accuracy: 0.9176\n",
      "Epoch 66/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0022 - accuracy: 0.9872 - val_loss: 0.0088 - val_accuracy: 0.9480\n",
      "Epoch 67/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0023 - accuracy: 0.9871 - val_loss: 0.0084 - val_accuracy: 0.9505\n",
      "Epoch 68/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0021 - accuracy: 0.9876 - val_loss: 0.0084 - val_accuracy: 0.9500\n",
      "Epoch 69/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0022 - accuracy: 0.9870 - val_loss: 0.0089 - val_accuracy: 0.9467\n",
      "Epoch 70/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0025 - accuracy: 0.9854 - val_loss: 0.0093 - val_accuracy: 0.9475\n",
      "Epoch 71/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0018 - accuracy: 0.9897 - val_loss: 0.0084 - val_accuracy: 0.9518\n",
      "==================================================\n",
      "Result of vggnet_model, fold 1\n",
      "Epoch: 71\n",
      "Accuracy: 0.9538305401802063\n",
      "Time taken:  2665.663770198822\n",
      "==================================================\n",
      "Epoch 1/300\n",
      "  1/986 [..............................] - ETA: 0s - loss: 0.0981 - accuracy: 0.0000e+00WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0140s vs `on_train_batch_end` time: 0.0279s). Check your callbacks.\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0888 - accuracy: 0.1884 - val_loss: 0.0920 - val_accuracy: 0.0870\n",
      "Epoch 2/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0777 - accuracy: 0.3310 - val_loss: 0.0700 - val_accuracy: 0.4409\n",
      "Epoch 3/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0641 - accuracy: 0.4954 - val_loss: 0.0573 - val_accuracy: 0.5373\n",
      "Epoch 4/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0479 - accuracy: 0.6518 - val_loss: 0.0430 - val_accuracy: 0.7009\n",
      "Epoch 5/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0352 - accuracy: 0.7579 - val_loss: 0.0266 - val_accuracy: 0.8245\n",
      "Epoch 6/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0258 - accuracy: 0.8290 - val_loss: 0.0170 - val_accuracy: 0.8891\n",
      "Epoch 7/300\n",
      "986/986 [==============================] - 39s 39ms/step - loss: 0.0218 - accuracy: 0.8557 - val_loss: 0.0117 - val_accuracy: 0.9249\n",
      "Epoch 8/300\n",
      "986/986 [==============================] - 38s 39ms/step - loss: 0.0171 - accuracy: 0.8892 - val_loss: 0.0125 - val_accuracy: 0.9216\n",
      "Epoch 9/300\n",
      "986/986 [==============================] - 38s 39ms/step - loss: 0.0155 - accuracy: 0.9009 - val_loss: 0.0122 - val_accuracy: 0.9211\n",
      "Epoch 10/300\n",
      "986/986 [==============================] - 38s 39ms/step - loss: 0.0137 - accuracy: 0.9129 - val_loss: 0.0077 - val_accuracy: 0.9526\n",
      "Epoch 11/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0123 - accuracy: 0.9216 - val_loss: 0.0095 - val_accuracy: 0.9417\n",
      "Epoch 12/300\n",
      "986/986 [==============================] - 37s 37ms/step - loss: 0.0109 - accuracy: 0.9321 - val_loss: 0.0099 - val_accuracy: 0.9366\n",
      "Epoch 13/300\n",
      "986/986 [==============================] - 38s 39ms/step - loss: 0.0102 - accuracy: 0.9379 - val_loss: 0.0080 - val_accuracy: 0.9500\n",
      "Epoch 14/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0101 - accuracy: 0.9380 - val_loss: 0.0092 - val_accuracy: 0.9450\n",
      "Epoch 15/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0104 - accuracy: 0.9352 - val_loss: 0.0089 - val_accuracy: 0.9434\n",
      "Epoch 16/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0089 - accuracy: 0.9448 - val_loss: 0.0066 - val_accuracy: 0.9584\n",
      "Epoch 17/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0080 - accuracy: 0.9494 - val_loss: 0.0056 - val_accuracy: 0.9652\n",
      "Epoch 18/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0079 - accuracy: 0.9517 - val_loss: 0.0044 - val_accuracy: 0.9739\n",
      "Epoch 19/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0073 - accuracy: 0.9552 - val_loss: 0.0036 - val_accuracy: 0.9769\n",
      "Epoch 20/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0070 - accuracy: 0.9578 - val_loss: 0.0056 - val_accuracy: 0.9637\n",
      "Epoch 21/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0077 - accuracy: 0.9535 - val_loss: 0.0048 - val_accuracy: 0.9701\n",
      "Epoch 22/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0071 - accuracy: 0.9566 - val_loss: 0.0049 - val_accuracy: 0.9688\n",
      "Epoch 23/300\n",
      "986/986 [==============================] - 38s 39ms/step - loss: 0.0064 - accuracy: 0.9608 - val_loss: 0.0067 - val_accuracy: 0.9604\n",
      "Epoch 24/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0062 - accuracy: 0.9622 - val_loss: 0.0046 - val_accuracy: 0.9688\n",
      "Epoch 25/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0063 - accuracy: 0.9613 - val_loss: 0.0047 - val_accuracy: 0.9693\n",
      "Epoch 26/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0057 - accuracy: 0.9661 - val_loss: 0.0043 - val_accuracy: 0.9744\n",
      "Epoch 27/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0057 - accuracy: 0.9652 - val_loss: 0.0053 - val_accuracy: 0.9685\n",
      "Epoch 28/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0063 - accuracy: 0.9613 - val_loss: 0.0032 - val_accuracy: 0.9797\n",
      "Epoch 29/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0051 - accuracy: 0.9691 - val_loss: 0.0034 - val_accuracy: 0.9792\n",
      "Epoch 30/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0054 - accuracy: 0.9680 - val_loss: 0.0032 - val_accuracy: 0.9800\n",
      "Epoch 31/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0053 - accuracy: 0.9676 - val_loss: 0.0056 - val_accuracy: 0.9642\n",
      "Epoch 32/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0048 - accuracy: 0.9711 - val_loss: 0.0038 - val_accuracy: 0.9777\n",
      "Epoch 33/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0054 - accuracy: 0.9684 - val_loss: 0.0029 - val_accuracy: 0.9827\n",
      "Epoch 34/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0046 - accuracy: 0.9727 - val_loss: 0.0032 - val_accuracy: 0.9789\n",
      "Epoch 35/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0054 - accuracy: 0.9680 - val_loss: 0.0041 - val_accuracy: 0.9759\n",
      "Epoch 36/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0039 - accuracy: 0.9771 - val_loss: 0.0041 - val_accuracy: 0.9746\n",
      "Epoch 37/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0047 - accuracy: 0.9716 - val_loss: 0.0032 - val_accuracy: 0.9802\n",
      "Epoch 38/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0043 - accuracy: 0.9742 - val_loss: 0.0040 - val_accuracy: 0.9769\n",
      "Epoch 39/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0039 - accuracy: 0.9768 - val_loss: 0.0046 - val_accuracy: 0.9718\n",
      "Epoch 40/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0040 - accuracy: 0.9751 - val_loss: 0.0029 - val_accuracy: 0.9827\n",
      "Epoch 41/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0039 - accuracy: 0.9765 - val_loss: 0.0032 - val_accuracy: 0.9805\n",
      "Epoch 42/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0035 - accuracy: 0.9786 - val_loss: 0.0032 - val_accuracy: 0.9789\n",
      "Epoch 43/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0040 - accuracy: 0.9769 - val_loss: 0.0043 - val_accuracy: 0.9749\n",
      "==================================================\n",
      "Result of vggnet_model, fold 2\n",
      "Epoch: 43\n",
      "Accuracy: 0.982749879360199\n",
      "Time taken:  1618.7381813526154\n",
      "==================================================\n",
      "Epoch 1/300\n",
      "  1/986 [..............................] - ETA: 0s - loss: 0.0937 - accuracy: 0.1250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0160s vs `on_train_batch_end` time: 0.0269s). Check your callbacks.\n",
      "986/986 [==============================] - 38s 39ms/step - loss: 0.0886 - accuracy: 0.1991 - val_loss: 0.1287 - val_accuracy: 0.1568\n",
      "Epoch 2/300\n",
      "986/986 [==============================] - 38s 39ms/step - loss: 0.0713 - accuracy: 0.4207 - val_loss: 0.0570 - val_accuracy: 0.5941\n",
      "Epoch 3/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0569 - accuracy: 0.5729 - val_loss: 0.0544 - val_accuracy: 0.5791\n",
      "Epoch 4/300\n",
      "986/986 [==============================] - 38s 39ms/step - loss: 0.0439 - accuracy: 0.6875 - val_loss: 0.0426 - val_accuracy: 0.6771\n",
      "Epoch 5/300\n",
      "986/986 [==============================] - 38s 39ms/step - loss: 0.0331 - accuracy: 0.7749 - val_loss: 0.0261 - val_accuracy: 0.8201\n",
      "Epoch 6/300\n",
      "986/986 [==============================] - 38s 39ms/step - loss: 0.0252 - accuracy: 0.8298 - val_loss: 0.0133 - val_accuracy: 0.9079\n",
      "Epoch 7/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0207 - accuracy: 0.8623 - val_loss: 0.0169 - val_accuracy: 0.8833\n",
      "Epoch 8/300\n",
      "986/986 [==============================] - 38s 39ms/step - loss: 0.0175 - accuracy: 0.8853 - val_loss: 0.0110 - val_accuracy: 0.9282\n",
      "Epoch 9/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0157 - accuracy: 0.8987 - val_loss: 0.0157 - val_accuracy: 0.9031\n",
      "Epoch 10/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0138 - accuracy: 0.9098 - val_loss: 0.0129 - val_accuracy: 0.9140\n",
      "Epoch 11/300\n",
      "986/986 [==============================] - 38s 39ms/step - loss: 0.0130 - accuracy: 0.9168 - val_loss: 0.0067 - val_accuracy: 0.9607\n",
      "Epoch 12/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0122 - accuracy: 0.9231 - val_loss: 0.0081 - val_accuracy: 0.9510\n",
      "Epoch 13/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0109 - accuracy: 0.9300 - val_loss: 0.0065 - val_accuracy: 0.9589\n",
      "Epoch 14/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0096 - accuracy: 0.9389 - val_loss: 0.0062 - val_accuracy: 0.9597\n",
      "Epoch 15/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0092 - accuracy: 0.9413 - val_loss: 0.0065 - val_accuracy: 0.9604\n",
      "Epoch 16/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0087 - accuracy: 0.9444 - val_loss: 0.0063 - val_accuracy: 0.9592\n",
      "Epoch 17/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0081 - accuracy: 0.9486 - val_loss: 0.0062 - val_accuracy: 0.9625\n",
      "Epoch 18/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0081 - accuracy: 0.9503 - val_loss: 0.0088 - val_accuracy: 0.9444\n",
      "Epoch 19/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0079 - accuracy: 0.9508 - val_loss: 0.0043 - val_accuracy: 0.9741\n",
      "Epoch 20/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0076 - accuracy: 0.9529 - val_loss: 0.0045 - val_accuracy: 0.9721\n",
      "Epoch 21/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0072 - accuracy: 0.9547 - val_loss: 0.0083 - val_accuracy: 0.9465\n",
      "Epoch 22/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0074 - accuracy: 0.9547 - val_loss: 0.0040 - val_accuracy: 0.9754\n",
      "Epoch 23/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0071 - accuracy: 0.9562 - val_loss: 0.0052 - val_accuracy: 0.9683\n",
      "Epoch 24/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0064 - accuracy: 0.9600 - val_loss: 0.0036 - val_accuracy: 0.9774\n",
      "Epoch 25/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0058 - accuracy: 0.9646 - val_loss: 0.0063 - val_accuracy: 0.9566\n",
      "Epoch 26/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0064 - accuracy: 0.9610 - val_loss: 0.0045 - val_accuracy: 0.9698\n",
      "Epoch 27/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0056 - accuracy: 0.9653 - val_loss: 0.0081 - val_accuracy: 0.9510\n",
      "Epoch 28/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0055 - accuracy: 0.9660 - val_loss: 0.0085 - val_accuracy: 0.9457\n",
      "Epoch 29/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0051 - accuracy: 0.9692 - val_loss: 0.0040 - val_accuracy: 0.9756\n",
      "Epoch 30/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0055 - accuracy: 0.9649 - val_loss: 0.0033 - val_accuracy: 0.9805\n",
      "Epoch 31/300\n",
      "986/986 [==============================] - 38s 39ms/step - loss: 0.0052 - accuracy: 0.9687 - val_loss: 0.0046 - val_accuracy: 0.9703\n",
      "Epoch 32/300\n",
      "986/986 [==============================] - 38s 39ms/step - loss: 0.0048 - accuracy: 0.9711 - val_loss: 0.0100 - val_accuracy: 0.9396\n",
      "Epoch 33/300\n",
      "986/986 [==============================] - 38s 39ms/step - loss: 0.0052 - accuracy: 0.9687 - val_loss: 0.0034 - val_accuracy: 0.9774\n",
      "Epoch 34/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0051 - accuracy: 0.9701 - val_loss: 0.0054 - val_accuracy: 0.9701\n",
      "Epoch 35/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0047 - accuracy: 0.9712 - val_loss: 0.0040 - val_accuracy: 0.9751\n",
      "Epoch 36/300\n",
      "986/986 [==============================] - 38s 39ms/step - loss: 0.0048 - accuracy: 0.9711 - val_loss: 0.0045 - val_accuracy: 0.9711\n",
      "Epoch 37/300\n",
      "986/986 [==============================] - 38s 39ms/step - loss: 0.0042 - accuracy: 0.9743 - val_loss: 0.0036 - val_accuracy: 0.9789\n",
      "Epoch 38/300\n",
      "986/986 [==============================] - 38s 39ms/step - loss: 0.0041 - accuracy: 0.9747 - val_loss: 0.0029 - val_accuracy: 0.9822\n",
      "Epoch 39/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0045 - accuracy: 0.9731 - val_loss: 0.0030 - val_accuracy: 0.9830\n",
      "Epoch 40/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0040 - accuracy: 0.9761 - val_loss: 0.0029 - val_accuracy: 0.9812\n",
      "Epoch 41/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "986/986 [==============================] - 37s 37ms/step - loss: 0.0049 - accuracy: 0.9704 - val_loss: 0.0043 - val_accuracy: 0.9726\n",
      "Epoch 42/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0036 - accuracy: 0.9784 - val_loss: 0.0030 - val_accuracy: 0.9827\n",
      "Epoch 43/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0037 - accuracy: 0.9779 - val_loss: 0.0026 - val_accuracy: 0.9833\n",
      "Epoch 44/300\n",
      "986/986 [==============================] - 40s 40ms/step - loss: 0.0044 - accuracy: 0.9733 - val_loss: 0.0040 - val_accuracy: 0.9756\n",
      "Epoch 45/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0033 - accuracy: 0.9805 - val_loss: 0.0034 - val_accuracy: 0.9777\n",
      "Epoch 46/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0040 - accuracy: 0.9760 - val_loss: 0.0028 - val_accuracy: 0.9833\n",
      "Epoch 47/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0037 - accuracy: 0.9775 - val_loss: 0.0052 - val_accuracy: 0.9660\n",
      "Epoch 48/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0036 - accuracy: 0.9789 - val_loss: 0.0033 - val_accuracy: 0.9795\n",
      "Epoch 49/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0035 - accuracy: 0.9793 - val_loss: 0.0028 - val_accuracy: 0.9830\n",
      "Epoch 50/300\n",
      "986/986 [==============================] - 40s 40ms/step - loss: 0.0035 - accuracy: 0.9791 - val_loss: 0.0032 - val_accuracy: 0.9800\n",
      "Epoch 51/300\n",
      "986/986 [==============================] - 40s 40ms/step - loss: 0.0034 - accuracy: 0.9799 - val_loss: 0.0045 - val_accuracy: 0.9726\n",
      "Epoch 52/300\n",
      "986/986 [==============================] - 39s 40ms/step - loss: 0.0033 - accuracy: 0.9806 - val_loss: 0.0021 - val_accuracy: 0.9873\n",
      "Epoch 53/300\n",
      "986/986 [==============================] - 39s 40ms/step - loss: 0.0037 - accuracy: 0.9786 - val_loss: 0.0032 - val_accuracy: 0.9797\n",
      "Epoch 54/300\n",
      "986/986 [==============================] - 39s 39ms/step - loss: 0.0036 - accuracy: 0.9791 - val_loss: 0.0042 - val_accuracy: 0.9739\n",
      "Epoch 55/300\n",
      "986/986 [==============================] - 39s 39ms/step - loss: 0.0031 - accuracy: 0.9814 - val_loss: 0.0042 - val_accuracy: 0.9746\n",
      "Epoch 56/300\n",
      "986/986 [==============================] - 39s 39ms/step - loss: 0.0032 - accuracy: 0.9808 - val_loss: 0.0035 - val_accuracy: 0.9782\n",
      "Epoch 57/300\n",
      "986/986 [==============================] - 39s 39ms/step - loss: 0.0034 - accuracy: 0.9798 - val_loss: 0.0045 - val_accuracy: 0.9741\n",
      "Epoch 58/300\n",
      "986/986 [==============================] - 38s 39ms/step - loss: 0.0025 - accuracy: 0.9855 - val_loss: 0.0037 - val_accuracy: 0.9772\n",
      "Epoch 59/300\n",
      "986/986 [==============================] - 38s 39ms/step - loss: 0.0028 - accuracy: 0.9838 - val_loss: 0.0034 - val_accuracy: 0.9779\n",
      "Epoch 60/300\n",
      "986/986 [==============================] - 39s 39ms/step - loss: 0.0031 - accuracy: 0.9815 - val_loss: 0.0026 - val_accuracy: 0.9833\n",
      "Epoch 61/300\n",
      "986/986 [==============================] - 39s 40ms/step - loss: 0.0025 - accuracy: 0.9851 - val_loss: 0.0031 - val_accuracy: 0.9807\n",
      "Epoch 62/300\n",
      "986/986 [==============================] - 38s 39ms/step - loss: 0.0035 - accuracy: 0.9796 - val_loss: 0.0034 - val_accuracy: 0.9789\n",
      "==================================================\n",
      "Result of vggnet_model, fold 3\n",
      "Epoch: 62\n",
      "Accuracy: 0.9873160719871521\n",
      "Time taken:  2355.9931466579437\n",
      "==================================================\n",
      "Epoch 1/300\n",
      "  1/986 [..............................] - ETA: 0s - loss: 0.0874 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0180s vs `on_train_batch_end` time: 0.0289s). Check your callbacks.\n",
      "986/986 [==============================] - 39s 40ms/step - loss: 0.0887 - accuracy: 0.1884 - val_loss: 0.1206 - val_accuracy: 0.1834\n",
      "Epoch 2/300\n",
      "986/986 [==============================] - 39s 40ms/step - loss: 0.0802 - accuracy: 0.2917 - val_loss: 0.0903 - val_accuracy: 0.1385\n",
      "Epoch 3/300\n",
      "986/986 [==============================] - 39s 40ms/step - loss: 0.0765 - accuracy: 0.3278 - val_loss: 0.0720 - val_accuracy: 0.3754\n",
      "Epoch 4/300\n",
      "986/986 [==============================] - 39s 40ms/step - loss: 0.0708 - accuracy: 0.4028 - val_loss: 0.0629 - val_accuracy: 0.4914\n",
      "Epoch 5/300\n",
      "986/986 [==============================] - 39s 40ms/step - loss: 0.0608 - accuracy: 0.5167 - val_loss: 0.0463 - val_accuracy: 0.6499\n",
      "Epoch 6/300\n",
      "986/986 [==============================] - 39s 39ms/step - loss: 0.0470 - accuracy: 0.6580 - val_loss: 0.0437 - val_accuracy: 0.6875\n",
      "Epoch 7/300\n",
      "986/986 [==============================] - 38s 39ms/step - loss: 0.0336 - accuracy: 0.7711 - val_loss: 0.0285 - val_accuracy: 0.8222\n",
      "Epoch 8/300\n",
      "986/986 [==============================] - 38s 39ms/step - loss: 0.0237 - accuracy: 0.8432 - val_loss: 0.0141 - val_accuracy: 0.9110\n",
      "Epoch 9/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0193 - accuracy: 0.8744 - val_loss: 0.0162 - val_accuracy: 0.8914\n",
      "Epoch 10/300\n",
      "986/986 [==============================] - 38s 39ms/step - loss: 0.0161 - accuracy: 0.8985 - val_loss: 0.0073 - val_accuracy: 0.9528\n",
      "Epoch 11/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0141 - accuracy: 0.9101 - val_loss: 0.0118 - val_accuracy: 0.9234\n",
      "Epoch 12/300\n",
      "986/986 [==============================] - 39s 39ms/step - loss: 0.0132 - accuracy: 0.9171 - val_loss: 0.0076 - val_accuracy: 0.9559\n",
      "Epoch 13/300\n",
      "986/986 [==============================] - 39s 39ms/step - loss: 0.0110 - accuracy: 0.9318 - val_loss: 0.0063 - val_accuracy: 0.9607\n",
      "Epoch 14/300\n",
      "986/986 [==============================] - 39s 39ms/step - loss: 0.0109 - accuracy: 0.9325 - val_loss: 0.0051 - val_accuracy: 0.9691\n",
      "Epoch 15/300\n",
      "986/986 [==============================] - 38s 39ms/step - loss: 0.0096 - accuracy: 0.9413 - val_loss: 0.0051 - val_accuracy: 0.9670\n",
      "Epoch 16/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0100 - accuracy: 0.9388 - val_loss: 0.0085 - val_accuracy: 0.9477\n",
      "Epoch 17/300\n",
      "986/986 [==============================] - 38s 39ms/step - loss: 0.0087 - accuracy: 0.9479 - val_loss: 0.0066 - val_accuracy: 0.9571\n",
      "Epoch 18/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0080 - accuracy: 0.9500 - val_loss: 0.0054 - val_accuracy: 0.9685\n",
      "Epoch 19/300\n",
      "986/986 [==============================] - 38s 39ms/step - loss: 0.0085 - accuracy: 0.9488 - val_loss: 0.0043 - val_accuracy: 0.9726\n",
      "Epoch 20/300\n",
      "986/986 [==============================] - 38s 39ms/step - loss: 0.0068 - accuracy: 0.9594 - val_loss: 0.0035 - val_accuracy: 0.9789\n",
      "Epoch 21/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0080 - accuracy: 0.9531 - val_loss: 0.0054 - val_accuracy: 0.9655\n",
      "Epoch 22/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0076 - accuracy: 0.9540 - val_loss: 0.0059 - val_accuracy: 0.9675\n",
      "Epoch 23/300\n",
      "986/986 [==============================] - 38s 39ms/step - loss: 0.0073 - accuracy: 0.9556 - val_loss: 0.0053 - val_accuracy: 0.9665\n",
      "Epoch 24/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0070 - accuracy: 0.9566 - val_loss: 0.0049 - val_accuracy: 0.9729\n",
      "Epoch 25/300\n",
      "986/986 [==============================] - 37s 37ms/step - loss: 0.0074 - accuracy: 0.9559 - val_loss: 0.0050 - val_accuracy: 0.9685\n",
      "Epoch 26/300\n",
      "986/986 [==============================] - 37s 37ms/step - loss: 0.0067 - accuracy: 0.9600 - val_loss: 0.0043 - val_accuracy: 0.9751\n",
      "Epoch 27/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0061 - accuracy: 0.9630 - val_loss: 0.0068 - val_accuracy: 0.9607\n",
      "Epoch 28/300\n",
      "986/986 [==============================] - 37s 37ms/step - loss: 0.0060 - accuracy: 0.9637 - val_loss: 0.0041 - val_accuracy: 0.9736\n",
      "Epoch 29/300\n",
      "986/986 [==============================] - 37s 37ms/step - loss: 0.0061 - accuracy: 0.9632 - val_loss: 0.0056 - val_accuracy: 0.9655\n",
      "Epoch 30/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0057 - accuracy: 0.9670 - val_loss: 0.0078 - val_accuracy: 0.9515\n",
      "==================================================\n",
      "Result of vggnet_model, fold 4\n",
      "Epoch: 30\n",
      "Accuracy: 0.978944718837738\n",
      "Time taken:  1146.1661093235016\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "  1/986 [..............................] - ETA: 0s - loss: 0.0904 - accuracy: 0.1875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0150s vs `on_train_batch_end` time: 0.0269s). Check your callbacks.\n",
      "986/986 [==============================] - 38s 39ms/step - loss: 0.0867 - accuracy: 0.2305 - val_loss: 0.0825 - val_accuracy: 0.2958\n",
      "Epoch 2/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0701 - accuracy: 0.4339 - val_loss: 0.0716 - val_accuracy: 0.4320\n",
      "Epoch 3/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0573 - accuracy: 0.5686 - val_loss: 0.0666 - val_accuracy: 0.4637\n",
      "Epoch 4/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0474 - accuracy: 0.6555 - val_loss: 0.0386 - val_accuracy: 0.7156\n",
      "Epoch 5/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0395 - accuracy: 0.7223 - val_loss: 0.0294 - val_accuracy: 0.7965\n",
      "Epoch 6/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0321 - accuracy: 0.7798 - val_loss: 0.0462 - val_accuracy: 0.6492\n",
      "Epoch 7/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0252 - accuracy: 0.8307 - val_loss: 0.0150 - val_accuracy: 0.9008\n",
      "Epoch 8/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0202 - accuracy: 0.8665 - val_loss: 0.0177 - val_accuracy: 0.8861\n",
      "Epoch 9/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0164 - accuracy: 0.8922 - val_loss: 0.0276 - val_accuracy: 0.8031\n",
      "Epoch 10/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0151 - accuracy: 0.9030 - val_loss: 0.0109 - val_accuracy: 0.9285\n",
      "Epoch 11/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0126 - accuracy: 0.9194 - val_loss: 0.0093 - val_accuracy: 0.9427\n",
      "Epoch 12/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0120 - accuracy: 0.9246 - val_loss: 0.0104 - val_accuracy: 0.9348\n",
      "Epoch 13/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0117 - accuracy: 0.9274 - val_loss: 0.0105 - val_accuracy: 0.9305\n",
      "Epoch 14/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0105 - accuracy: 0.9336 - val_loss: 0.0076 - val_accuracy: 0.9505\n",
      "Epoch 15/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0103 - accuracy: 0.9342 - val_loss: 0.0061 - val_accuracy: 0.9607\n",
      "Epoch 16/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0085 - accuracy: 0.9457 - val_loss: 0.0061 - val_accuracy: 0.9617\n",
      "Epoch 17/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0086 - accuracy: 0.9474 - val_loss: 0.0085 - val_accuracy: 0.9424\n",
      "Epoch 18/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0082 - accuracy: 0.9496 - val_loss: 0.0067 - val_accuracy: 0.9564\n",
      "Epoch 19/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0081 - accuracy: 0.9488 - val_loss: 0.0040 - val_accuracy: 0.9759\n",
      "Epoch 20/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0068 - accuracy: 0.9578 - val_loss: 0.0050 - val_accuracy: 0.9716\n",
      "Epoch 21/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0077 - accuracy: 0.9533 - val_loss: 0.0040 - val_accuracy: 0.9769\n",
      "Epoch 22/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0067 - accuracy: 0.9577 - val_loss: 0.0046 - val_accuracy: 0.9741\n",
      "Epoch 23/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0058 - accuracy: 0.9641 - val_loss: 0.0052 - val_accuracy: 0.9729\n",
      "Epoch 24/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0061 - accuracy: 0.9623 - val_loss: 0.0046 - val_accuracy: 0.9749\n",
      "Epoch 25/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0066 - accuracy: 0.9597 - val_loss: 0.0076 - val_accuracy: 0.9584\n",
      "Epoch 26/300\n",
      "986/986 [==============================] - 38s 38ms/step - loss: 0.0061 - accuracy: 0.9628 - val_loss: 0.0036 - val_accuracy: 0.9772\n",
      "Epoch 27/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0059 - accuracy: 0.9642 - val_loss: 0.0048 - val_accuracy: 0.9706\n",
      "Epoch 28/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0056 - accuracy: 0.9654 - val_loss: 0.0070 - val_accuracy: 0.9569\n",
      "Epoch 29/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0058 - accuracy: 0.9656 - val_loss: 0.0044 - val_accuracy: 0.9741\n",
      "Epoch 30/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0046 - accuracy: 0.9719 - val_loss: 0.0048 - val_accuracy: 0.9678\n",
      "Epoch 31/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0056 - accuracy: 0.9663 - val_loss: 0.0049 - val_accuracy: 0.9691\n",
      "Epoch 32/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0054 - accuracy: 0.9676 - val_loss: 0.0059 - val_accuracy: 0.9655\n",
      "Epoch 33/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0046 - accuracy: 0.9717 - val_loss: 0.0041 - val_accuracy: 0.9754\n",
      "Epoch 34/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0043 - accuracy: 0.9743 - val_loss: 0.0064 - val_accuracy: 0.9614\n",
      "Epoch 35/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0049 - accuracy: 0.9708 - val_loss: 0.0047 - val_accuracy: 0.9693\n",
      "Epoch 36/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0052 - accuracy: 0.9685 - val_loss: 0.0029 - val_accuracy: 0.9835\n",
      "Epoch 37/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0039 - accuracy: 0.9760 - val_loss: 0.0038 - val_accuracy: 0.9767\n",
      "Epoch 38/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0039 - accuracy: 0.9761 - val_loss: 0.0032 - val_accuracy: 0.9825\n",
      "Epoch 39/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0045 - accuracy: 0.9730 - val_loss: 0.0040 - val_accuracy: 0.9792\n",
      "Epoch 40/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0038 - accuracy: 0.9768 - val_loss: 0.0035 - val_accuracy: 0.9767\n",
      "Epoch 41/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0043 - accuracy: 0.9748 - val_loss: 0.0032 - val_accuracy: 0.9820\n",
      "Epoch 42/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0045 - accuracy: 0.9726 - val_loss: 0.0023 - val_accuracy: 0.9863\n",
      "Epoch 43/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0039 - accuracy: 0.9752 - val_loss: 0.0026 - val_accuracy: 0.9848\n",
      "Epoch 44/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0037 - accuracy: 0.9772 - val_loss: 0.0079 - val_accuracy: 0.9480\n",
      "Epoch 45/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0035 - accuracy: 0.9798 - val_loss: 0.0027 - val_accuracy: 0.9840\n",
      "Epoch 46/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0038 - accuracy: 0.9772 - val_loss: 0.0040 - val_accuracy: 0.9741\n",
      "Epoch 47/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0032 - accuracy: 0.9818 - val_loss: 0.0027 - val_accuracy: 0.9850\n",
      "Epoch 48/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0041 - accuracy: 0.9762 - val_loss: 0.0028 - val_accuracy: 0.9827\n",
      "Epoch 49/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0034 - accuracy: 0.9791 - val_loss: 0.0063 - val_accuracy: 0.9602\n",
      "Epoch 50/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0038 - accuracy: 0.9778 - val_loss: 0.0033 - val_accuracy: 0.9787\n",
      "Epoch 51/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0026 - accuracy: 0.9848 - val_loss: 0.0025 - val_accuracy: 0.9855\n",
      "Epoch 52/300\n",
      "986/986 [==============================] - 37s 38ms/step - loss: 0.0033 - accuracy: 0.9812 - val_loss: 0.0036 - val_accuracy: 0.9779\n",
      "==================================================\n",
      "Result of vggnet_model, fold 5\n",
      "Epoch: 52\n",
      "Accuracy: 0.9863013625144958\n",
      "Time taken:  1946.6316130161285\n",
      "==================================================\n",
      "Epoch 1/300\n",
      "986/986 [==============================] - 65s 66ms/step - loss: 0.0839 - accuracy: 0.2751 - val_loss: 0.0750 - val_accuracy: 0.3800\n",
      "Epoch 2/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "986/986 [==============================] - 62s 62ms/step - loss: 0.0620 - accuracy: 0.5286 - val_loss: 0.1303 - val_accuracy: 0.1337\n",
      "Epoch 3/300\n",
      "986/986 [==============================] - 62s 62ms/step - loss: 0.0446 - accuracy: 0.6838 - val_loss: 0.1086 - val_accuracy: 0.2453\n",
      "Epoch 4/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0331 - accuracy: 0.7732 - val_loss: 0.0327 - val_accuracy: 0.7712\n",
      "Epoch 5/300\n",
      "986/986 [==============================] - 62s 62ms/step - loss: 0.0266 - accuracy: 0.8232 - val_loss: 0.0370 - val_accuracy: 0.7410\n",
      "Epoch 6/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0227 - accuracy: 0.8491 - val_loss: 0.0219 - val_accuracy: 0.8534\n",
      "Epoch 7/300\n",
      "986/986 [==============================] - 62s 62ms/step - loss: 0.0202 - accuracy: 0.8650 - val_loss: 0.0266 - val_accuracy: 0.8247\n",
      "Epoch 8/300\n",
      "986/986 [==============================] - 62s 62ms/step - loss: 0.0194 - accuracy: 0.8720 - val_loss: 0.0381 - val_accuracy: 0.7314\n",
      "Epoch 9/300\n",
      "986/986 [==============================] - 62s 62ms/step - loss: 0.0166 - accuracy: 0.8904 - val_loss: 0.0331 - val_accuracy: 0.7785\n",
      "Epoch 10/300\n",
      "986/986 [==============================] - 62s 62ms/step - loss: 0.0152 - accuracy: 0.9005 - val_loss: 0.1108 - val_accuracy: 0.2783\n",
      "Epoch 11/300\n",
      "986/986 [==============================] - 62s 62ms/step - loss: 0.0140 - accuracy: 0.9094 - val_loss: 0.0371 - val_accuracy: 0.7506\n",
      "Epoch 12/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0132 - accuracy: 0.9141 - val_loss: 0.0220 - val_accuracy: 0.8620\n",
      "Epoch 13/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0118 - accuracy: 0.9238 - val_loss: 0.0132 - val_accuracy: 0.9193\n",
      "Epoch 14/300\n",
      "986/986 [==============================] - 62s 62ms/step - loss: 0.0113 - accuracy: 0.9261 - val_loss: 0.0268 - val_accuracy: 0.8194\n",
      "Epoch 15/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0106 - accuracy: 0.9316 - val_loss: 0.0188 - val_accuracy: 0.8760\n",
      "Epoch 16/300\n",
      "986/986 [==============================] - 62s 62ms/step - loss: 0.0103 - accuracy: 0.9345 - val_loss: 0.0820 - val_accuracy: 0.3625\n",
      "Epoch 17/300\n",
      "986/986 [==============================] - 62s 62ms/step - loss: 0.0097 - accuracy: 0.9394 - val_loss: 0.0166 - val_accuracy: 0.9039\n",
      "Epoch 18/300\n",
      "986/986 [==============================] - 62s 62ms/step - loss: 0.0089 - accuracy: 0.9432 - val_loss: 0.0328 - val_accuracy: 0.8014\n",
      "Epoch 19/300\n",
      "986/986 [==============================] - 62s 62ms/step - loss: 0.0088 - accuracy: 0.9429 - val_loss: 0.0145 - val_accuracy: 0.9143\n",
      "Epoch 20/300\n",
      "986/986 [==============================] - 62s 62ms/step - loss: 0.0087 - accuracy: 0.9446 - val_loss: 0.0247 - val_accuracy: 0.8389\n",
      "Epoch 21/300\n",
      "986/986 [==============================] - 62s 62ms/step - loss: 0.0084 - accuracy: 0.9461 - val_loss: 0.0498 - val_accuracy: 0.6276\n",
      "Epoch 22/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0080 - accuracy: 0.9498 - val_loss: 0.0111 - val_accuracy: 0.9346\n",
      "Epoch 23/300\n",
      "986/986 [==============================] - 62s 62ms/step - loss: 0.0077 - accuracy: 0.9512 - val_loss: 0.0225 - val_accuracy: 0.8620\n",
      "Epoch 24/300\n",
      "986/986 [==============================] - 62s 62ms/step - loss: 0.0070 - accuracy: 0.9557 - val_loss: 0.0140 - val_accuracy: 0.9186\n",
      "Epoch 25/300\n",
      "986/986 [==============================] - 62s 62ms/step - loss: 0.0067 - accuracy: 0.9576 - val_loss: 0.0130 - val_accuracy: 0.9267\n",
      "Epoch 26/300\n",
      "986/986 [==============================] - 62s 62ms/step - loss: 0.0066 - accuracy: 0.9575 - val_loss: 0.0197 - val_accuracy: 0.8838\n",
      "Epoch 27/300\n",
      "986/986 [==============================] - 62s 62ms/step - loss: 0.0067 - accuracy: 0.9561 - val_loss: 0.0152 - val_accuracy: 0.9198\n",
      "Epoch 28/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0057 - accuracy: 0.9636 - val_loss: 0.0130 - val_accuracy: 0.9353\n",
      "Epoch 29/300\n",
      "986/986 [==============================] - 62s 62ms/step - loss: 0.0061 - accuracy: 0.9612 - val_loss: 0.0150 - val_accuracy: 0.9181\n",
      "Epoch 30/300\n",
      "986/986 [==============================] - 62s 62ms/step - loss: 0.0066 - accuracy: 0.9582 - val_loss: 0.0302 - val_accuracy: 0.8064\n",
      "Epoch 31/300\n",
      "986/986 [==============================] - 63s 63ms/step - loss: 0.0063 - accuracy: 0.9595 - val_loss: 0.0479 - val_accuracy: 0.6697\n",
      "Epoch 32/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0057 - accuracy: 0.9640 - val_loss: 0.0134 - val_accuracy: 0.9320\n",
      "Epoch 33/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0048 - accuracy: 0.9703 - val_loss: 0.0130 - val_accuracy: 0.9290\n",
      "Epoch 34/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0058 - accuracy: 0.9635 - val_loss: 0.0144 - val_accuracy: 0.9376\n",
      "Epoch 35/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0047 - accuracy: 0.9710 - val_loss: 0.0137 - val_accuracy: 0.9188\n",
      "Epoch 36/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0055 - accuracy: 0.9661 - val_loss: 0.0603 - val_accuracy: 0.5457\n",
      "Epoch 37/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0051 - accuracy: 0.9682 - val_loss: 0.0104 - val_accuracy: 0.9386\n",
      "Epoch 38/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0053 - accuracy: 0.9666 - val_loss: 0.0133 - val_accuracy: 0.9272\n",
      "Epoch 39/300\n",
      "986/986 [==============================] - 62s 62ms/step - loss: 0.0051 - accuracy: 0.9682 - val_loss: 0.0117 - val_accuracy: 0.9384\n",
      "Epoch 40/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0048 - accuracy: 0.9706 - val_loss: 0.0099 - val_accuracy: 0.9503\n",
      "Epoch 41/300\n",
      "986/986 [==============================] - 61s 62ms/step - loss: 0.0048 - accuracy: 0.9711 - val_loss: 0.0097 - val_accuracy: 0.9442\n",
      "Epoch 42/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0048 - accuracy: 0.9705 - val_loss: 0.0114 - val_accuracy: 0.9330\n",
      "Epoch 43/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0043 - accuracy: 0.9722 - val_loss: 0.0131 - val_accuracy: 0.9252\n",
      "Epoch 44/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0042 - accuracy: 0.9737 - val_loss: 0.0138 - val_accuracy: 0.9247\n",
      "Epoch 45/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0046 - accuracy: 0.9710 - val_loss: 0.1045 - val_accuracy: 0.2184\n",
      "Epoch 46/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0040 - accuracy: 0.9743 - val_loss: 0.0170 - val_accuracy: 0.9061\n",
      "Epoch 47/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0042 - accuracy: 0.9729 - val_loss: 0.0116 - val_accuracy: 0.9346\n",
      "Epoch 48/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0040 - accuracy: 0.9741 - val_loss: 0.0101 - val_accuracy: 0.9455\n",
      "Epoch 49/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0038 - accuracy: 0.9767 - val_loss: 0.0123 - val_accuracy: 0.9305\n",
      "Epoch 50/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0038 - accuracy: 0.9757 - val_loss: 0.0495 - val_accuracy: 0.6286\n",
      "==================================================\n",
      "Result of resnet_model, fold 1\n",
      "Epoch: 50\n",
      "Accuracy: 0.9502790570259094\n",
      "Time taken:  3133.0771975517273\n",
      "==================================================\n",
      "Epoch 1/300\n",
      "986/986 [==============================] - 66s 66ms/step - loss: 0.0842 - accuracy: 0.2720 - val_loss: 0.1033 - val_accuracy: 0.3387\n",
      "Epoch 2/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0647 - accuracy: 0.5022 - val_loss: 0.1069 - val_accuracy: 0.1875\n",
      "Epoch 3/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0474 - accuracy: 0.6613 - val_loss: 0.1477 - val_accuracy: 0.1357\n",
      "Epoch 4/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0354 - accuracy: 0.7590 - val_loss: 0.1606 - val_accuracy: 0.1032\n",
      "Epoch 5/300\n",
      "986/986 [==============================] - 64s 64ms/step - loss: 0.0293 - accuracy: 0.8036 - val_loss: 0.0380 - val_accuracy: 0.7418\n",
      "Epoch 6/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0256 - accuracy: 0.8300 - val_loss: 0.0356 - val_accuracy: 0.7603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0221 - accuracy: 0.8525 - val_loss: 0.0575 - val_accuracy: 0.5614\n",
      "Epoch 8/300\n",
      "986/986 [==============================] - 63s 63ms/step - loss: 0.0194 - accuracy: 0.8715 - val_loss: 0.0798 - val_accuracy: 0.4614\n",
      "Epoch 9/300\n",
      "986/986 [==============================] - 65s 66ms/step - loss: 0.0184 - accuracy: 0.8793 - val_loss: 0.0322 - val_accuracy: 0.7702\n",
      "Epoch 10/300\n",
      "986/986 [==============================] - 65s 66ms/step - loss: 0.0166 - accuracy: 0.8907 - val_loss: 0.0157 - val_accuracy: 0.9099\n",
      "Epoch 11/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0151 - accuracy: 0.9019 - val_loss: 0.1002 - val_accuracy: 0.2385\n",
      "Epoch 12/300\n",
      "986/986 [==============================] - 65s 65ms/step - loss: 0.0144 - accuracy: 0.9050 - val_loss: 0.0139 - val_accuracy: 0.9188\n",
      "Epoch 13/300\n",
      "986/986 [==============================] - 68s 69ms/step - loss: 0.0134 - accuracy: 0.9127 - val_loss: 0.0096 - val_accuracy: 0.9401\n",
      "Epoch 14/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0125 - accuracy: 0.9182 - val_loss: 0.0567 - val_accuracy: 0.5946\n",
      "Epoch 15/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0121 - accuracy: 0.9232 - val_loss: 0.0124 - val_accuracy: 0.9280\n",
      "Epoch 16/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0118 - accuracy: 0.9231 - val_loss: 0.0453 - val_accuracy: 0.6689\n",
      "Epoch 17/300\n",
      "986/986 [==============================] - 65s 66ms/step - loss: 0.0107 - accuracy: 0.9311 - val_loss: 0.0110 - val_accuracy: 0.9518\n",
      "Epoch 18/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0105 - accuracy: 0.9338 - val_loss: 0.0113 - val_accuracy: 0.9389\n",
      "Epoch 19/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0096 - accuracy: 0.9387 - val_loss: 0.0152 - val_accuracy: 0.9148\n",
      "Epoch 20/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0093 - accuracy: 0.9411 - val_loss: 0.0119 - val_accuracy: 0.9409\n",
      "Epoch 21/300\n",
      "986/986 [==============================] - 65s 65ms/step - loss: 0.0095 - accuracy: 0.9394 - val_loss: 0.0160 - val_accuracy: 0.8962\n",
      "Epoch 22/300\n",
      "986/986 [==============================] - 69s 70ms/step - loss: 0.0079 - accuracy: 0.9492 - val_loss: 0.0091 - val_accuracy: 0.9594\n",
      "Epoch 23/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0089 - accuracy: 0.9427 - val_loss: 0.0102 - val_accuracy: 0.9462\n",
      "Epoch 24/300\n",
      "986/986 [==============================] - 66s 67ms/step - loss: 0.0082 - accuracy: 0.9479 - val_loss: 0.0099 - val_accuracy: 0.9462\n",
      "Epoch 25/300\n",
      "986/986 [==============================] - 68s 69ms/step - loss: 0.0080 - accuracy: 0.9492 - val_loss: 0.0135 - val_accuracy: 0.9376\n",
      "Epoch 26/300\n",
      "986/986 [==============================] - 66s 67ms/step - loss: 0.0080 - accuracy: 0.9500 - val_loss: 0.0113 - val_accuracy: 0.9384\n",
      "Epoch 27/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0074 - accuracy: 0.9533 - val_loss: 0.0137 - val_accuracy: 0.9323\n",
      "Epoch 28/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0068 - accuracy: 0.9557 - val_loss: 0.0621 - val_accuracy: 0.5185\n",
      "Epoch 29/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0070 - accuracy: 0.9563 - val_loss: 0.0145 - val_accuracy: 0.9338\n",
      "Epoch 30/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0070 - accuracy: 0.9548 - val_loss: 0.0090 - val_accuracy: 0.9528\n",
      "Epoch 31/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0070 - accuracy: 0.9550 - val_loss: 0.0081 - val_accuracy: 0.9678\n",
      "Epoch 32/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0064 - accuracy: 0.9595 - val_loss: 0.0127 - val_accuracy: 0.9333\n",
      "Epoch 33/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0066 - accuracy: 0.9585 - val_loss: 0.0079 - val_accuracy: 0.9675\n",
      "Epoch 34/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0062 - accuracy: 0.9610 - val_loss: 0.0053 - val_accuracy: 0.9782\n",
      "Epoch 35/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0062 - accuracy: 0.9610 - val_loss: 0.0090 - val_accuracy: 0.9635\n",
      "Epoch 36/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0059 - accuracy: 0.9621 - val_loss: 0.0080 - val_accuracy: 0.9655\n",
      "Epoch 37/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0054 - accuracy: 0.9659 - val_loss: 0.0078 - val_accuracy: 0.9706\n",
      "Epoch 38/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0058 - accuracy: 0.9634 - val_loss: 0.0092 - val_accuracy: 0.9718\n",
      "Epoch 39/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0054 - accuracy: 0.9652 - val_loss: 0.0660 - val_accuracy: 0.4767\n",
      "Epoch 40/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0056 - accuracy: 0.9649 - val_loss: 0.0077 - val_accuracy: 0.9683\n",
      "Epoch 41/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0056 - accuracy: 0.9651 - val_loss: 0.0067 - val_accuracy: 0.9711\n",
      "Epoch 42/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0058 - accuracy: 0.9640 - val_loss: 0.0106 - val_accuracy: 0.9721\n",
      "Epoch 43/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0048 - accuracy: 0.9706 - val_loss: 0.0117 - val_accuracy: 0.9561\n",
      "Epoch 44/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0051 - accuracy: 0.9677 - val_loss: 0.0102 - val_accuracy: 0.9645\n",
      "==================================================\n",
      "Result of resnet_model, fold 2\n",
      "Epoch: 44\n",
      "Accuracy: 0.9781836867332458\n",
      "Time taken:  2821.3436851501465\n",
      "==================================================\n",
      "Epoch 1/300\n",
      "986/986 [==============================] - 65s 66ms/step - loss: 0.0842 - accuracy: 0.2675 - val_loss: 0.0780 - val_accuracy: 0.4163\n",
      "Epoch 2/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0637 - accuracy: 0.5101 - val_loss: 0.0786 - val_accuracy: 0.4437\n",
      "Epoch 3/300\n",
      "986/986 [==============================] - 63s 63ms/step - loss: 0.0462 - accuracy: 0.6738 - val_loss: 0.1313 - val_accuracy: 0.1459\n",
      "Epoch 4/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0348 - accuracy: 0.7620 - val_loss: 0.0343 - val_accuracy: 0.7712\n",
      "Epoch 5/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0299 - accuracy: 0.7982 - val_loss: 0.0825 - val_accuracy: 0.4168\n",
      "Epoch 6/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0257 - accuracy: 0.8288 - val_loss: 0.0292 - val_accuracy: 0.8034\n",
      "Epoch 7/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0231 - accuracy: 0.8476 - val_loss: 0.0238 - val_accuracy: 0.8455\n",
      "Epoch 8/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0202 - accuracy: 0.8678 - val_loss: 0.0256 - val_accuracy: 0.8331\n",
      "Epoch 9/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0183 - accuracy: 0.8792 - val_loss: 0.0146 - val_accuracy: 0.9287\n",
      "Epoch 10/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0175 - accuracy: 0.8854 - val_loss: 0.0102 - val_accuracy: 0.9439\n",
      "Epoch 11/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0155 - accuracy: 0.8988 - val_loss: 0.0189 - val_accuracy: 0.8813\n",
      "Epoch 12/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0148 - accuracy: 0.9047 - val_loss: 0.0132 - val_accuracy: 0.9196\n",
      "Epoch 13/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0134 - accuracy: 0.9144 - val_loss: 0.0098 - val_accuracy: 0.9505\n",
      "Epoch 14/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0130 - accuracy: 0.9162 - val_loss: 0.0168 - val_accuracy: 0.9092\n",
      "Epoch 15/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0124 - accuracy: 0.9205 - val_loss: 0.0122 - val_accuracy: 0.9277\n",
      "Epoch 16/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0119 - accuracy: 0.9207 - val_loss: 0.0150 - val_accuracy: 0.9125\n",
      "Epoch 17/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0111 - accuracy: 0.9292 - val_loss: 0.0200 - val_accuracy: 0.8800\n",
      "Epoch 18/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0109 - accuracy: 0.9313 - val_loss: 0.0173 - val_accuracy: 0.9054\n",
      "Epoch 19/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0099 - accuracy: 0.9368 - val_loss: 0.0068 - val_accuracy: 0.9670\n",
      "Epoch 20/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0098 - accuracy: 0.9368 - val_loss: 0.0089 - val_accuracy: 0.9642\n",
      "Epoch 21/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0093 - accuracy: 0.9406 - val_loss: 0.0124 - val_accuracy: 0.9396\n",
      "Epoch 22/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0089 - accuracy: 0.9431 - val_loss: 0.0222 - val_accuracy: 0.8694\n",
      "Epoch 23/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0091 - accuracy: 0.9427 - val_loss: 0.0093 - val_accuracy: 0.9599\n",
      "Epoch 24/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0086 - accuracy: 0.9453 - val_loss: 0.0098 - val_accuracy: 0.9554\n",
      "Epoch 25/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0074 - accuracy: 0.9529 - val_loss: 0.0678 - val_accuracy: 0.4772\n",
      "Epoch 26/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0082 - accuracy: 0.9490 - val_loss: 0.0072 - val_accuracy: 0.9736\n",
      "Epoch 27/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0078 - accuracy: 0.9515 - val_loss: 0.0168 - val_accuracy: 0.9107\n",
      "Epoch 28/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0077 - accuracy: 0.9509 - val_loss: 0.0106 - val_accuracy: 0.9523\n",
      "Epoch 29/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0068 - accuracy: 0.9574 - val_loss: 0.0066 - val_accuracy: 0.9708\n",
      "Epoch 30/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0072 - accuracy: 0.9544 - val_loss: 0.0576 - val_accuracy: 0.5726\n",
      "Epoch 31/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0065 - accuracy: 0.9590 - val_loss: 0.0174 - val_accuracy: 0.9064\n",
      "Epoch 32/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0066 - accuracy: 0.9585 - val_loss: 0.0069 - val_accuracy: 0.9767\n",
      "Epoch 33/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0065 - accuracy: 0.9599 - val_loss: 0.0108 - val_accuracy: 0.9566\n",
      "Epoch 34/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0065 - accuracy: 0.9591 - val_loss: 0.0083 - val_accuracy: 0.9713\n",
      "Epoch 35/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0058 - accuracy: 0.9640 - val_loss: 0.0619 - val_accuracy: 0.5066\n",
      "Epoch 36/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0064 - accuracy: 0.9585 - val_loss: 0.0077 - val_accuracy: 0.9716\n",
      "Epoch 37/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0058 - accuracy: 0.9632 - val_loss: 0.0103 - val_accuracy: 0.9647\n",
      "Epoch 38/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0058 - accuracy: 0.9632 - val_loss: 0.0212 - val_accuracy: 0.8851\n",
      "Epoch 39/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0065 - accuracy: 0.9597 - val_loss: 0.0076 - val_accuracy: 0.9723\n",
      "Epoch 40/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0055 - accuracy: 0.9658 - val_loss: 0.0185 - val_accuracy: 0.8993\n",
      "Epoch 41/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0051 - accuracy: 0.9671 - val_loss: 0.0518 - val_accuracy: 0.6162\n",
      "Epoch 42/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0055 - accuracy: 0.9663 - val_loss: 0.0074 - val_accuracy: 0.9759\n",
      "==================================================\n",
      "Result of resnet_model, fold 3\n",
      "Epoch: 42\n",
      "Accuracy: 0.9766616225242615\n",
      "Time taken:  2639.529753923416\n",
      "==================================================\n",
      "Epoch 1/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0842 - accuracy: 0.2719 - val_loss: 0.0869 - val_accuracy: 0.3673\n",
      "Epoch 2/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0633 - accuracy: 0.5140 - val_loss: 0.0617 - val_accuracy: 0.5642\n",
      "Epoch 3/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0455 - accuracy: 0.6804 - val_loss: 0.0686 - val_accuracy: 0.5221\n",
      "Epoch 4/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0341 - accuracy: 0.7701 - val_loss: 0.1734 - val_accuracy: 0.0926\n",
      "Epoch 5/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0291 - accuracy: 0.8045 - val_loss: 0.0299 - val_accuracy: 0.8009\n",
      "Epoch 6/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0256 - accuracy: 0.8291 - val_loss: 0.0285 - val_accuracy: 0.8120\n",
      "Epoch 7/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0231 - accuracy: 0.8479 - val_loss: 0.0295 - val_accuracy: 0.8044\n",
      "Epoch 8/300\n",
      "986/986 [==============================] - 65s 66ms/step - loss: 0.0200 - accuracy: 0.8657 - val_loss: 0.0206 - val_accuracy: 0.8633\n",
      "Epoch 9/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0185 - accuracy: 0.8797 - val_loss: 0.0417 - val_accuracy: 0.7311\n",
      "Epoch 10/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0169 - accuracy: 0.8895 - val_loss: 0.0275 - val_accuracy: 0.8153\n",
      "Epoch 11/300\n",
      "986/986 [==============================] - 65s 66ms/step - loss: 0.0151 - accuracy: 0.9003 - val_loss: 0.0164 - val_accuracy: 0.9234\n",
      "Epoch 12/300\n",
      "986/986 [==============================] - 65s 66ms/step - loss: 0.0142 - accuracy: 0.9075 - val_loss: 0.0130 - val_accuracy: 0.9259\n",
      "Epoch 13/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0139 - accuracy: 0.9093 - val_loss: 0.0234 - val_accuracy: 0.8584\n",
      "Epoch 14/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0121 - accuracy: 0.9218 - val_loss: 0.0718 - val_accuracy: 0.4594\n",
      "Epoch 15/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0119 - accuracy: 0.9231 - val_loss: 0.0232 - val_accuracy: 0.8678\n",
      "Epoch 16/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0115 - accuracy: 0.9245 - val_loss: 0.0191 - val_accuracy: 0.8785\n",
      "Epoch 17/300\n",
      "986/986 [==============================] - 67s 68ms/step - loss: 0.0105 - accuracy: 0.9327 - val_loss: 0.0174 - val_accuracy: 0.9272\n",
      "Epoch 18/300\n",
      "986/986 [==============================] - 67s 68ms/step - loss: 0.0100 - accuracy: 0.9373 - val_loss: 0.0137 - val_accuracy: 0.9424\n",
      "Epoch 19/300\n",
      "986/986 [==============================] - 65s 66ms/step - loss: 0.0098 - accuracy: 0.9373 - val_loss: 0.0092 - val_accuracy: 0.9571\n",
      "Epoch 20/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0090 - accuracy: 0.9439 - val_loss: 0.0152 - val_accuracy: 0.9173\n",
      "Epoch 21/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0089 - accuracy: 0.9440 - val_loss: 0.0534 - val_accuracy: 0.6045\n",
      "Epoch 22/300\n",
      "986/986 [==============================] - 66s 67ms/step - loss: 0.0083 - accuracy: 0.9477 - val_loss: 0.0093 - val_accuracy: 0.9652\n",
      "Epoch 23/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0082 - accuracy: 0.9486 - val_loss: 0.0131 - val_accuracy: 0.9318\n",
      "Epoch 24/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0087 - accuracy: 0.9441 - val_loss: 0.0084 - val_accuracy: 0.9609\n",
      "Epoch 25/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0079 - accuracy: 0.9493 - val_loss: 0.0368 - val_accuracy: 0.7486\n",
      "Epoch 26/300\n",
      "986/986 [==============================] - 65s 66ms/step - loss: 0.0074 - accuracy: 0.9532 - val_loss: 0.0094 - val_accuracy: 0.9665\n",
      "Epoch 27/300\n",
      "986/986 [==============================] - 65s 66ms/step - loss: 0.0084 - accuracy: 0.9470 - val_loss: 0.0443 - val_accuracy: 0.6738\n",
      "Epoch 28/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0069 - accuracy: 0.9567 - val_loss: 0.0126 - val_accuracy: 0.9493\n",
      "Epoch 29/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0071 - accuracy: 0.9550 - val_loss: 0.0072 - val_accuracy: 0.9655\n",
      "Epoch 30/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0070 - accuracy: 0.9553 - val_loss: 0.0132 - val_accuracy: 0.9427\n",
      "Epoch 31/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0071 - accuracy: 0.9551 - val_loss: 0.0120 - val_accuracy: 0.9444\n",
      "Epoch 32/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0070 - accuracy: 0.9560 - val_loss: 0.0125 - val_accuracy: 0.9399\n",
      "Epoch 33/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0064 - accuracy: 0.9602 - val_loss: 0.0079 - val_accuracy: 0.9708\n",
      "Epoch 34/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0061 - accuracy: 0.9623 - val_loss: 0.0064 - val_accuracy: 0.9754\n",
      "Epoch 35/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0060 - accuracy: 0.9624 - val_loss: 0.0133 - val_accuracy: 0.9292\n",
      "Epoch 36/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0059 - accuracy: 0.9631 - val_loss: 0.0288 - val_accuracy: 0.8270\n",
      "Epoch 37/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0061 - accuracy: 0.9621 - val_loss: 0.0083 - val_accuracy: 0.9685\n",
      "Epoch 38/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0055 - accuracy: 0.9655 - val_loss: 0.0102 - val_accuracy: 0.9609\n",
      "Epoch 39/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0057 - accuracy: 0.9640 - val_loss: 0.0093 - val_accuracy: 0.9701\n",
      "Epoch 40/300\n",
      "986/986 [==============================] - 65s 66ms/step - loss: 0.0051 - accuracy: 0.9681 - val_loss: 0.0072 - val_accuracy: 0.9782\n",
      "Epoch 41/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0054 - accuracy: 0.9652 - val_loss: 0.0066 - val_accuracy: 0.9802\n",
      "Epoch 42/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0052 - accuracy: 0.9668 - val_loss: 0.0119 - val_accuracy: 0.9581\n",
      "Epoch 43/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0049 - accuracy: 0.9693 - val_loss: 0.0073 - val_accuracy: 0.9696\n",
      "Epoch 44/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0046 - accuracy: 0.9717 - val_loss: 0.0103 - val_accuracy: 0.9609\n",
      "Epoch 45/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0051 - accuracy: 0.9674 - val_loss: 0.0131 - val_accuracy: 0.9630\n",
      "Epoch 46/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0046 - accuracy: 0.9708 - val_loss: 0.0096 - val_accuracy: 0.9637\n",
      "Epoch 47/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0053 - accuracy: 0.9671 - val_loss: 0.0063 - val_accuracy: 0.9762\n",
      "Epoch 48/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0049 - accuracy: 0.9697 - val_loss: 0.0062 - val_accuracy: 0.9795\n",
      "Epoch 49/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0047 - accuracy: 0.9710 - val_loss: 0.0063 - val_accuracy: 0.9759\n",
      "Epoch 50/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0049 - accuracy: 0.9703 - val_loss: 0.0134 - val_accuracy: 0.9462\n",
      "Epoch 51/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0044 - accuracy: 0.9718 - val_loss: 0.0077 - val_accuracy: 0.9703\n",
      "==================================================\n",
      "Result of resnet_model, fold 4\n",
      "Epoch: 51\n",
      "Accuracy: 0.9802131056785583\n",
      "Time taken:  3274.931854724884\n",
      "==================================================\n",
      "Epoch 1/300\n",
      "986/986 [==============================] - 66s 67ms/step - loss: 0.0840 - accuracy: 0.2686 - val_loss: 0.0790 - val_accuracy: 0.3995\n",
      "Epoch 2/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0650 - accuracy: 0.4955 - val_loss: 0.1027 - val_accuracy: 0.3744\n",
      "Epoch 3/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0467 - accuracy: 0.6675 - val_loss: 0.1186 - val_accuracy: 0.2189\n",
      "Epoch 4/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0348 - accuracy: 0.7621 - val_loss: 0.1685 - val_accuracy: 0.1162\n",
      "Epoch 5/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0297 - accuracy: 0.8007 - val_loss: 0.0666 - val_accuracy: 0.5571\n",
      "Epoch 6/300\n",
      "986/986 [==============================] - 65s 65ms/step - loss: 0.0247 - accuracy: 0.8364 - val_loss: 0.0466 - val_accuracy: 0.6804\n",
      "Epoch 7/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0225 - accuracy: 0.8498 - val_loss: 0.0219 - val_accuracy: 0.8623\n",
      "Epoch 8/300\n",
      "986/986 [==============================] - 63s 64ms/step - loss: 0.0202 - accuracy: 0.8649 - val_loss: 0.0258 - val_accuracy: 0.8341\n",
      "Epoch 9/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0180 - accuracy: 0.8835 - val_loss: 0.0223 - val_accuracy: 0.8579\n",
      "Epoch 10/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0164 - accuracy: 0.8937 - val_loss: 0.0306 - val_accuracy: 0.7948\n",
      "Epoch 11/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0146 - accuracy: 0.9055 - val_loss: 0.0144 - val_accuracy: 0.9153\n",
      "Epoch 12/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0144 - accuracy: 0.9056 - val_loss: 0.0136 - val_accuracy: 0.9285\n",
      "Epoch 13/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0134 - accuracy: 0.9131 - val_loss: 0.0112 - val_accuracy: 0.9450\n",
      "Epoch 14/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0128 - accuracy: 0.9167 - val_loss: 0.0188 - val_accuracy: 0.8917\n",
      "Epoch 15/300\n",
      "986/986 [==============================] - 64s 64ms/step - loss: 0.0112 - accuracy: 0.9268 - val_loss: 0.0115 - val_accuracy: 0.9467\n",
      "Epoch 16/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0114 - accuracy: 0.9254 - val_loss: 0.0197 - val_accuracy: 0.8792\n",
      "Epoch 17/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0107 - accuracy: 0.9303 - val_loss: 0.0160 - val_accuracy: 0.9236\n",
      "Epoch 18/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0099 - accuracy: 0.9366 - val_loss: 0.0516 - val_accuracy: 0.6220\n",
      "Epoch 19/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0099 - accuracy: 0.9355 - val_loss: 0.0131 - val_accuracy: 0.9500\n",
      "Epoch 20/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0100 - accuracy: 0.9378 - val_loss: 0.0152 - val_accuracy: 0.9191\n",
      "Epoch 21/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0092 - accuracy: 0.9423 - val_loss: 0.0125 - val_accuracy: 0.9432\n",
      "Epoch 22/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0088 - accuracy: 0.9442 - val_loss: 0.0124 - val_accuracy: 0.9371\n",
      "Epoch 23/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0085 - accuracy: 0.9467 - val_loss: 0.0156 - val_accuracy: 0.9269\n",
      "Epoch 24/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0084 - accuracy: 0.9475 - val_loss: 0.0249 - val_accuracy: 0.8612\n",
      "Epoch 25/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0074 - accuracy: 0.9529 - val_loss: 0.0113 - val_accuracy: 0.9376\n",
      "Epoch 26/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0072 - accuracy: 0.9543 - val_loss: 0.0172 - val_accuracy: 0.9033\n",
      "Epoch 27/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0076 - accuracy: 0.9533 - val_loss: 0.0195 - val_accuracy: 0.8980\n",
      "Epoch 28/300\n",
      "986/986 [==============================] - 64s 65ms/step - loss: 0.0073 - accuracy: 0.9542 - val_loss: 0.0093 - val_accuracy: 0.9731\n",
      "Epoch 29/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0070 - accuracy: 0.9560 - val_loss: 0.0136 - val_accuracy: 0.9404\n",
      "Epoch 30/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0069 - accuracy: 0.9558 - val_loss: 0.0108 - val_accuracy: 0.9673\n",
      "Epoch 31/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0067 - accuracy: 0.9575 - val_loss: 0.0233 - val_accuracy: 0.8671\n",
      "Epoch 32/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0066 - accuracy: 0.9586 - val_loss: 0.0072 - val_accuracy: 0.9706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0061 - accuracy: 0.9617 - val_loss: 0.0603 - val_accuracy: 0.5518\n",
      "Epoch 34/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0069 - accuracy: 0.9573 - val_loss: 0.0135 - val_accuracy: 0.9338\n",
      "Epoch 35/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0060 - accuracy: 0.9628 - val_loss: 0.0089 - val_accuracy: 0.9584\n",
      "Epoch 36/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0063 - accuracy: 0.9613 - val_loss: 0.0103 - val_accuracy: 0.9632\n",
      "Epoch 37/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0056 - accuracy: 0.9649 - val_loss: 0.0149 - val_accuracy: 0.9371\n",
      "Epoch 38/300\n",
      "986/986 [==============================] - 62s 63ms/step - loss: 0.0055 - accuracy: 0.9661 - val_loss: 0.0090 - val_accuracy: 0.9706\n",
      "==================================================\n",
      "Result of resnet_model, fold 5\n",
      "Epoch: 38\n",
      "Accuracy: 0.9731100797653198\n",
      "Time taken:  2403.983068704605\n",
      "==================================================\n",
      "Epoch 1/300\n",
      "986/986 [==============================] - 60s 61ms/step - loss: 0.0722 - accuracy: 0.4091 - val_loss: 0.1351 - val_accuracy: 0.1304\n",
      "Epoch 2/300\n",
      "986/986 [==============================] - 57s 57ms/step - loss: 0.0366 - accuracy: 0.7423 - val_loss: 0.1058 - val_accuracy: 0.1814\n",
      "Epoch 3/300\n",
      "986/986 [==============================] - 57s 57ms/step - loss: 0.0250 - accuracy: 0.8316 - val_loss: 0.0783 - val_accuracy: 0.4173\n",
      "Epoch 4/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0201 - accuracy: 0.8654 - val_loss: 0.1215 - val_accuracy: 0.1240\n",
      "Epoch 5/300\n",
      "986/986 [==============================] - 57s 57ms/step - loss: 0.0170 - accuracy: 0.8855 - val_loss: 0.0656 - val_accuracy: 0.4863\n",
      "Epoch 6/300\n",
      "986/986 [==============================] - 57s 57ms/step - loss: 0.0160 - accuracy: 0.8950 - val_loss: 0.0410 - val_accuracy: 0.7050\n",
      "Epoch 7/300\n",
      "986/986 [==============================] - 57s 57ms/step - loss: 0.0136 - accuracy: 0.9100 - val_loss: 0.0182 - val_accuracy: 0.9003\n",
      "Epoch 8/300\n",
      "986/986 [==============================] - 57s 57ms/step - loss: 0.0122 - accuracy: 0.9197 - val_loss: 0.0155 - val_accuracy: 0.9115\n",
      "Epoch 9/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0124 - accuracy: 0.9178 - val_loss: 0.0142 - val_accuracy: 0.9115\n",
      "Epoch 10/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0112 - accuracy: 0.9267 - val_loss: 0.0153 - val_accuracy: 0.9107\n",
      "Epoch 11/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0107 - accuracy: 0.9302 - val_loss: 0.1062 - val_accuracy: 0.1667\n",
      "Epoch 12/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0102 - accuracy: 0.9326 - val_loss: 0.0835 - val_accuracy: 0.3285\n",
      "Epoch 13/300\n",
      "986/986 [==============================] - 57s 58ms/step - loss: 0.0092 - accuracy: 0.9398 - val_loss: 0.0132 - val_accuracy: 0.9267\n",
      "Epoch 14/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0091 - accuracy: 0.9404 - val_loss: 0.0222 - val_accuracy: 0.8691\n",
      "Epoch 15/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0086 - accuracy: 0.9444 - val_loss: 0.0734 - val_accuracy: 0.4148\n",
      "Epoch 16/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0082 - accuracy: 0.9452 - val_loss: 0.0441 - val_accuracy: 0.6989\n",
      "Epoch 17/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0073 - accuracy: 0.9522 - val_loss: 0.0704 - val_accuracy: 0.4399\n",
      "Epoch 18/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0073 - accuracy: 0.9514 - val_loss: 0.0179 - val_accuracy: 0.8856\n",
      "Epoch 19/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0073 - accuracy: 0.9538 - val_loss: 0.0350 - val_accuracy: 0.7577\n",
      "Epoch 20/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0070 - accuracy: 0.9540 - val_loss: 0.0257 - val_accuracy: 0.8356\n",
      "Epoch 21/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0065 - accuracy: 0.9578 - val_loss: 0.0637 - val_accuracy: 0.5183\n",
      "Epoch 22/300\n",
      "986/986 [==============================] - 57s 58ms/step - loss: 0.0064 - accuracy: 0.9593 - val_loss: 0.0119 - val_accuracy: 0.9323\n",
      "Epoch 23/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0062 - accuracy: 0.9605 - val_loss: 0.0137 - val_accuracy: 0.9117\n",
      "Epoch 24/300\n",
      "986/986 [==============================] - 57s 58ms/step - loss: 0.0064 - accuracy: 0.9590 - val_loss: 0.0094 - val_accuracy: 0.9409\n",
      "Epoch 25/300\n",
      "986/986 [==============================] - 57s 58ms/step - loss: 0.0056 - accuracy: 0.9651 - val_loss: 0.0090 - val_accuracy: 0.9493\n",
      "Epoch 26/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0057 - accuracy: 0.9621 - val_loss: 0.0108 - val_accuracy: 0.9389\n",
      "Epoch 27/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0057 - accuracy: 0.9632 - val_loss: 0.0109 - val_accuracy: 0.9373\n",
      "Epoch 28/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0054 - accuracy: 0.9656 - val_loss: 0.0567 - val_accuracy: 0.5500\n",
      "Epoch 29/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0053 - accuracy: 0.9663 - val_loss: 0.0094 - val_accuracy: 0.9450\n",
      "Epoch 30/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0056 - accuracy: 0.9636 - val_loss: 0.0094 - val_accuracy: 0.9477\n",
      "Epoch 31/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0046 - accuracy: 0.9708 - val_loss: 0.0479 - val_accuracy: 0.6398\n",
      "Epoch 32/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0044 - accuracy: 0.9719 - val_loss: 0.0179 - val_accuracy: 0.8869\n",
      "Epoch 33/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0053 - accuracy: 0.9650 - val_loss: 0.0107 - val_accuracy: 0.9358\n",
      "Epoch 34/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0049 - accuracy: 0.9689 - val_loss: 0.0091 - val_accuracy: 0.9465\n",
      "Epoch 35/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0046 - accuracy: 0.9699 - val_loss: 0.0384 - val_accuracy: 0.7357\n",
      "==================================================\n",
      "Result of densenet_model, fold 1\n",
      "Epoch: 35\n",
      "Accuracy: 0.9492643475532532\n",
      "Time taken:  1984.5696098804474\n",
      "==================================================\n",
      "Epoch 1/300\n",
      "986/986 [==============================] - 58s 59ms/step - loss: 0.0735 - accuracy: 0.3968 - val_loss: 0.0841 - val_accuracy: 0.2900\n",
      "Epoch 2/300\n",
      "986/986 [==============================] - 57s 58ms/step - loss: 0.0371 - accuracy: 0.7411 - val_loss: 0.0446 - val_accuracy: 0.6763\n",
      "Epoch 3/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0259 - accuracy: 0.8231 - val_loss: 0.1545 - val_accuracy: 0.1053\n",
      "Epoch 4/300\n",
      "986/986 [==============================] - 57s 58ms/step - loss: 0.0219 - accuracy: 0.8509 - val_loss: 0.0200 - val_accuracy: 0.8760\n",
      "Epoch 5/300\n",
      "986/986 [==============================] - 57s 58ms/step - loss: 0.0184 - accuracy: 0.8756 - val_loss: 0.0207 - val_accuracy: 0.8780\n",
      "Epoch 6/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0161 - accuracy: 0.8914 - val_loss: 0.0265 - val_accuracy: 0.8526\n",
      "Epoch 7/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0156 - accuracy: 0.8968 - val_loss: 0.0230 - val_accuracy: 0.8635\n",
      "Epoch 8/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0145 - accuracy: 0.9042 - val_loss: 0.0535 - val_accuracy: 0.6002\n",
      "Epoch 9/300\n",
      "986/986 [==============================] - 57s 58ms/step - loss: 0.0134 - accuracy: 0.9123 - val_loss: 0.0166 - val_accuracy: 0.9300\n",
      "Epoch 10/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0125 - accuracy: 0.9188 - val_loss: 0.0271 - val_accuracy: 0.8422\n",
      "Epoch 11/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0118 - accuracy: 0.9233 - val_loss: 0.0455 - val_accuracy: 0.6575\n",
      "Epoch 12/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0111 - accuracy: 0.9273 - val_loss: 0.0182 - val_accuracy: 0.8861\n",
      "Epoch 13/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0106 - accuracy: 0.9308 - val_loss: 0.0152 - val_accuracy: 0.9186\n",
      "Epoch 14/300\n",
      "986/986 [==============================] - 57s 58ms/step - loss: 0.0104 - accuracy: 0.9318 - val_loss: 0.0086 - val_accuracy: 0.9597\n",
      "Epoch 15/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0102 - accuracy: 0.9346 - val_loss: 0.0434 - val_accuracy: 0.6826\n",
      "Epoch 16/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0090 - accuracy: 0.9398 - val_loss: 0.0116 - val_accuracy: 0.9351\n",
      "Epoch 17/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0086 - accuracy: 0.9441 - val_loss: 0.0112 - val_accuracy: 0.9346\n",
      "Epoch 18/300\n",
      "986/986 [==============================] - 57s 58ms/step - loss: 0.0089 - accuracy: 0.9417 - val_loss: 0.0050 - val_accuracy: 0.9777\n",
      "Epoch 19/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0084 - accuracy: 0.9444 - val_loss: 0.0938 - val_accuracy: 0.1692\n",
      "Epoch 20/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0078 - accuracy: 0.9484 - val_loss: 0.0083 - val_accuracy: 0.9617\n",
      "Epoch 21/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0077 - accuracy: 0.9512 - val_loss: 0.0337 - val_accuracy: 0.7773\n",
      "Epoch 22/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0074 - accuracy: 0.9515 - val_loss: 0.0083 - val_accuracy: 0.9594\n",
      "Epoch 23/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0072 - accuracy: 0.9540 - val_loss: 0.0122 - val_accuracy: 0.9363\n",
      "Epoch 24/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0073 - accuracy: 0.9535 - val_loss: 0.0088 - val_accuracy: 0.9592\n",
      "Epoch 25/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0073 - accuracy: 0.9528 - val_loss: 0.0142 - val_accuracy: 0.9137\n",
      "Epoch 26/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0066 - accuracy: 0.9579 - val_loss: 0.0180 - val_accuracy: 0.8871\n",
      "Epoch 27/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0067 - accuracy: 0.9564 - val_loss: 0.0066 - val_accuracy: 0.9665\n",
      "Epoch 28/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0066 - accuracy: 0.9577 - val_loss: 0.0066 - val_accuracy: 0.9670\n",
      "==================================================\n",
      "Result of densenet_model, fold 2\n",
      "Epoch: 28\n",
      "Accuracy: 0.9776763319969177\n",
      "Time taken:  1589.0482869148254\n",
      "==================================================\n",
      "Epoch 1/300\n",
      "986/986 [==============================] - 58s 59ms/step - loss: 0.0740 - accuracy: 0.3930 - val_loss: 0.0860 - val_accuracy: 0.3516\n",
      "Epoch 2/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0385 - accuracy: 0.7303 - val_loss: 0.1055 - val_accuracy: 0.2035\n",
      "Epoch 3/300\n",
      "986/986 [==============================] - 57s 58ms/step - loss: 0.0267 - accuracy: 0.8191 - val_loss: 0.0624 - val_accuracy: 0.5241\n",
      "Epoch 4/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0215 - accuracy: 0.8568 - val_loss: 0.0749 - val_accuracy: 0.4013\n",
      "Epoch 5/300\n",
      "986/986 [==============================] - 57s 58ms/step - loss: 0.0185 - accuracy: 0.8750 - val_loss: 0.0228 - val_accuracy: 0.8666\n",
      "Epoch 6/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0169 - accuracy: 0.8876 - val_loss: 0.0996 - val_accuracy: 0.2179\n",
      "Epoch 7/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0157 - accuracy: 0.8967 - val_loss: 0.0906 - val_accuracy: 0.1839\n",
      "Epoch 8/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0142 - accuracy: 0.9068 - val_loss: 0.1333 - val_accuracy: 0.1225\n",
      "Epoch 9/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0140 - accuracy: 0.9064 - val_loss: 0.0332 - val_accuracy: 0.7775\n",
      "Epoch 10/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0125 - accuracy: 0.9196 - val_loss: 0.1155 - val_accuracy: 0.1324\n",
      "Epoch 11/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0121 - accuracy: 0.9206 - val_loss: 0.0261 - val_accuracy: 0.8252\n",
      "Epoch 12/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0106 - accuracy: 0.9307 - val_loss: 0.0570 - val_accuracy: 0.5644\n",
      "Epoch 13/300\n",
      "986/986 [==============================] - 57s 58ms/step - loss: 0.0105 - accuracy: 0.9321 - val_loss: 0.0104 - val_accuracy: 0.9584\n",
      "Epoch 14/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0102 - accuracy: 0.9345 - val_loss: 0.0329 - val_accuracy: 0.7887\n",
      "Epoch 15/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0104 - accuracy: 0.9319 - val_loss: 0.0206 - val_accuracy: 0.9115\n",
      "Epoch 16/300\n",
      "986/986 [==============================] - 57s 58ms/step - loss: 0.0095 - accuracy: 0.9364 - val_loss: 0.0063 - val_accuracy: 0.9706\n",
      "Epoch 17/300\n",
      "986/986 [==============================] - 57s 58ms/step - loss: 0.0084 - accuracy: 0.9454 - val_loss: 0.0052 - val_accuracy: 0.9797\n",
      "Epoch 18/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0087 - accuracy: 0.9428 - val_loss: 0.0259 - val_accuracy: 0.8678\n",
      "Epoch 19/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0082 - accuracy: 0.9467 - val_loss: 0.0504 - val_accuracy: 0.6215\n",
      "Epoch 20/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0086 - accuracy: 0.9439 - val_loss: 0.0702 - val_accuracy: 0.4485\n",
      "Epoch 21/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0078 - accuracy: 0.9499 - val_loss: 0.0895 - val_accuracy: 0.2204\n",
      "Epoch 22/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0075 - accuracy: 0.9517 - val_loss: 0.0049 - val_accuracy: 0.9769\n",
      "Epoch 23/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0072 - accuracy: 0.9533 - val_loss: 0.0688 - val_accuracy: 0.4756\n",
      "Epoch 24/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0070 - accuracy: 0.9558 - val_loss: 0.0860 - val_accuracy: 0.3196\n",
      "Epoch 25/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0073 - accuracy: 0.9524 - val_loss: 0.0184 - val_accuracy: 0.8955\n",
      "Epoch 26/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0069 - accuracy: 0.9559 - val_loss: 0.0690 - val_accuracy: 0.4422\n",
      "Epoch 27/300\n",
      "986/986 [==============================] - 57s 58ms/step - loss: 0.0068 - accuracy: 0.9555 - val_loss: 0.0048 - val_accuracy: 0.9800\n",
      "Epoch 28/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0064 - accuracy: 0.9588 - val_loss: 0.0276 - val_accuracy: 0.8146\n",
      "Epoch 29/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0062 - accuracy: 0.9592 - val_loss: 0.0588 - val_accuracy: 0.5512\n",
      "Epoch 30/300\n",
      "986/986 [==============================] - 57s 58ms/step - loss: 0.0065 - accuracy: 0.9583 - val_loss: 0.0030 - val_accuracy: 0.9886\n",
      "Epoch 31/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0063 - accuracy: 0.9593 - val_loss: 0.0151 - val_accuracy: 0.9234\n",
      "Epoch 32/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0057 - accuracy: 0.9629 - val_loss: 0.0824 - val_accuracy: 0.3696\n",
      "Epoch 33/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0059 - accuracy: 0.9623 - val_loss: 0.0046 - val_accuracy: 0.9795\n",
      "Epoch 34/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0058 - accuracy: 0.9621 - val_loss: 0.0035 - val_accuracy: 0.9871\n",
      "Epoch 35/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0049 - accuracy: 0.9682 - val_loss: 0.0987 - val_accuracy: 0.2171\n",
      "Epoch 36/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0054 - accuracy: 0.9661 - val_loss: 0.0036 - val_accuracy: 0.9815\n",
      "Epoch 37/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0053 - accuracy: 0.9660 - val_loss: 0.0031 - val_accuracy: 0.9850\n",
      "Epoch 38/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0055 - accuracy: 0.9639 - val_loss: 0.0487 - val_accuracy: 0.6279\n",
      "Epoch 39/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0048 - accuracy: 0.9680 - val_loss: 0.0225 - val_accuracy: 0.8559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0052 - accuracy: 0.9672 - val_loss: 0.0050 - val_accuracy: 0.9744\n",
      "==================================================\n",
      "Result of densenet_model, fold 3\n",
      "Epoch: 40\n",
      "Accuracy: 0.9885844588279724\n",
      "Time taken:  2265.4392750263214\n",
      "==================================================\n",
      "Epoch 1/300\n",
      "986/986 [==============================] - 58s 59ms/step - loss: 0.0748 - accuracy: 0.3857 - val_loss: 0.0680 - val_accuracy: 0.4658\n",
      "Epoch 2/300\n",
      "986/986 [==============================] - 57s 57ms/step - loss: 0.0389 - accuracy: 0.7275 - val_loss: 0.0501 - val_accuracy: 0.6443\n",
      "Epoch 3/300\n",
      "986/986 [==============================] - 57s 57ms/step - loss: 0.0268 - accuracy: 0.8186 - val_loss: 0.0293 - val_accuracy: 0.7965\n",
      "Epoch 4/300\n",
      "986/986 [==============================] - 57s 58ms/step - loss: 0.0221 - accuracy: 0.8482 - val_loss: 0.0243 - val_accuracy: 0.8323\n",
      "Epoch 5/300\n",
      "986/986 [==============================] - 57s 58ms/step - loss: 0.0189 - accuracy: 0.8719 - val_loss: 0.0168 - val_accuracy: 0.8973\n",
      "Epoch 6/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0167 - accuracy: 0.8887 - val_loss: 0.1061 - val_accuracy: 0.2151\n",
      "Epoch 7/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0150 - accuracy: 0.9006 - val_loss: 0.1469 - val_accuracy: 0.1002\n",
      "Epoch 8/300\n",
      "986/986 [==============================] - 57s 58ms/step - loss: 0.0144 - accuracy: 0.9041 - val_loss: 0.0144 - val_accuracy: 0.9092\n",
      "Epoch 9/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0133 - accuracy: 0.9122 - val_loss: 0.0866 - val_accuracy: 0.3029\n",
      "Epoch 10/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0124 - accuracy: 0.9198 - val_loss: 0.0611 - val_accuracy: 0.5266\n",
      "Epoch 11/300\n",
      "986/986 [==============================] - 57s 58ms/step - loss: 0.0122 - accuracy: 0.9200 - val_loss: 0.0133 - val_accuracy: 0.9348\n",
      "Epoch 12/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0107 - accuracy: 0.9292 - val_loss: 0.1021 - val_accuracy: 0.1913\n",
      "Epoch 13/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0110 - accuracy: 0.9292 - val_loss: 0.0391 - val_accuracy: 0.7215\n",
      "Epoch 14/300\n",
      "986/986 [==============================] - 57s 57ms/step - loss: 0.0098 - accuracy: 0.9368 - val_loss: 0.0048 - val_accuracy: 0.9784\n",
      "Epoch 15/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0097 - accuracy: 0.9365 - val_loss: 0.0467 - val_accuracy: 0.6705\n",
      "Epoch 16/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0086 - accuracy: 0.9451 - val_loss: 0.1060 - val_accuracy: 0.1542\n",
      "Epoch 17/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0089 - accuracy: 0.9418 - val_loss: 0.0046 - val_accuracy: 0.9779\n",
      "Epoch 18/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0086 - accuracy: 0.9448 - val_loss: 0.0636 - val_accuracy: 0.5137\n",
      "Epoch 19/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0082 - accuracy: 0.9463 - val_loss: 0.0062 - val_accuracy: 0.9701\n",
      "Epoch 20/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0080 - accuracy: 0.9484 - val_loss: 0.0105 - val_accuracy: 0.9348\n",
      "Epoch 21/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0075 - accuracy: 0.9514 - val_loss: 0.0858 - val_accuracy: 0.3232\n",
      "Epoch 22/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0073 - accuracy: 0.9538 - val_loss: 0.0068 - val_accuracy: 0.9708\n",
      "Epoch 23/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0071 - accuracy: 0.9548 - val_loss: 0.0062 - val_accuracy: 0.9718\n",
      "Epoch 24/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0075 - accuracy: 0.9514 - val_loss: 0.0082 - val_accuracy: 0.9574\n",
      "==================================================\n",
      "Result of densenet_model, fold 4\n",
      "Epoch: 24\n",
      "Accuracy: 0.9784373641014099\n",
      "Time taken:  1361.4751827716827\n",
      "==================================================\n",
      "Epoch 1/300\n",
      "986/986 [==============================] - 58s 59ms/step - loss: 0.0714 - accuracy: 0.4183 - val_loss: 0.1805 - val_accuracy: 0.0916\n",
      "Epoch 2/300\n",
      "986/986 [==============================] - 57s 58ms/step - loss: 0.0374 - accuracy: 0.7348 - val_loss: 0.0615 - val_accuracy: 0.5591\n",
      "Epoch 3/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0261 - accuracy: 0.8232 - val_loss: 0.1046 - val_accuracy: 0.1801\n",
      "Epoch 4/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0213 - accuracy: 0.8551 - val_loss: 0.0670 - val_accuracy: 0.5581\n",
      "Epoch 5/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0196 - accuracy: 0.8694 - val_loss: 0.0891 - val_accuracy: 0.2425\n",
      "Epoch 6/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0175 - accuracy: 0.8833 - val_loss: 0.0912 - val_accuracy: 0.3366\n",
      "Epoch 7/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0154 - accuracy: 0.8980 - val_loss: 0.1121 - val_accuracy: 0.1669\n",
      "Epoch 8/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0143 - accuracy: 0.9067 - val_loss: 0.0929 - val_accuracy: 0.2666\n",
      "Epoch 9/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0134 - accuracy: 0.9101 - val_loss: 0.1074 - val_accuracy: 0.2288\n",
      "Epoch 10/300\n",
      "986/986 [==============================] - 57s 58ms/step - loss: 0.0124 - accuracy: 0.9177 - val_loss: 0.0346 - val_accuracy: 0.7608\n",
      "Epoch 11/300\n",
      "986/986 [==============================] - 57s 58ms/step - loss: 0.0115 - accuracy: 0.9241 - val_loss: 0.0083 - val_accuracy: 0.9531\n",
      "Epoch 12/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0113 - accuracy: 0.9265 - val_loss: 0.0999 - val_accuracy: 0.1966\n",
      "Epoch 13/300\n",
      "986/986 [==============================] - 57s 58ms/step - loss: 0.0107 - accuracy: 0.9296 - val_loss: 0.0083 - val_accuracy: 0.9680\n",
      "Epoch 14/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0103 - accuracy: 0.9327 - val_loss: 0.0348 - val_accuracy: 0.7867\n",
      "Epoch 15/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0097 - accuracy: 0.9366 - val_loss: 0.0136 - val_accuracy: 0.9351\n",
      "Epoch 16/300\n",
      "986/986 [==============================] - 57s 58ms/step - loss: 0.0094 - accuracy: 0.9376 - val_loss: 0.0079 - val_accuracy: 0.9729\n",
      "Epoch 17/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0091 - accuracy: 0.9402 - val_loss: 0.0109 - val_accuracy: 0.9559\n",
      "Epoch 18/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0087 - accuracy: 0.9428 - val_loss: 0.0111 - val_accuracy: 0.9386\n",
      "Epoch 19/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0078 - accuracy: 0.9504 - val_loss: 0.0374 - val_accuracy: 0.7326\n",
      "Epoch 20/300\n",
      "986/986 [==============================] - 57s 58ms/step - loss: 0.0084 - accuracy: 0.9444 - val_loss: 0.0070 - val_accuracy: 0.9744\n",
      "Epoch 21/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0080 - accuracy: 0.9475 - val_loss: 0.0092 - val_accuracy: 0.9495\n",
      "Epoch 22/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0079 - accuracy: 0.9476 - val_loss: 0.0379 - val_accuracy: 0.7692\n",
      "Epoch 23/300\n",
      "986/986 [==============================] - 57s 58ms/step - loss: 0.0075 - accuracy: 0.9516 - val_loss: 0.0080 - val_accuracy: 0.9767\n",
      "Epoch 24/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0067 - accuracy: 0.9559 - val_loss: 0.0085 - val_accuracy: 0.9655\n",
      "Epoch 25/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0069 - accuracy: 0.9548 - val_loss: 0.0156 - val_accuracy: 0.9044\n",
      "Epoch 26/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0065 - accuracy: 0.9584 - val_loss: 0.0076 - val_accuracy: 0.9551\n",
      "Epoch 27/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0067 - accuracy: 0.9566 - val_loss: 0.0246 - val_accuracy: 0.8394\n",
      "Epoch 28/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0065 - accuracy: 0.9587 - val_loss: 0.0123 - val_accuracy: 0.9297\n",
      "Epoch 29/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "986/986 [==============================] - 57s 58ms/step - loss: 0.0063 - accuracy: 0.9586 - val_loss: 0.0043 - val_accuracy: 0.9845\n",
      "Epoch 30/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0059 - accuracy: 0.9611 - val_loss: 0.0094 - val_accuracy: 0.9576\n",
      "Epoch 31/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0060 - accuracy: 0.9623 - val_loss: 0.0049 - val_accuracy: 0.9777\n",
      "Epoch 32/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0060 - accuracy: 0.9611 - val_loss: 0.0130 - val_accuracy: 0.9252\n",
      "Epoch 33/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0056 - accuracy: 0.9645 - val_loss: 0.0082 - val_accuracy: 0.9566\n",
      "Epoch 34/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0057 - accuracy: 0.9641 - val_loss: 0.0052 - val_accuracy: 0.9736\n",
      "Epoch 35/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0057 - accuracy: 0.9635 - val_loss: 0.0055 - val_accuracy: 0.9744\n",
      "Epoch 36/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0055 - accuracy: 0.9652 - val_loss: 0.0138 - val_accuracy: 0.9376\n",
      "Epoch 37/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0055 - accuracy: 0.9647 - val_loss: 0.0102 - val_accuracy: 0.9536\n",
      "Epoch 38/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0049 - accuracy: 0.9675 - val_loss: 0.0054 - val_accuracy: 0.9795\n",
      "Epoch 39/300\n",
      "986/986 [==============================] - 56s 57ms/step - loss: 0.0050 - accuracy: 0.9681 - val_loss: 0.0070 - val_accuracy: 0.9711\n",
      "==================================================\n",
      "Result of densenet_model, fold 5\n",
      "Epoch: 39\n",
      "Accuracy: 0.9845256209373474\n",
      "Time taken:  2210.7134952545166\n",
      "==================================================\n",
      "Epoch 1/300\n",
      "  2/986 [..............................] - ETA: 33s - loss: 0.0973 - accuracy: 0.0625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0240s vs `on_train_batch_end` time: 0.0439s). Check your callbacks.\n",
      "986/986 [==============================] - 73s 74ms/step - loss: 0.0774 - accuracy: 0.3634 - val_loss: 0.1168 - val_accuracy: 0.3300\n",
      "Epoch 2/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0445 - accuracy: 0.6828 - val_loss: 0.0257 - val_accuracy: 0.8229\n",
      "Epoch 3/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0280 - accuracy: 0.8106 - val_loss: 0.0249 - val_accuracy: 0.8313\n",
      "Epoch 4/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0218 - accuracy: 0.8553 - val_loss: 0.0221 - val_accuracy: 0.8590\n",
      "Epoch 5/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0176 - accuracy: 0.8832 - val_loss: 0.0113 - val_accuracy: 0.9315\n",
      "Epoch 6/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0163 - accuracy: 0.8911 - val_loss: 0.0162 - val_accuracy: 0.9006\n",
      "Epoch 7/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0145 - accuracy: 0.9040 - val_loss: 0.0336 - val_accuracy: 0.7775\n",
      "Epoch 8/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0130 - accuracy: 0.9151 - val_loss: 0.0116 - val_accuracy: 0.9267\n",
      "Epoch 9/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0121 - accuracy: 0.9193 - val_loss: 0.0158 - val_accuracy: 0.8993\n",
      "Epoch 10/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0109 - accuracy: 0.9288 - val_loss: 0.0155 - val_accuracy: 0.8975\n",
      "Epoch 11/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0099 - accuracy: 0.9354 - val_loss: 0.0122 - val_accuracy: 0.9249\n",
      "Epoch 12/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0097 - accuracy: 0.9370 - val_loss: 0.0149 - val_accuracy: 0.9084\n",
      "Epoch 13/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0094 - accuracy: 0.9392 - val_loss: 0.0188 - val_accuracy: 0.8820\n",
      "Epoch 14/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0081 - accuracy: 0.9477 - val_loss: 0.0100 - val_accuracy: 0.9376\n",
      "Epoch 15/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0083 - accuracy: 0.9468 - val_loss: 0.0106 - val_accuracy: 0.9384\n",
      "Epoch 16/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0079 - accuracy: 0.9485 - val_loss: 0.0147 - val_accuracy: 0.9049\n",
      "Epoch 17/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0075 - accuracy: 0.9518 - val_loss: 0.0130 - val_accuracy: 0.9150\n",
      "Epoch 18/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0077 - accuracy: 0.9508 - val_loss: 0.0094 - val_accuracy: 0.9394\n",
      "Epoch 19/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0076 - accuracy: 0.9517 - val_loss: 0.0140 - val_accuracy: 0.9140\n",
      "Epoch 20/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0061 - accuracy: 0.9606 - val_loss: 0.0093 - val_accuracy: 0.9411\n",
      "Epoch 21/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0061 - accuracy: 0.9602 - val_loss: 0.0236 - val_accuracy: 0.8425\n",
      "Epoch 22/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0060 - accuracy: 0.9626 - val_loss: 0.0092 - val_accuracy: 0.9465\n",
      "Epoch 23/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0058 - accuracy: 0.9622 - val_loss: 0.0092 - val_accuracy: 0.9465\n",
      "Epoch 24/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0056 - accuracy: 0.9644 - val_loss: 0.0096 - val_accuracy: 0.9457\n",
      "Epoch 25/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0057 - accuracy: 0.9637 - val_loss: 0.0115 - val_accuracy: 0.9320\n",
      "Epoch 26/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0056 - accuracy: 0.9640 - val_loss: 0.0078 - val_accuracy: 0.9521\n",
      "Epoch 27/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0058 - accuracy: 0.9625 - val_loss: 0.0101 - val_accuracy: 0.9419\n",
      "Epoch 28/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0055 - accuracy: 0.9645 - val_loss: 0.0085 - val_accuracy: 0.9503\n",
      "Epoch 29/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0047 - accuracy: 0.9705 - val_loss: 0.0074 - val_accuracy: 0.9533\n",
      "Epoch 30/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0045 - accuracy: 0.9709 - val_loss: 0.0278 - val_accuracy: 0.8097\n",
      "Epoch 31/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0049 - accuracy: 0.9696 - val_loss: 0.0078 - val_accuracy: 0.9536\n",
      "Epoch 32/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0049 - accuracy: 0.9695 - val_loss: 0.0090 - val_accuracy: 0.9414\n",
      "Epoch 33/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0045 - accuracy: 0.9704 - val_loss: 0.0077 - val_accuracy: 0.9536\n",
      "Epoch 34/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0045 - accuracy: 0.9713 - val_loss: 0.0083 - val_accuracy: 0.9498\n",
      "Epoch 35/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0046 - accuracy: 0.9702 - val_loss: 0.0114 - val_accuracy: 0.9272\n",
      "Epoch 36/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0045 - accuracy: 0.9711 - val_loss: 0.0070 - val_accuracy: 0.9579\n",
      "Epoch 37/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0042 - accuracy: 0.9734 - val_loss: 0.0167 - val_accuracy: 0.8940\n",
      "Epoch 38/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0040 - accuracy: 0.9748 - val_loss: 0.0186 - val_accuracy: 0.8795\n",
      "Epoch 39/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0041 - accuracy: 0.9737 - val_loss: 0.0074 - val_accuracy: 0.9548\n",
      "Epoch 40/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0043 - accuracy: 0.9739 - val_loss: 0.0083 - val_accuracy: 0.9462\n",
      "Epoch 41/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0042 - accuracy: 0.9741 - val_loss: 0.0074 - val_accuracy: 0.9528\n",
      "Epoch 42/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0039 - accuracy: 0.9755 - val_loss: 0.0081 - val_accuracy: 0.9523\n",
      "Epoch 43/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0036 - accuracy: 0.9769 - val_loss: 0.0102 - val_accuracy: 0.9366\n",
      "Epoch 44/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0039 - accuracy: 0.9760 - val_loss: 0.0074 - val_accuracy: 0.9546\n",
      "Epoch 45/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0035 - accuracy: 0.9781 - val_loss: 0.0159 - val_accuracy: 0.8950\n",
      "Epoch 46/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0035 - accuracy: 0.9777 - val_loss: 0.0068 - val_accuracy: 0.9564\n",
      "==================================================\n",
      "Result of xception_model, fold 1\n",
      "Epoch: 46\n",
      "Accuracy: 0.9578893780708313\n",
      "Time taken:  3281.5663270950317\n",
      "==================================================\n",
      "Epoch 1/300\n",
      "  2/986 [..............................] - ETA: 40s - loss: 0.0997 - accuracy: 0.0938   WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0299s vs `on_train_batch_end` time: 0.0519s). Check your callbacks.\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0791 - accuracy: 0.3331 - val_loss: 0.0795 - val_accuracy: 0.4107\n",
      "Epoch 2/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0457 - accuracy: 0.6714 - val_loss: 0.0453 - val_accuracy: 0.6991\n",
      "Epoch 3/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0287 - accuracy: 0.8054 - val_loss: 0.0457 - val_accuracy: 0.6717\n",
      "Epoch 4/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0221 - accuracy: 0.8521 - val_loss: 0.0225 - val_accuracy: 0.8519\n",
      "Epoch 5/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0187 - accuracy: 0.8773 - val_loss: 0.0170 - val_accuracy: 0.8968\n",
      "Epoch 6/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0167 - accuracy: 0.8894 - val_loss: 0.0118 - val_accuracy: 0.9244\n",
      "Epoch 7/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0146 - accuracy: 0.9028 - val_loss: 0.0103 - val_accuracy: 0.9475\n",
      "Epoch 8/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0130 - accuracy: 0.9157 - val_loss: 0.0128 - val_accuracy: 0.9242\n",
      "Epoch 9/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0130 - accuracy: 0.9145 - val_loss: 0.0095 - val_accuracy: 0.9477\n",
      "Epoch 10/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0117 - accuracy: 0.9220 - val_loss: 0.0215 - val_accuracy: 0.8610\n",
      "Epoch 11/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0116 - accuracy: 0.9234 - val_loss: 0.0066 - val_accuracy: 0.9630\n",
      "Epoch 12/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0109 - accuracy: 0.9278 - val_loss: 0.0048 - val_accuracy: 0.9723\n",
      "Epoch 13/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0097 - accuracy: 0.9366 - val_loss: 0.0109 - val_accuracy: 0.9495\n",
      "Epoch 14/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0093 - accuracy: 0.9382 - val_loss: 0.0084 - val_accuracy: 0.9518\n",
      "Epoch 15/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0093 - accuracy: 0.9394 - val_loss: 0.0153 - val_accuracy: 0.9056\n",
      "Epoch 16/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0081 - accuracy: 0.9472 - val_loss: 0.0095 - val_accuracy: 0.9422\n",
      "Epoch 17/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0080 - accuracy: 0.9467 - val_loss: 0.0072 - val_accuracy: 0.9696\n",
      "Epoch 18/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0079 - accuracy: 0.9497 - val_loss: 0.0047 - val_accuracy: 0.9754\n",
      "Epoch 19/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0078 - accuracy: 0.9502 - val_loss: 0.0090 - val_accuracy: 0.9574\n",
      "Epoch 20/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0074 - accuracy: 0.9524 - val_loss: 0.0042 - val_accuracy: 0.9822\n",
      "Epoch 21/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0069 - accuracy: 0.9562 - val_loss: 0.0061 - val_accuracy: 0.9693\n",
      "Epoch 22/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0072 - accuracy: 0.9541 - val_loss: 0.0056 - val_accuracy: 0.9736\n",
      "Epoch 23/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0065 - accuracy: 0.9580 - val_loss: 0.0069 - val_accuracy: 0.9612\n",
      "Epoch 24/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0071 - accuracy: 0.9552 - val_loss: 0.0053 - val_accuracy: 0.9782\n",
      "Epoch 25/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0062 - accuracy: 0.9605 - val_loss: 0.0056 - val_accuracy: 0.9767\n",
      "Epoch 26/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0063 - accuracy: 0.9602 - val_loss: 0.0050 - val_accuracy: 0.9779\n",
      "Epoch 27/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0060 - accuracy: 0.9620 - val_loss: 0.0061 - val_accuracy: 0.9716\n",
      "Epoch 28/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0060 - accuracy: 0.9621 - val_loss: 0.0037 - val_accuracy: 0.9820\n",
      "Epoch 29/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0064 - accuracy: 0.9598 - val_loss: 0.0103 - val_accuracy: 0.9373\n",
      "Epoch 30/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0058 - accuracy: 0.9632 - val_loss: 0.0055 - val_accuracy: 0.9718\n",
      "==================================================\n",
      "Result of xception_model, fold 2\n",
      "Epoch: 30\n",
      "Accuracy: 0.9822425246238708\n",
      "Time taken:  2146.2537291049957\n",
      "==================================================\n",
      "Epoch 1/300\n",
      "  2/986 [..............................] - ETA: 42s - loss: 0.0923 - accuracy: 0.2188WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0309s vs `on_train_batch_end` time: 0.0548s). Check your callbacks.\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0786 - accuracy: 0.3441 - val_loss: 0.1323 - val_accuracy: 0.1783\n",
      "Epoch 2/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0469 - accuracy: 0.6639 - val_loss: 0.0279 - val_accuracy: 0.8110\n",
      "Epoch 3/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0280 - accuracy: 0.8114 - val_loss: 0.0189 - val_accuracy: 0.8790\n",
      "Epoch 4/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0229 - accuracy: 0.8460 - val_loss: 0.0172 - val_accuracy: 0.8983\n",
      "Epoch 5/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0194 - accuracy: 0.8725 - val_loss: 0.0138 - val_accuracy: 0.9206\n",
      "Epoch 6/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0172 - accuracy: 0.8850 - val_loss: 0.0124 - val_accuracy: 0.9252\n",
      "Epoch 7/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0149 - accuracy: 0.9021 - val_loss: 0.0109 - val_accuracy: 0.9368\n",
      "Epoch 8/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0135 - accuracy: 0.9102 - val_loss: 0.0099 - val_accuracy: 0.9381\n",
      "Epoch 9/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0132 - accuracy: 0.9125 - val_loss: 0.0096 - val_accuracy: 0.9467\n",
      "Epoch 10/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0119 - accuracy: 0.9227 - val_loss: 0.0057 - val_accuracy: 0.9680\n",
      "Epoch 11/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0115 - accuracy: 0.9248 - val_loss: 0.0077 - val_accuracy: 0.9579\n",
      "Epoch 12/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0106 - accuracy: 0.9321 - val_loss: 0.0057 - val_accuracy: 0.9716\n",
      "Epoch 13/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0103 - accuracy: 0.9314 - val_loss: 0.0043 - val_accuracy: 0.9807\n",
      "Epoch 14/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0092 - accuracy: 0.9406 - val_loss: 0.0092 - val_accuracy: 0.9490\n",
      "Epoch 15/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0091 - accuracy: 0.9413 - val_loss: 0.0068 - val_accuracy: 0.9696\n",
      "Epoch 16/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0086 - accuracy: 0.9460 - val_loss: 0.0092 - val_accuracy: 0.9457\n",
      "Epoch 17/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0083 - accuracy: 0.9464 - val_loss: 0.0051 - val_accuracy: 0.9772\n",
      "Epoch 18/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0087 - accuracy: 0.9439 - val_loss: 0.0067 - val_accuracy: 0.9739\n",
      "Epoch 19/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0080 - accuracy: 0.9482 - val_loss: 0.0047 - val_accuracy: 0.9772\n",
      "Epoch 20/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0076 - accuracy: 0.9505 - val_loss: 0.0043 - val_accuracy: 0.9792\n",
      "Epoch 21/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0072 - accuracy: 0.9538 - val_loss: 0.0074 - val_accuracy: 0.9604\n",
      "Epoch 22/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0076 - accuracy: 0.9511 - val_loss: 0.0116 - val_accuracy: 0.9247\n",
      "Epoch 23/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0068 - accuracy: 0.9568 - val_loss: 0.0053 - val_accuracy: 0.9711\n",
      "==================================================\n",
      "Result of xception_model, fold 3\n",
      "Epoch: 23\n",
      "Accuracy: 0.9807204604148865\n",
      "Time taken:  1643.4275538921356\n",
      "==================================================\n",
      "Epoch 1/300\n",
      "  2/986 [..............................] - ETA: 42s - loss: 0.0963 - accuracy: 0.1875WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0299s vs `on_train_batch_end` time: 0.0559s). Check your callbacks.\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0782 - accuracy: 0.3495 - val_loss: 0.1320 - val_accuracy: 0.1611\n",
      "Epoch 2/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0466 - accuracy: 0.6647 - val_loss: 0.0684 - val_accuracy: 0.5728\n",
      "Epoch 3/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0295 - accuracy: 0.8002 - val_loss: 0.0325 - val_accuracy: 0.7633\n",
      "Epoch 4/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0226 - accuracy: 0.8477 - val_loss: 0.0199 - val_accuracy: 0.8716\n",
      "Epoch 5/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0187 - accuracy: 0.8750 - val_loss: 0.0184 - val_accuracy: 0.8815\n",
      "Epoch 6/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0167 - accuracy: 0.8887 - val_loss: 0.0116 - val_accuracy: 0.9323\n",
      "Epoch 7/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0148 - accuracy: 0.9032 - val_loss: 0.0195 - val_accuracy: 0.8790\n",
      "Epoch 8/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0140 - accuracy: 0.9072 - val_loss: 0.0085 - val_accuracy: 0.9521\n",
      "Epoch 9/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0129 - accuracy: 0.9152 - val_loss: 0.0123 - val_accuracy: 0.9267\n",
      "Epoch 10/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0119 - accuracy: 0.9257 - val_loss: 0.0078 - val_accuracy: 0.9597\n",
      "Epoch 11/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0117 - accuracy: 0.9229 - val_loss: 0.0089 - val_accuracy: 0.9503\n",
      "Epoch 12/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0104 - accuracy: 0.9328 - val_loss: 0.0058 - val_accuracy: 0.9683\n",
      "Epoch 13/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0102 - accuracy: 0.9331 - val_loss: 0.0121 - val_accuracy: 0.9295\n",
      "Epoch 14/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0092 - accuracy: 0.9404 - val_loss: 0.0177 - val_accuracy: 0.8858\n",
      "Epoch 15/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0096 - accuracy: 0.9373 - val_loss: 0.0100 - val_accuracy: 0.9480\n",
      "Epoch 16/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0089 - accuracy: 0.9425 - val_loss: 0.0058 - val_accuracy: 0.9660\n",
      "Epoch 17/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0079 - accuracy: 0.9495 - val_loss: 0.0039 - val_accuracy: 0.9789\n",
      "Epoch 18/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0078 - accuracy: 0.9487 - val_loss: 0.0058 - val_accuracy: 0.9663\n",
      "Epoch 19/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0080 - accuracy: 0.9487 - val_loss: 0.0055 - val_accuracy: 0.9680\n",
      "Epoch 20/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0072 - accuracy: 0.9534 - val_loss: 0.0057 - val_accuracy: 0.9729\n",
      "Epoch 21/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0068 - accuracy: 0.9566 - val_loss: 0.0051 - val_accuracy: 0.9751\n",
      "Epoch 22/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0068 - accuracy: 0.9563 - val_loss: 0.0041 - val_accuracy: 0.9787\n",
      "Epoch 23/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0070 - accuracy: 0.9551 - val_loss: 0.0051 - val_accuracy: 0.9741\n",
      "Epoch 24/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0070 - accuracy: 0.9557 - val_loss: 0.0036 - val_accuracy: 0.9817\n",
      "Epoch 25/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0067 - accuracy: 0.9578 - val_loss: 0.0051 - val_accuracy: 0.9736\n",
      "Epoch 26/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0065 - accuracy: 0.9578 - val_loss: 0.0104 - val_accuracy: 0.9389\n",
      "Epoch 27/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0067 - accuracy: 0.9571 - val_loss: 0.0055 - val_accuracy: 0.9731\n",
      "Epoch 28/300\n",
      "986/986 [==============================] - 71s 73ms/step - loss: 0.0060 - accuracy: 0.9617 - val_loss: 0.0041 - val_accuracy: 0.9827\n",
      "Epoch 29/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0061 - accuracy: 0.9607 - val_loss: 0.0048 - val_accuracy: 0.9779\n",
      "Epoch 30/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0059 - accuracy: 0.9613 - val_loss: 0.0038 - val_accuracy: 0.9802\n",
      "Epoch 31/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0056 - accuracy: 0.9637 - val_loss: 0.0044 - val_accuracy: 0.9802\n",
      "Epoch 32/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0055 - accuracy: 0.9656 - val_loss: 0.0038 - val_accuracy: 0.9815\n",
      "Epoch 33/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0058 - accuracy: 0.9632 - val_loss: 0.0049 - val_accuracy: 0.9713\n",
      "Epoch 34/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0054 - accuracy: 0.9651 - val_loss: 0.0039 - val_accuracy: 0.9782\n",
      "Epoch 35/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0055 - accuracy: 0.9662 - val_loss: 0.0044 - val_accuracy: 0.9792\n",
      "Epoch 36/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0050 - accuracy: 0.9682 - val_loss: 0.0034 - val_accuracy: 0.9815\n",
      "Epoch 37/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0050 - accuracy: 0.9675 - val_loss: 0.0055 - val_accuracy: 0.9688\n",
      "Epoch 38/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0051 - accuracy: 0.9677 - val_loss: 0.0045 - val_accuracy: 0.9777\n",
      "==================================================\n",
      "Result of xception_model, fold 4\n",
      "Epoch: 38\n",
      "Accuracy: 0.982749879360199\n",
      "Time taken:  2709.027147769928\n",
      "==================================================\n",
      "Epoch 1/300\n",
      "  2/986 [..............................] - ETA: 39s - loss: 0.0951 - accuracy: 0.1250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0309s vs `on_train_batch_end` time: 0.0489s). Check your callbacks.\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0772 - accuracy: 0.3572 - val_loss: 0.1267 - val_accuracy: 0.1281\n",
      "Epoch 2/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0417 - accuracy: 0.7046 - val_loss: 0.0225 - val_accuracy: 0.8531\n",
      "Epoch 3/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0278 - accuracy: 0.8113 - val_loss: 0.0212 - val_accuracy: 0.8645\n",
      "Epoch 4/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0214 - accuracy: 0.8568 - val_loss: 0.0204 - val_accuracy: 0.8714\n",
      "Epoch 5/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0184 - accuracy: 0.8778 - val_loss: 0.0162 - val_accuracy: 0.9084\n",
      "Epoch 6/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0167 - accuracy: 0.8924 - val_loss: 0.0094 - val_accuracy: 0.9470\n",
      "Epoch 7/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0151 - accuracy: 0.9014 - val_loss: 0.0103 - val_accuracy: 0.9488\n",
      "Epoch 8/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0141 - accuracy: 0.9079 - val_loss: 0.0150 - val_accuracy: 0.9371\n",
      "Epoch 9/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0127 - accuracy: 0.9179 - val_loss: 0.0163 - val_accuracy: 0.8980\n",
      "Epoch 10/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0118 - accuracy: 0.9236 - val_loss: 0.0093 - val_accuracy: 0.9536\n",
      "Epoch 11/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0111 - accuracy: 0.9271 - val_loss: 0.0089 - val_accuracy: 0.9523\n",
      "Epoch 12/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0102 - accuracy: 0.9337 - val_loss: 0.0472 - val_accuracy: 0.7047\n",
      "Epoch 13/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0107 - accuracy: 0.9320 - val_loss: 0.0065 - val_accuracy: 0.9670\n",
      "Epoch 14/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0097 - accuracy: 0.9368 - val_loss: 0.0210 - val_accuracy: 0.8595\n",
      "Epoch 15/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0092 - accuracy: 0.9389 - val_loss: 0.0044 - val_accuracy: 0.9787\n",
      "Epoch 16/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0091 - accuracy: 0.9408 - val_loss: 0.0156 - val_accuracy: 0.8924\n",
      "Epoch 17/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0082 - accuracy: 0.9475 - val_loss: 0.0097 - val_accuracy: 0.9442\n",
      "Epoch 18/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0085 - accuracy: 0.9443 - val_loss: 0.0110 - val_accuracy: 0.9323\n",
      "Epoch 19/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0081 - accuracy: 0.9482 - val_loss: 0.0048 - val_accuracy: 0.9772\n",
      "Epoch 20/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0073 - accuracy: 0.9526 - val_loss: 0.0063 - val_accuracy: 0.9711\n",
      "Epoch 21/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0080 - accuracy: 0.9476 - val_loss: 0.0044 - val_accuracy: 0.9812\n",
      "Epoch 22/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0072 - accuracy: 0.9545 - val_loss: 0.0037 - val_accuracy: 0.9827\n",
      "Epoch 23/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0072 - accuracy: 0.9543 - val_loss: 0.0041 - val_accuracy: 0.9802\n",
      "Epoch 24/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0068 - accuracy: 0.9569 - val_loss: 0.0106 - val_accuracy: 0.9371\n",
      "Epoch 25/300\n",
      "986/986 [==============================] - 71s 73ms/step - loss: 0.0062 - accuracy: 0.9602 - val_loss: 0.0041 - val_accuracy: 0.9782\n",
      "Epoch 26/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0062 - accuracy: 0.9607 - val_loss: 0.0069 - val_accuracy: 0.9589\n",
      "Epoch 27/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0065 - accuracy: 0.9588 - val_loss: 0.0039 - val_accuracy: 0.9810\n",
      "Epoch 28/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0061 - accuracy: 0.9607 - val_loss: 0.0076 - val_accuracy: 0.9536\n",
      "Epoch 29/300\n",
      "986/986 [==============================] - 72s 73ms/step - loss: 0.0063 - accuracy: 0.9578 - val_loss: 0.0037 - val_accuracy: 0.9815\n",
      "Epoch 30/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0055 - accuracy: 0.9645 - val_loss: 0.0043 - val_accuracy: 0.9797\n",
      "Epoch 31/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0053 - accuracy: 0.9661 - val_loss: 0.0061 - val_accuracy: 0.9688\n",
      "Epoch 32/300\n",
      "986/986 [==============================] - 71s 72ms/step - loss: 0.0054 - accuracy: 0.9661 - val_loss: 0.0046 - val_accuracy: 0.9764\n",
      "==================================================\n",
      "Result of xception_model, fold 5\n",
      "Epoch: 32\n",
      "Accuracy: 0.982749879360199\n",
      "Time taken:  2298.6791367530823\n",
      "==================================================\n",
      "Epoch 1/300\n",
      "986/986 [==============================] - 49s 49ms/step - loss: 0.0871 - accuracy: 0.2245 - val_loss: 0.0778 - val_accuracy: 0.3699\n",
      "Epoch 2/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0721 - accuracy: 0.4269 - val_loss: 0.0588 - val_accuracy: 0.6010\n",
      "Epoch 3/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0504 - accuracy: 0.6412 - val_loss: 0.0238 - val_accuracy: 0.8318\n",
      "Epoch 4/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0375 - accuracy: 0.7461 - val_loss: 0.0269 - val_accuracy: 0.8143\n",
      "Epoch 5/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0304 - accuracy: 0.7977 - val_loss: 0.0381 - val_accuracy: 0.7369\n",
      "Epoch 6/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0275 - accuracy: 0.8203 - val_loss: 0.0290 - val_accuracy: 0.8118\n",
      "Epoch 7/300\n",
      "986/986 [==============================] - 47s 47ms/step - loss: 0.0236 - accuracy: 0.8459 - val_loss: 0.0249 - val_accuracy: 0.8354\n",
      "Epoch 8/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0209 - accuracy: 0.8653 - val_loss: 0.0359 - val_accuracy: 0.7626\n",
      "Epoch 9/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0194 - accuracy: 0.8749 - val_loss: 0.0171 - val_accuracy: 0.8886\n",
      "Epoch 10/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0175 - accuracy: 0.8862 - val_loss: 0.0135 - val_accuracy: 0.9153\n",
      "Epoch 11/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0160 - accuracy: 0.8969 - val_loss: 0.0206 - val_accuracy: 0.8772\n",
      "Epoch 12/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0158 - accuracy: 0.9002 - val_loss: 0.0120 - val_accuracy: 0.9267\n",
      "Epoch 13/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0140 - accuracy: 0.9107 - val_loss: 0.0164 - val_accuracy: 0.9018\n",
      "Epoch 14/300\n",
      "986/986 [==============================] - 47s 47ms/step - loss: 0.0130 - accuracy: 0.9167 - val_loss: 0.0131 - val_accuracy: 0.9272\n",
      "Epoch 15/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0122 - accuracy: 0.9221 - val_loss: 0.0212 - val_accuracy: 0.8734\n",
      "Epoch 16/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0109 - accuracy: 0.9319 - val_loss: 0.0125 - val_accuracy: 0.9242\n",
      "Epoch 17/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0116 - accuracy: 0.9271 - val_loss: 0.0135 - val_accuracy: 0.9209\n",
      "Epoch 18/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0109 - accuracy: 0.9313 - val_loss: 0.0174 - val_accuracy: 0.9054\n",
      "Epoch 19/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0093 - accuracy: 0.9414 - val_loss: 0.0148 - val_accuracy: 0.9259\n",
      "Epoch 20/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0098 - accuracy: 0.9381 - val_loss: 0.0150 - val_accuracy: 0.9145\n",
      "Epoch 21/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0102 - accuracy: 0.9354 - val_loss: 0.0106 - val_accuracy: 0.9394\n",
      "Epoch 22/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0091 - accuracy: 0.9423 - val_loss: 0.0171 - val_accuracy: 0.9310\n",
      "Epoch 23/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0086 - accuracy: 0.9450 - val_loss: 0.0092 - val_accuracy: 0.9434\n",
      "Epoch 24/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0085 - accuracy: 0.9473 - val_loss: 0.0138 - val_accuracy: 0.9229\n",
      "Epoch 25/300\n",
      "986/986 [==============================] - 45s 46ms/step - loss: 0.0083 - accuracy: 0.9486 - val_loss: 0.0110 - val_accuracy: 0.9381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0079 - accuracy: 0.9507 - val_loss: 0.0127 - val_accuracy: 0.9297\n",
      "Epoch 27/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0075 - accuracy: 0.9533 - val_loss: 0.0126 - val_accuracy: 0.9300\n",
      "Epoch 28/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0073 - accuracy: 0.9541 - val_loss: 0.0101 - val_accuracy: 0.9450\n",
      "Epoch 29/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0071 - accuracy: 0.9560 - val_loss: 0.0171 - val_accuracy: 0.9077\n",
      "Epoch 30/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0074 - accuracy: 0.9536 - val_loss: 0.0094 - val_accuracy: 0.9515\n",
      "Epoch 31/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0071 - accuracy: 0.9546 - val_loss: 0.0211 - val_accuracy: 0.8625\n",
      "Epoch 32/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0068 - accuracy: 0.9561 - val_loss: 0.0139 - val_accuracy: 0.9328\n",
      "Epoch 33/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0065 - accuracy: 0.9583 - val_loss: 0.0092 - val_accuracy: 0.9584\n",
      "Epoch 34/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0068 - accuracy: 0.9578 - val_loss: 0.0109 - val_accuracy: 0.9452\n",
      "Epoch 35/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0060 - accuracy: 0.9621 - val_loss: 0.0097 - val_accuracy: 0.9521\n",
      "Epoch 36/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0061 - accuracy: 0.9622 - val_loss: 0.0250 - val_accuracy: 0.8683\n",
      "Epoch 37/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0056 - accuracy: 0.9643 - val_loss: 0.0112 - val_accuracy: 0.9437\n",
      "Epoch 38/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0056 - accuracy: 0.9651 - val_loss: 0.0123 - val_accuracy: 0.9417\n",
      "Epoch 39/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0056 - accuracy: 0.9644 - val_loss: 0.0128 - val_accuracy: 0.9409\n",
      "Epoch 40/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0056 - accuracy: 0.9656 - val_loss: 0.0138 - val_accuracy: 0.9488\n",
      "Epoch 41/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0056 - accuracy: 0.9649 - val_loss: 0.0095 - val_accuracy: 0.9548\n",
      "Epoch 42/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0055 - accuracy: 0.9658 - val_loss: 0.0109 - val_accuracy: 0.9503\n",
      "Epoch 43/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0052 - accuracy: 0.9676 - val_loss: 0.0122 - val_accuracy: 0.9373\n",
      "==================================================\n",
      "Result of inception_model, fold 1\n",
      "Epoch: 43\n",
      "Accuracy: 0.9583967328071594\n",
      "Time taken:  1987.5978400707245\n",
      "==================================================\n",
      "Epoch 1/300\n",
      "986/986 [==============================] - 47s 48ms/step - loss: 0.0861 - accuracy: 0.2381 - val_loss: 0.0762 - val_accuracy: 0.4521\n",
      "Epoch 2/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0687 - accuracy: 0.4618 - val_loss: 0.0492 - val_accuracy: 0.6383\n",
      "Epoch 3/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0485 - accuracy: 0.6573 - val_loss: 0.0209 - val_accuracy: 0.8645\n",
      "Epoch 4/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0363 - accuracy: 0.7525 - val_loss: 0.0479 - val_accuracy: 0.6761\n",
      "Epoch 5/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0311 - accuracy: 0.7922 - val_loss: 0.0134 - val_accuracy: 0.9168\n",
      "Epoch 6/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0267 - accuracy: 0.8210 - val_loss: 0.0428 - val_accuracy: 0.7136\n",
      "Epoch 7/300\n",
      "986/986 [==============================] - 47s 47ms/step - loss: 0.0234 - accuracy: 0.8476 - val_loss: 0.0119 - val_accuracy: 0.9247\n",
      "Epoch 8/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0209 - accuracy: 0.8631 - val_loss: 0.0105 - val_accuracy: 0.9358\n",
      "Epoch 9/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0199 - accuracy: 0.8709 - val_loss: 0.0089 - val_accuracy: 0.9531\n",
      "Epoch 10/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0179 - accuracy: 0.8837 - val_loss: 0.0106 - val_accuracy: 0.9417\n",
      "Epoch 11/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0160 - accuracy: 0.8962 - val_loss: 0.0100 - val_accuracy: 0.9470\n",
      "Epoch 12/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0156 - accuracy: 0.8978 - val_loss: 0.0095 - val_accuracy: 0.9518\n",
      "Epoch 13/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0147 - accuracy: 0.9069 - val_loss: 0.0087 - val_accuracy: 0.9538\n",
      "Epoch 14/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0130 - accuracy: 0.9162 - val_loss: 0.0052 - val_accuracy: 0.9756\n",
      "Epoch 15/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0127 - accuracy: 0.9186 - val_loss: 0.0159 - val_accuracy: 0.9016\n",
      "Epoch 16/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0124 - accuracy: 0.9212 - val_loss: 0.0095 - val_accuracy: 0.9566\n",
      "Epoch 17/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0116 - accuracy: 0.9276 - val_loss: 0.0117 - val_accuracy: 0.9384\n",
      "Epoch 18/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0112 - accuracy: 0.9284 - val_loss: 0.0108 - val_accuracy: 0.9569\n",
      "Epoch 19/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0105 - accuracy: 0.9333 - val_loss: 0.0084 - val_accuracy: 0.9551\n",
      "Epoch 20/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0103 - accuracy: 0.9333 - val_loss: 0.0098 - val_accuracy: 0.9498\n",
      "Epoch 21/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0096 - accuracy: 0.9393 - val_loss: 0.0058 - val_accuracy: 0.9774\n",
      "Epoch 22/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0099 - accuracy: 0.9377 - val_loss: 0.0101 - val_accuracy: 0.9564\n",
      "Epoch 23/300\n",
      "986/986 [==============================] - 45s 46ms/step - loss: 0.0091 - accuracy: 0.9418 - val_loss: 0.0076 - val_accuracy: 0.9597\n",
      "Epoch 24/300\n",
      "986/986 [==============================] - 45s 46ms/step - loss: 0.0086 - accuracy: 0.9442 - val_loss: 0.0197 - val_accuracy: 0.9018\n",
      "Epoch 25/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0083 - accuracy: 0.9481 - val_loss: 0.0076 - val_accuracy: 0.9772\n",
      "Epoch 26/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0081 - accuracy: 0.9476 - val_loss: 0.0085 - val_accuracy: 0.9703\n",
      "Epoch 27/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0083 - accuracy: 0.9472 - val_loss: 0.0072 - val_accuracy: 0.9691\n",
      "Epoch 28/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0078 - accuracy: 0.9512 - val_loss: 0.0088 - val_accuracy: 0.9556\n",
      "Epoch 29/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0078 - accuracy: 0.9507 - val_loss: 0.0085 - val_accuracy: 0.9767\n",
      "Epoch 30/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0075 - accuracy: 0.9535 - val_loss: 0.0053 - val_accuracy: 0.9762\n",
      "Epoch 31/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0070 - accuracy: 0.9569 - val_loss: 0.0094 - val_accuracy: 0.9556\n",
      "==================================================\n",
      "Result of inception_model, fold 2\n",
      "Epoch: 31\n",
      "Accuracy: 0.9774226546287537\n",
      "Time taken:  1430.754697561264\n",
      "==================================================\n",
      "Epoch 1/300\n",
      "986/986 [==============================] - 48s 49ms/step - loss: 0.0849 - accuracy: 0.2566 - val_loss: 0.0821 - val_accuracy: 0.4280\n",
      "Epoch 2/300\n",
      "986/986 [==============================] - 45s 46ms/step - loss: 0.0652 - accuracy: 0.5009 - val_loss: 0.1016 - val_accuracy: 0.3097\n",
      "Epoch 3/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0451 - accuracy: 0.6862 - val_loss: 0.0569 - val_accuracy: 0.6433\n",
      "Epoch 4/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0359 - accuracy: 0.7585 - val_loss: 0.0225 - val_accuracy: 0.8478\n",
      "Epoch 5/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0311 - accuracy: 0.7952 - val_loss: 0.0207 - val_accuracy: 0.8640\n",
      "Epoch 6/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0265 - accuracy: 0.8254 - val_loss: 0.0225 - val_accuracy: 0.8488\n",
      "Epoch 7/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0241 - accuracy: 0.8441 - val_loss: 0.0095 - val_accuracy: 0.9490\n",
      "Epoch 8/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0221 - accuracy: 0.8545 - val_loss: 0.0118 - val_accuracy: 0.9267\n",
      "Epoch 9/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0188 - accuracy: 0.8785 - val_loss: 0.0164 - val_accuracy: 0.8927\n",
      "Epoch 10/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0179 - accuracy: 0.8855 - val_loss: 0.0201 - val_accuracy: 0.8729\n",
      "Epoch 11/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0166 - accuracy: 0.8929 - val_loss: 0.0147 - val_accuracy: 0.9049\n",
      "Epoch 12/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0150 - accuracy: 0.9031 - val_loss: 0.0066 - val_accuracy: 0.9625\n",
      "Epoch 13/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0139 - accuracy: 0.9110 - val_loss: 0.0057 - val_accuracy: 0.9713\n",
      "Epoch 14/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0129 - accuracy: 0.9191 - val_loss: 0.0074 - val_accuracy: 0.9533\n",
      "Epoch 15/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0129 - accuracy: 0.9188 - val_loss: 0.0232 - val_accuracy: 0.8486\n",
      "Epoch 16/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0125 - accuracy: 0.9198 - val_loss: 0.0101 - val_accuracy: 0.9510\n",
      "Epoch 17/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0119 - accuracy: 0.9242 - val_loss: 0.0080 - val_accuracy: 0.9579\n",
      "Epoch 18/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0106 - accuracy: 0.9333 - val_loss: 0.0045 - val_accuracy: 0.9795\n",
      "Epoch 19/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0109 - accuracy: 0.9304 - val_loss: 0.0105 - val_accuracy: 0.9589\n",
      "Epoch 20/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0105 - accuracy: 0.9331 - val_loss: 0.0052 - val_accuracy: 0.9751\n",
      "Epoch 21/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0096 - accuracy: 0.9384 - val_loss: 0.0062 - val_accuracy: 0.9688\n",
      "Epoch 22/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0090 - accuracy: 0.9436 - val_loss: 0.0062 - val_accuracy: 0.9711\n",
      "Epoch 23/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0097 - accuracy: 0.9388 - val_loss: 0.0069 - val_accuracy: 0.9647\n",
      "Epoch 24/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0090 - accuracy: 0.9434 - val_loss: 0.0049 - val_accuracy: 0.9769\n",
      "Epoch 25/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0084 - accuracy: 0.9467 - val_loss: 0.0052 - val_accuracy: 0.9726\n",
      "Epoch 26/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0081 - accuracy: 0.9477 - val_loss: 0.0069 - val_accuracy: 0.9729\n",
      "Epoch 27/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0080 - accuracy: 0.9508 - val_loss: 0.0055 - val_accuracy: 0.9731\n",
      "Epoch 28/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0088 - accuracy: 0.9448 - val_loss: 0.0048 - val_accuracy: 0.9835\n",
      "Epoch 29/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0076 - accuracy: 0.9523 - val_loss: 0.0057 - val_accuracy: 0.9736\n",
      "Epoch 30/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0079 - accuracy: 0.9494 - val_loss: 0.0056 - val_accuracy: 0.9767\n",
      "Epoch 31/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0073 - accuracy: 0.9545 - val_loss: 0.0062 - val_accuracy: 0.9762\n",
      "Epoch 32/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0070 - accuracy: 0.9555 - val_loss: 0.0063 - val_accuracy: 0.9726\n",
      "Epoch 33/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0072 - accuracy: 0.9539 - val_loss: 0.0073 - val_accuracy: 0.9650\n",
      "Epoch 34/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0071 - accuracy: 0.9557 - val_loss: 0.0048 - val_accuracy: 0.9789\n",
      "Epoch 35/300\n",
      "986/986 [==============================] - 45s 46ms/step - loss: 0.0062 - accuracy: 0.9603 - val_loss: 0.0082 - val_accuracy: 0.9734\n",
      "Epoch 36/300\n",
      "986/986 [==============================] - 45s 46ms/step - loss: 0.0063 - accuracy: 0.9602 - val_loss: 0.0039 - val_accuracy: 0.9815\n",
      "Epoch 37/300\n",
      "986/986 [==============================] - 45s 46ms/step - loss: 0.0065 - accuracy: 0.9589 - val_loss: 0.0074 - val_accuracy: 0.9599\n",
      "Epoch 38/300\n",
      "986/986 [==============================] - 45s 46ms/step - loss: 0.0061 - accuracy: 0.9609 - val_loss: 0.0050 - val_accuracy: 0.9767\n",
      "==================================================\n",
      "Result of inception_model, fold 3\n",
      "Epoch: 38\n",
      "Accuracy: 0.9835109114646912\n",
      "Time taken:  1749.4701561927795\n",
      "==================================================\n",
      "Epoch 1/300\n",
      "986/986 [==============================] - 48s 49ms/step - loss: 0.0872 - accuracy: 0.2185 - val_loss: 0.0808 - val_accuracy: 0.3592\n",
      "Epoch 2/300\n",
      "986/986 [==============================] - 47s 48ms/step - loss: 0.0730 - accuracy: 0.4169 - val_loss: 0.0493 - val_accuracy: 0.6514\n",
      "Epoch 3/300\n",
      "986/986 [==============================] - 47s 48ms/step - loss: 0.0506 - accuracy: 0.6379 - val_loss: 0.0329 - val_accuracy: 0.7765\n",
      "Epoch 4/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0371 - accuracy: 0.7487 - val_loss: 0.0395 - val_accuracy: 0.7243\n",
      "Epoch 5/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0318 - accuracy: 0.7879 - val_loss: 0.0360 - val_accuracy: 0.7506\n",
      "Epoch 6/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0281 - accuracy: 0.8144 - val_loss: 0.0135 - val_accuracy: 0.9209\n",
      "Epoch 7/300\n",
      "986/986 [==============================] - 47s 47ms/step - loss: 0.0246 - accuracy: 0.8404 - val_loss: 0.0155 - val_accuracy: 0.9158\n",
      "Epoch 8/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0238 - accuracy: 0.8448 - val_loss: 0.0137 - val_accuracy: 0.9150\n",
      "Epoch 9/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0203 - accuracy: 0.8685 - val_loss: 0.0078 - val_accuracy: 0.9619\n",
      "Epoch 10/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0182 - accuracy: 0.8827 - val_loss: 0.0227 - val_accuracy: 0.8526\n",
      "Epoch 11/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0172 - accuracy: 0.8890 - val_loss: 0.0198 - val_accuracy: 0.8658\n",
      "Epoch 12/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0163 - accuracy: 0.8954 - val_loss: 0.0116 - val_accuracy: 0.9470\n",
      "Epoch 13/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0154 - accuracy: 0.9004 - val_loss: 0.0177 - val_accuracy: 0.8983\n",
      "Epoch 14/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0142 - accuracy: 0.9076 - val_loss: 0.0093 - val_accuracy: 0.9670\n",
      "Epoch 15/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0125 - accuracy: 0.9198 - val_loss: 0.0110 - val_accuracy: 0.9470\n",
      "Epoch 16/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0126 - accuracy: 0.9206 - val_loss: 0.0093 - val_accuracy: 0.9614\n",
      "Epoch 17/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0119 - accuracy: 0.9240 - val_loss: 0.0078 - val_accuracy: 0.9630\n",
      "Epoch 18/300\n",
      "986/986 [==============================] - 47s 48ms/step - loss: 0.0114 - accuracy: 0.9259 - val_loss: 0.0065 - val_accuracy: 0.9723\n",
      "Epoch 19/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0101 - accuracy: 0.9369 - val_loss: 0.0099 - val_accuracy: 0.9683\n",
      "Epoch 20/300\n",
      "986/986 [==============================] - 47s 47ms/step - loss: 0.0107 - accuracy: 0.9323 - val_loss: 0.0081 - val_accuracy: 0.9764\n",
      "Epoch 21/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0102 - accuracy: 0.9339 - val_loss: 0.0109 - val_accuracy: 0.9665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/300\n",
      "986/986 [==============================] - 45s 46ms/step - loss: 0.0096 - accuracy: 0.9380 - val_loss: 0.0114 - val_accuracy: 0.9614\n",
      "Epoch 23/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0097 - accuracy: 0.9396 - val_loss: 0.0075 - val_accuracy: 0.9746\n",
      "Epoch 24/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0081 - accuracy: 0.9489 - val_loss: 0.0087 - val_accuracy: 0.9762\n",
      "Epoch 25/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0086 - accuracy: 0.9457 - val_loss: 0.0091 - val_accuracy: 0.9736\n",
      "Epoch 26/300\n",
      "986/986 [==============================] - 45s 46ms/step - loss: 0.0080 - accuracy: 0.9494 - val_loss: 0.0110 - val_accuracy: 0.9721\n",
      "Epoch 27/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0080 - accuracy: 0.9490 - val_loss: 0.0122 - val_accuracy: 0.9498\n",
      "Epoch 28/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0084 - accuracy: 0.9468 - val_loss: 0.0121 - val_accuracy: 0.9762\n",
      "Epoch 29/300\n",
      "986/986 [==============================] - 47s 47ms/step - loss: 0.0083 - accuracy: 0.9481 - val_loss: 0.0075 - val_accuracy: 0.9825\n",
      "Epoch 30/300\n",
      "986/986 [==============================] - 45s 46ms/step - loss: 0.0071 - accuracy: 0.9558 - val_loss: 0.0107 - val_accuracy: 0.9645\n",
      "Epoch 31/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0068 - accuracy: 0.9583 - val_loss: 0.0092 - val_accuracy: 0.9660\n",
      "Epoch 32/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0070 - accuracy: 0.9561 - val_loss: 0.0089 - val_accuracy: 0.9749\n",
      "Epoch 33/300\n",
      "986/986 [==============================] - 47s 47ms/step - loss: 0.0072 - accuracy: 0.9550 - val_loss: 0.0061 - val_accuracy: 0.9805\n",
      "Epoch 34/300\n",
      "986/986 [==============================] - 45s 46ms/step - loss: 0.0071 - accuracy: 0.9552 - val_loss: 0.0064 - val_accuracy: 0.9754\n",
      "Epoch 35/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0066 - accuracy: 0.9585 - val_loss: 0.0083 - val_accuracy: 0.9795\n",
      "Epoch 36/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0067 - accuracy: 0.9574 - val_loss: 0.0077 - val_accuracy: 0.9789\n",
      "Epoch 37/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0059 - accuracy: 0.9623 - val_loss: 0.0062 - val_accuracy: 0.9762\n",
      "Epoch 38/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0061 - accuracy: 0.9618 - val_loss: 0.0128 - val_accuracy: 0.9587\n",
      "Epoch 39/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0061 - accuracy: 0.9625 - val_loss: 0.0078 - val_accuracy: 0.9754\n",
      "==================================================\n",
      "Result of inception_model, fold 4\n",
      "Epoch: 39\n",
      "Accuracy: 0.9824962019920349\n",
      "Time taken:  1808.742411851883\n",
      "==================================================\n",
      "Epoch 1/300\n",
      "986/986 [==============================] - 49s 49ms/step - loss: 0.0871 - accuracy: 0.2208 - val_loss: 0.1692 - val_accuracy: 0.1400\n",
      "Epoch 2/300\n",
      "986/986 [==============================] - 47s 47ms/step - loss: 0.0734 - accuracy: 0.4143 - val_loss: 0.0520 - val_accuracy: 0.6390\n",
      "Epoch 3/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0524 - accuracy: 0.6260 - val_loss: 0.0704 - val_accuracy: 0.4782\n",
      "Epoch 4/300\n",
      "986/986 [==============================] - 47s 48ms/step - loss: 0.0397 - accuracy: 0.7294 - val_loss: 0.0179 - val_accuracy: 0.8800\n",
      "Epoch 5/300\n",
      "986/986 [==============================] - 47s 48ms/step - loss: 0.0316 - accuracy: 0.7894 - val_loss: 0.0166 - val_accuracy: 0.8922\n",
      "Epoch 6/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0285 - accuracy: 0.8123 - val_loss: 0.0366 - val_accuracy: 0.7516\n",
      "Epoch 7/300\n",
      "986/986 [==============================] - 47s 48ms/step - loss: 0.0256 - accuracy: 0.8317 - val_loss: 0.0122 - val_accuracy: 0.9203\n",
      "Epoch 8/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0219 - accuracy: 0.8550 - val_loss: 0.0427 - val_accuracy: 0.7035\n",
      "Epoch 9/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0201 - accuracy: 0.8690 - val_loss: 0.0274 - val_accuracy: 0.8148\n",
      "Epoch 10/300\n",
      "986/986 [==============================] - 47s 47ms/step - loss: 0.0186 - accuracy: 0.8806 - val_loss: 0.0105 - val_accuracy: 0.9439\n",
      "Epoch 11/300\n",
      "986/986 [==============================] - 47s 47ms/step - loss: 0.0181 - accuracy: 0.8842 - val_loss: 0.0071 - val_accuracy: 0.9632\n",
      "Epoch 12/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0156 - accuracy: 0.8978 - val_loss: 0.0062 - val_accuracy: 0.9708\n",
      "Epoch 13/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0147 - accuracy: 0.9052 - val_loss: 0.0101 - val_accuracy: 0.9475\n",
      "Epoch 14/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0132 - accuracy: 0.9151 - val_loss: 0.0152 - val_accuracy: 0.9282\n",
      "Epoch 15/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0129 - accuracy: 0.9170 - val_loss: 0.0061 - val_accuracy: 0.9660\n",
      "Epoch 16/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0121 - accuracy: 0.9226 - val_loss: 0.0215 - val_accuracy: 0.8739\n",
      "Epoch 17/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0123 - accuracy: 0.9208 - val_loss: 0.0081 - val_accuracy: 0.9584\n",
      "Epoch 18/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0106 - accuracy: 0.9316 - val_loss: 0.0055 - val_accuracy: 0.9777\n",
      "Epoch 19/300\n",
      "986/986 [==============================] - 45s 46ms/step - loss: 0.0104 - accuracy: 0.9344 - val_loss: 0.0114 - val_accuracy: 0.9290\n",
      "Epoch 20/300\n",
      "986/986 [==============================] - 47s 48ms/step - loss: 0.0106 - accuracy: 0.9325 - val_loss: 0.0084 - val_accuracy: 0.9551\n",
      "Epoch 21/300\n",
      "986/986 [==============================] - 47s 47ms/step - loss: 0.0097 - accuracy: 0.9388 - val_loss: 0.0131 - val_accuracy: 0.9505\n",
      "Epoch 22/300\n",
      "986/986 [==============================] - 47s 47ms/step - loss: 0.0096 - accuracy: 0.9398 - val_loss: 0.0067 - val_accuracy: 0.9708\n",
      "Epoch 23/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0100 - accuracy: 0.9351 - val_loss: 0.0104 - val_accuracy: 0.9462\n",
      "Epoch 24/300\n",
      "986/986 [==============================] - 47s 47ms/step - loss: 0.0092 - accuracy: 0.9416 - val_loss: 0.0094 - val_accuracy: 0.9706\n",
      "Epoch 25/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0088 - accuracy: 0.9441 - val_loss: 0.0068 - val_accuracy: 0.9772\n",
      "Epoch 26/300\n",
      "986/986 [==============================] - 47s 47ms/step - loss: 0.0085 - accuracy: 0.9467 - val_loss: 0.0058 - val_accuracy: 0.9784\n",
      "Epoch 27/300\n",
      "986/986 [==============================] - 45s 46ms/step - loss: 0.0083 - accuracy: 0.9482 - val_loss: 0.0100 - val_accuracy: 0.9472\n",
      "Epoch 28/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0086 - accuracy: 0.9457 - val_loss: 0.0060 - val_accuracy: 0.9789\n",
      "Epoch 29/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0080 - accuracy: 0.9506 - val_loss: 0.0055 - val_accuracy: 0.9800\n",
      "Epoch 30/300\n",
      "986/986 [==============================] - 45s 46ms/step - loss: 0.0078 - accuracy: 0.9513 - val_loss: 0.0074 - val_accuracy: 0.9693\n",
      "Epoch 31/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0070 - accuracy: 0.9561 - val_loss: 0.0063 - val_accuracy: 0.9772\n",
      "Epoch 32/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0075 - accuracy: 0.9533 - val_loss: 0.0055 - val_accuracy: 0.9777\n",
      "Epoch 33/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0070 - accuracy: 0.9558 - val_loss: 0.0073 - val_accuracy: 0.9726\n",
      "Epoch 34/300\n",
      "986/986 [==============================] - 45s 46ms/step - loss: 0.0071 - accuracy: 0.9554 - val_loss: 0.0179 - val_accuracy: 0.9148\n",
      "Epoch 35/300\n",
      "986/986 [==============================] - 45s 46ms/step - loss: 0.0069 - accuracy: 0.9564 - val_loss: 0.0071 - val_accuracy: 0.9713\n",
      "Epoch 36/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0063 - accuracy: 0.9612 - val_loss: 0.0056 - val_accuracy: 0.9789\n",
      "Epoch 37/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0068 - accuracy: 0.9581 - val_loss: 0.0114 - val_accuracy: 0.9429\n",
      "Epoch 38/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0063 - accuracy: 0.9608 - val_loss: 0.0038 - val_accuracy: 0.9838\n",
      "Epoch 39/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0064 - accuracy: 0.9587 - val_loss: 0.0060 - val_accuracy: 0.9827\n",
      "Epoch 40/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0059 - accuracy: 0.9640 - val_loss: 0.0049 - val_accuracy: 0.9835\n",
      "Epoch 41/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0063 - accuracy: 0.9598 - val_loss: 0.0125 - val_accuracy: 0.9307\n",
      "Epoch 42/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0062 - accuracy: 0.9621 - val_loss: 0.0053 - val_accuracy: 0.9746\n",
      "Epoch 43/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0058 - accuracy: 0.9642 - val_loss: 0.0052 - val_accuracy: 0.9759\n",
      "Epoch 44/300\n",
      "986/986 [==============================] - 47s 48ms/step - loss: 0.0058 - accuracy: 0.9645 - val_loss: 0.0049 - val_accuracy: 0.9878\n",
      "Epoch 45/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0056 - accuracy: 0.9658 - val_loss: 0.0053 - val_accuracy: 0.9706\n",
      "Epoch 46/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0057 - accuracy: 0.9645 - val_loss: 0.0079 - val_accuracy: 0.9665\n",
      "Epoch 47/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0054 - accuracy: 0.9659 - val_loss: 0.0050 - val_accuracy: 0.9802\n",
      "Epoch 48/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0055 - accuracy: 0.9651 - val_loss: 0.0070 - val_accuracy: 0.9777\n",
      "Epoch 49/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0057 - accuracy: 0.9628 - val_loss: 0.0052 - val_accuracy: 0.9789\n",
      "Epoch 50/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0051 - accuracy: 0.9671 - val_loss: 0.0077 - val_accuracy: 0.9774\n",
      "Epoch 51/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0056 - accuracy: 0.9664 - val_loss: 0.0062 - val_accuracy: 0.9731\n",
      "Epoch 52/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0052 - accuracy: 0.9677 - val_loss: 0.0060 - val_accuracy: 0.9764\n",
      "Epoch 53/300\n",
      "986/986 [==============================] - 46s 47ms/step - loss: 0.0052 - accuracy: 0.9680 - val_loss: 0.0043 - val_accuracy: 0.9835\n",
      "Epoch 54/300\n",
      "986/986 [==============================] - 46s 46ms/step - loss: 0.0047 - accuracy: 0.9715 - val_loss: 0.0033 - val_accuracy: 0.9855\n",
      "==================================================\n",
      "Result of inception_model, fold 5\n",
      "Epoch: 54\n",
      "Accuracy: 0.9878234267234802\n",
      "Time taken:  2499.896492242813\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for model_fn in model_fn_list:\n",
    "    kf = KFold(n_splits=5)\n",
    "    for i, (train_index, val_index) in enumerate(kf.split(train_digits)):\n",
    "        start_time = time.time()\n",
    "        model = model_fn(input_shape_1=input_shape_1, input_shape_2=input_shape_2, output_size=output_size)\n",
    "\n",
    "        # Validation 점수가 가장 좋은 모델만 저장합니다.\n",
    "        checkpoint_path = os.path.join(CHECKPOINT_PATH, f'{model.name}_fold{i+1}')\n",
    "        if os.path.isdir(checkpoint_path):\n",
    "            shutil.rmtree(checkpoint_path, ignore_errors=True)\n",
    "        os.mkdir(checkpoint_path)\n",
    "        checkpoint_file_path = os.path.join(checkpoint_path, 'Epoch_{epoch:03d}_Val_{val_loss:.3f}.hdf5')\n",
    "        checkpoint = ModelCheckpoint(filepath=checkpoint_file_path, monitor='val_accuracy', verbose=0, save_best_only=True)\n",
    "\n",
    "        # 30회 간 Validation 점수가 좋아지지 않으면 중지합니다.\n",
    "        early_stopping = EarlyStopping(monitor='val_accuracy', patience=10)\n",
    "\n",
    "        train_data = [train_letters[train_index], train_pixels[train_index]]\n",
    "        train_label = train_digits[train_index]\n",
    "\n",
    "        val_data = [train_letters[val_index], train_pixels[val_index]]\n",
    "        val_label = train_digits[val_index]\n",
    "\n",
    "        history = model.fit(\n",
    "            train_data, train_label,\n",
    "            epochs=EPOCHS,\n",
    "            validation_data=(val_data, val_label),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            verbose=1,\n",
    "            callbacks=[early_stopping, checkpoint],\n",
    "        )\n",
    "\n",
    "        # 가장 좋은 모델의 weight를 불러옵니다.\n",
    "        weigth_file = glob.glob('{}/*.hdf5'.format(checkpoint_path))[-1]\n",
    "        model.load_weights(weigth_file)\n",
    "        model.save(os.path.join(MODEL_PATH, f'{model.name}_{i+1}.h5'))\n",
    "        \n",
    "        shutil.rmtree(checkpoint_path, ignore_errors=True)\n",
    "\n",
    "        epoch_num = len(history.history['val_accuracy'])\n",
    "        max_accuracy = max(history.history['val_accuracy'])\n",
    "        print('='*50)\n",
    "        print(f'Result of {model.name}, fold {i+1}')\n",
    "        print(f'Epoch: {epoch_num}')\n",
    "        print(f'Accuracy: {max_accuracy}')\n",
    "        print('Time taken: ', time.time() - start_time)\n",
    "        print('='*50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free: 159MB / 6144MB    97.41% used \n"
     ]
    }
   ],
   "source": [
    "import subprocess as sp\n",
    "import os\n",
    "\n",
    "def get_gpu_memory():\n",
    "    total = 6144\n",
    "    _output_to_list = lambda x: x.decode('ascii').split('\\n')[:-1]\n",
    "\n",
    "    ACCEPTABLE_AVAILABLE_MEMORY = 1024\n",
    "    COMMAND = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "    memory_free_info = _output_to_list(sp.check_output(COMMAND.split()))[1:]\n",
    "    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "    print(f\"Free: {memory_free_values[0]}MB / {total}MB    {(total - memory_free_values[0]) / total * 100:.2f}% used \")\n",
    "    \n",
    "def reset_keras():\n",
    "    sess = tf.compat.v1.keras.backend.get_session()\n",
    "    tf.compat.v1.keras.backend.clear_session()\n",
    "    sess.close()\n",
    "    sess = tf.compat.v1.keras.backend.get_session()\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "    config.gpu_options.visible_device_list = \"0\"\n",
    "    tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))\n",
    "\n",
    "get_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EpyeYgaAIgDO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 densenet_model\n",
      "Free: 148MB / 6144MB    97.59% used \n",
      "==================================================\n",
      "2 densenet_model\n",
      "Free: 153MB / 6144MB    97.51% used \n",
      "==================================================\n",
      "3 densenet_model\n",
      "Free: 164MB / 6144MB    97.33% used \n",
      "==================================================\n",
      "4 densenet_model\n",
      "Free: 162MB / 6144MB    97.36% used \n",
      "==================================================\n",
      "5 densenet_model\n",
      "Free: 162MB / 6144MB    97.36% used \n",
      "==================================================\n",
      "6 inception_model\n",
      "Free: 152MB / 6144MB    97.53% used \n",
      "==================================================\n",
      "7 inception_model\n",
      "Free: 127MB / 6144MB    97.93% used \n",
      "==================================================\n",
      "8 inception_model\n",
      "Free: 148MB / 6144MB    97.59% used \n",
      "==================================================\n",
      "9 inception_model\n",
      "Free: 130MB / 6144MB    97.88% used \n",
      "==================================================\n",
      "10 inception_model\n",
      "Free: 122MB / 6144MB    98.01% used \n",
      "==================================================\n",
      "11 resnet_model\n",
      "Free: 134MB / 6144MB    97.82% used \n",
      "==================================================\n",
      "12 resnet_model\n",
      "Free: 112MB / 6144MB    98.18% used \n",
      "==================================================\n",
      "13 resnet_model\n",
      "Free: 109MB / 6144MB    98.23% used \n",
      "==================================================\n",
      "14 resnet_model\n",
      "Free: 105MB / 6144MB    98.29% used \n",
      "==================================================\n",
      "15 resnet_model\n",
      "Free: 106MB / 6144MB    98.27% used \n",
      "==================================================\n",
      "16 thin_resnet_model\n",
      "Free: 106MB / 6144MB    98.27% used \n",
      "==================================================\n",
      "17 thin_resnet_model\n",
      "Free: 110MB / 6144MB    98.21% used \n",
      "==================================================\n",
      "18 thin_resnet_model\n",
      "Free: 110MB / 6144MB    98.21% used \n",
      "==================================================\n",
      "19 thin_resnet_model\n",
      "Free: 118MB / 6144MB    98.08% used \n",
      "==================================================\n",
      "20 thin_resnet_model\n",
      "Free: 120MB / 6144MB    98.05% used \n",
      "==================================================\n",
      "21 vggnet_model\n",
      "Free: 117MB / 6144MB    98.10% used \n",
      "==================================================\n",
      "22 vggnet_model\n",
      "Free: 107MB / 6144MB    98.26% used \n",
      "==================================================\n",
      "23 vggnet_model\n",
      "Free: 106MB / 6144MB    98.27% used \n",
      "==================================================\n",
      "24 vggnet_model\n",
      "Free: 104MB / 6144MB    98.31% used \n",
      "==================================================\n",
      "25 vggnet_model\n",
      "Free: 109MB / 6144MB    98.23% used \n",
      "==================================================\n",
      "26 xception_model\n",
      "Free: 110MB / 6144MB    98.21% used \n",
      "==================================================\n",
      "27 xception_model\n",
      "Free: 111MB / 6144MB    98.19% used \n",
      "==================================================\n",
      "28 xception_model\n",
      "Free: 104MB / 6144MB    98.31% used \n",
      "==================================================\n",
      "29 xception_model\n",
      "Free: 108MB / 6144MB    98.24% used \n",
      "==================================================\n",
      "30 xception_model\n",
      "Free: 111MB / 6144MB    98.19% used \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "model_files = glob.glob(f'{MODEL_PATH}/*.h5')\n",
    "pred_list = []\n",
    "for i, model_file in enumerate(model_files):\n",
    "    model = keras.models.load_model(model_file,\n",
    "                                    custom_objects={'RandomRollLayer':RandomRollLayer})\n",
    "\n",
    "    print(i+1, model.name)\n",
    "    y_pred = model.predict([test_letters, test_pixels])\n",
    "    pred_list.append(y_pred)\n",
    "    model = None\n",
    "    gc.collect()\n",
    "    reset_keras()\n",
    "    get_gpu_memory()\n",
    "    print('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>digit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2049</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2050</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2051</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2052</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2053</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20475</th>\n",
       "      <td>22524</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20476</th>\n",
       "      <td>22525</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20477</th>\n",
       "      <td>22526</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20478</th>\n",
       "      <td>22527</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20479</th>\n",
       "      <td>22528</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20480 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  digit\n",
       "0       2049      6\n",
       "1       2050      9\n",
       "2       2051      8\n",
       "3       2052      0\n",
       "4       2053      3\n",
       "...      ...    ...\n",
       "20475  22524      4\n",
       "20476  22525      1\n",
       "20477  22526      6\n",
       "20478  22527      8\n",
       "20479  22528      0\n",
       "\n",
       "[20480 rows x 2 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_data = np.zeros(pred_list[0].shape)\n",
    "\n",
    "for pred in pred_list:\n",
    "    ensemble_data += pred ** 0.5\n",
    "    \n",
    "y_pred = np.argmax(ensemble_data, axis=1)\n",
    "\n",
    "submission_csv['digit'] = y_pred\n",
    "submission_csv.to_csv('CNN.csv', index=False)\n",
    "submission_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19710, 10), (19710, 26), (19710, 28, 28, 1))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(ensemble_data)\n",
    "df['max'] = df.iloc[:, 0:10].max(axis=1)\n",
    "df['pred'] = df.iloc[:, 0:10].idxmax(axis=1)\n",
    "\n",
    "good_df = df[df['max'] > 20]\n",
    "\n",
    "test_digits_int = good_df['pred'].to_numpy()\n",
    "test_digits = tf.keras.utils.to_categorical(test_digits_int)\n",
    "\n",
    "new_digits = np.concatenate((train_digits, test_digits))\n",
    "new_letters = np.concatenate((train_letters, test_letters[good_df.index]))\n",
    "new_pixels = np.concatenate((train_pixels, test_pixels[good_df.index]))\n",
    "new_digits.shape, new_letters.shape, new_pixels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_digits = new_digits\n",
    "train_letters = new_letters\n",
    "train_pixels = new_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmKuoQ5ute9G"
   },
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict([test_letters, test_pixels]), axis=1)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rMBz0xqKv9Jo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "submission_csv['digit'] = y_pred\n",
    "submission_csv.to_csv('CNN.csv', index=False)\n",
    "submission_csv\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}